{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from fastembed import SparseTextEmbedding, TextEmbedding\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import SparseVector\n",
    "from typing import List, Union\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "from typing import List\n",
    "import pprint\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI \n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "Qdrant_API_KEY = os.getenv('Qdrant_API_KEY')\n",
    "Qdrant_URL = os.getenv('Qdrant_URL')\n",
    "Collection_Name = os.getenv('Collection_Name')\n",
    "\n",
    "from pydantic import BaseModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Search Strategy Interface\n",
    "class SearchStrategy:\n",
    "    def search(self, query: str, metadata_filter: None) -> List[str]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SemanticSearch(SearchStrategy):\n",
    "    def query_semantic_search(self, query: str, metadata_filter: models.Filter) -> List[str]:\n",
    "        # Load the dense embedding model\n",
    "        embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "        # Initialize the Qdrant client\n",
    "        qdrant_client = QdrantClient(\n",
    "            url=Qdrant_URL,\n",
    "            api_key=Qdrant_API_KEY,\n",
    "            timeout=30\n",
    "        )\n",
    "\n",
    "        # Embed the query using the dense embedding model\n",
    "        dense_query = embedding_model.encode([query]).tolist()[0]\n",
    "\n",
    "        # Perform the semantic search\n",
    "        results = qdrant_client.search(\n",
    "            collection_name=Collection_Name,\n",
    "            query_vector=dense_query,\n",
    "            query_filter=metadata_filter,\n",
    "            limit=4,\n",
    "        )\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Define the reranker models\n",
    "class SentenceTransformerRerank:\n",
    "    def __init__(self, model, top_n):\n",
    "        self.model = CrossEncoder(model)\n",
    "        self.top_n = top_n\n",
    "\n",
    "    def rerank(self, query, documents):\n",
    "        # Compute the similarity scores between the query and each document\n",
    "        scores = self.model.predict([(query, doc) for doc in documents])\n",
    "\n",
    "        # Sort the documents based on their similarity scores\n",
    "        ranked_documents = sorted(zip(documents, scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Select the top documents\n",
    "        top_documents = [doc for doc, score in ranked_documents[:self.top_n]]\n",
    "\n",
    "        return top_documents\n",
    "\n",
    "# Dictionary of reranker models\n",
    "RERANKERS = {\n",
    "    \"WithoutReranker\": None,\n",
    "    \"CrossEncoder\": SentenceTransformerRerank(model='cross-encoder/ms-marco-MiniLM-L-6-v2', top_n=2),\n",
    "    \"bge-reranker-base\": SentenceTransformerRerank(model=\"BAAI/bge-reranker-base\", top_n=2),\n",
    "    \"bge-reranker-large\": SentenceTransformerRerank(model=\"BAAI/bge-reranker-large\", top_n=2)\n",
    "}\n",
    "\n",
    "# ReRankingAgent function\n",
    "def ReRankingAgent(query, documents, reranking_model: str):\n",
    "    # Get the reranker model based on user preference\n",
    "    reranker = RERANKERS.get(reranking_model)\n",
    "\n",
    "    if reranker is None:\n",
    "        # If no reranker is specified, return the documents as is\n",
    "        return documents\n",
    "\n",
    "    # Perform reranking\n",
    "    top_documents = reranker.rerank(query, documents)\n",
    "\n",
    "    return top_documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSearch(SearchStrategy):\n",
    "    def query_hybrid_search(self, query: str) -> List[str]:\n",
    "\n",
    "        embedding_model = TextEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        sparse_embedding_model = SparseTextEmbedding(model_name=\"Qdrant/bm42-all-minilm-l6-v2-attentions\")\n",
    "        qdrant_client = QdrantClient(\n",
    "            url=Qdrant_URL,\n",
    "            api_key=Qdrant_API_KEY,\n",
    "            timeout=30\n",
    "        )\n",
    "\n",
    "        # Embed the query using the dense embedding model\n",
    "        dense_query = list(embedding_model.embed([query]))[0].tolist()\n",
    "\n",
    "        # Embed the query using the sparse embedding model\n",
    "        sparse_query = list(sparse_embedding_model.embed([query]))[0]\n",
    "\n",
    "        results = qdrant_client.query_points(\n",
    "            collection_name=Collection_Name,\n",
    "            prefetch=[\n",
    "                models.Prefetch(\n",
    "                    query=models.SparseVector(indices=sparse_query.indices.tolist(), values=sparse_query.values.tolist()),\n",
    "                    using=\"sparse\",\n",
    "                    limit=4,\n",
    "                ),\n",
    "                models.Prefetch(\n",
    "                    query=dense_query,\n",
    "                    using=\"dense\",\n",
    "                    limit=4,\n",
    "                ),\n",
    "            ],\n",
    "            query=models.FusionQuery(fusion=models.Fusion.RRF), #Reciprocal Rerank Fusion\n",
    "        )\n",
    "        \n",
    "        # Extract the text from the payload of each scored point\n",
    "        documents = [point.payload['text'] for point in results.points]\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factory Function to Get the Appropriate Search Strategy\n",
    "def get_search_strategy(search_type: str) -> SearchStrategy:\n",
    "    if search_type == 'semantic':\n",
    "        return SemanticSearch()\n",
    "    elif search_type == 'hybrid':\n",
    "        return HybridSearch()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid search type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(search_type: str, query: str, reranking_model: str):\n",
    "        \"\"\"\n",
    "        Perform the search and retrieval process based on the specified search type, query, metadata filter, and reranking model.\n",
    "        \"\"\"\n",
    "        print(\"Starting the search and retrieval process\")\n",
    "        search_strategy = get_search_strategy(search_type)\n",
    "        documents = search_strategy.query_hybrid_search(query)\n",
    "        print(\"Search and retrieval process completed\")\n",
    "        reranked_documents = ReRankingAgent(query, documents, reranking_model)\n",
    "        print(\"Reranking of the retrieved documents is complete\")\n",
    "\n",
    "        return reranked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever('hybrid', 'what is self-RAG?', reranking_model='CrossEncoder')\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http.models import Filter\n",
    "from pydantic import BaseModel, validator\n",
    "from qdrant_client.http.models import Filter\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "# RetrieverAgent function\n",
    "def RetrieverAgent(state: dict) -> OpenAIAgent:\n",
    "    '''        \n",
    "    class MyModel(BaseModel):\n",
    "       \n",
    "        search_type: str\n",
    "        query: str\n",
    "        metadata_filter: Any\n",
    "        reranking_model: str\n",
    "\n",
    "        class Config:\n",
    "            arbitrary_types_allowed = True\n",
    "    '''\n",
    "\n",
    "    def retriever(search_type: str, query: str, metadata_filter: Any, reranking_model: str):\n",
    "        \"\"\"\n",
    "        Perform the search and retrieval process based on the specified search type, query, metadata filter, and reranking model.\n",
    "        \"\"\"\n",
    "        print(\"Starting the search and retrieval process\")\n",
    "        search_strategy = get_search_strategy(search_type)\n",
    "        documents = search_strategy.search(query, metadata_filter, reranking_model)\n",
    "        print(\"Search and retrieval process completed\")\n",
    "        reranked_documents = ReRankingAgent(query, documents, reranking_model)\n",
    "        print(\"Reranking of the retrieved documents is complete\")\n",
    "\n",
    "        return reranked_documents\n",
    "\n",
    "    def done() -> None:\n",
    "        \"\"\"\n",
    "        Signal that the retrieval process is complete and update the state.\n",
    "        \"\"\"\n",
    "        logging.info(\"Retrieval process is complete and updating the state\")\n",
    "        state[\"current_speaker\"] = None\n",
    "        state[\"just_finished\"] = True\n",
    "\n",
    "    tools = [\n",
    "        \n",
    "        FunctionTool.from_defaults(fn=retriever),\n",
    "        FunctionTool.from_defaults(fn=done),\n",
    "    ]\n",
    "\n",
    "    system_prompt = (f\"\"\"\n",
    "    You are a helpful assistant that is performing search and retrieval tasks for a retrieval-augmented generation (RAG) system.\n",
    "    Your task is to retrieve documents based on the user's query, search type, metadata filter, and reranking model.\n",
    "    To do this, you need to know the search type, query, metadata filter, and reranking model.\n",
    "    You can ask the user to supply these details.\n",
    "    If the user supplies the necessary information, then call the tool \"retriever\" using the provided details to perform the search and retrieval process.\n",
    "    But remember for search() function do not use search_type argument and for ReRankingAgent function don't use metadata_filter and search arguments \n",
    "    The current user state is:\n",
    "    {pprint.pformat(state, indent=4)}\n",
    "    When you have completed the retrieval process, call the tool \"done\" to signal that you are done.\n",
    "    If the user asks to do anything other than retrieve documents, call the tool \"done\" to signal that some other agent should help.\n",
    "    \"\"\")\n",
    "\n",
    "    return OpenAIAgent.from_tools(\n",
    "        tools,\n",
    "        llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "        system_prompt=system_prompt,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the search and retrieval process\n",
      "Starting the search and retrieval process\n",
      "Starting the search and retrieval process\n",
      "Starting the search and retrieval process\n"
     ]
    }
   ],
   "source": [
    "state = {}\n",
    "agent = RetrieverAgent(state = state)\n",
    "response = agent.chat(\"I want to query what is self-RAG? with hybrid search type and SELF-RAG.pdf as add metadata filter, following with CrossEncoder Reranking model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='It seems there is still an issue with the retrieval process. Let me try to retrieve the documents again with the correct arguments.', sources=[ToolOutput(content='Error: SearchStrategy.search() takes 3 positional arguments but 4 were given', tool_name='retriever', raw_input={'kwargs': {'search_type': 'hybrid', 'query': 'what is self-RAG?', 'metadata_filter': 'SELF-RAG.pdf', 'reranking_model': 'CrossEncoder'}}, raw_output=TypeError('SearchStrategy.search() takes 3 positional arguments but 4 were given'), is_error=False), ToolOutput(content='Error: SearchStrategy.search() takes 3 positional arguments but 4 were given', tool_name='retriever', raw_input={'kwargs': {'search_type': 'hybrid', 'query': 'what is self-RAG?', 'metadata_filter': 'SELF-RAG.pdf', 'reranking_model': 'CrossEncoder'}}, raw_output=TypeError('SearchStrategy.search() takes 3 positional arguments but 4 were given'), is_error=False), ToolOutput(content='None', tool_name='done', raw_input={'args': (), 'kwargs': {}}, raw_output=None, is_error=False), ToolOutput(content=\"Error: RetrieverAgent.<locals>.retriever() missing 1 required positional argument: 'metadata_filter'\", tool_name='retriever', raw_input={'kwargs': {'search_type': 'hybrid', 'query': 'what is self-RAG?', 'reranking_model': 'CrossEncoder'}}, raw_output=TypeError(\"RetrieverAgent.<locals>.retriever() missing 1 required positional argument: 'metadata_filter'\"), is_error=False), ToolOutput(content='Error: SearchStrategy.search() takes 3 positional arguments but 4 were given', tool_name='retriever', raw_input={'kwargs': {'search_type': 'hybrid', 'query': 'what is self-RAG?', 'metadata_filter': 'SELF-RAG.pdf', 'reranking_model': 'CrossEncoder'}}, raw_output=TypeError('SearchStrategy.search() takes 3 positional arguments but 4 were given'), is_error=False), ToolOutput(content='Error: SearchStrategy.search() takes 3 positional arguments but 4 were given', tool_name='retriever', raw_input={'kwargs': {'search_type': 'hybrid', 'query': 'what is self-RAG?', 'metadata_filter': 'SELF-RAG.pdf', 'reranking_model': 'CrossEncoder'}}, raw_output=TypeError('SearchStrategy.search() takes 3 positional arguments but 4 were given'), is_error=False), ToolOutput(content='None', tool_name='done', raw_input={'args': (), 'kwargs': {}}, raw_output=None, is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"I encountered an error while trying to retrieve the documents.\n",
    " Let me attempt the retrieval process again by specifying the search type, query, metadata filter, \n",
    " and reranking model all at once.\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pavan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from retriever_agent import Retriever\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core.response_synthesizers import BaseSynthesizer\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prompt_template():\n",
    "    \"\"\"\n",
    "    Define the prompt template for generating explanations based on the context and query.\n",
    "    \"\"\"\n",
    "    prompt_str = \"\"\"\n",
    "    You are an AI assistant specializing in explaining complex topics related to Retrieval-Augmented Generation(RAG).\n",
    "    Your task is to provide a clear, concise, and informative explanation based on the following context and query.\n",
    "\n",
    "    Context:\n",
    "    {context_str}\n",
    "\n",
    "    Query: {query_str}\n",
    "\n",
    "    Please follow these guidelines in your response:\n",
    "    1. Start with a brief overview of the concept mentioned in the query.\n",
    "    2. Provide at least one concrete example or use case to illustrate the concept.\n",
    "    3. If there are any limitations or challenges associated with this concept, briefly mention them.\n",
    "    4. Conclude with a sentence about the potential future impact or applications of this concept.\n",
    "\n",
    "    Your explanation should be informative yet accessible, suitable for someone with a basic understanding of RAG.\n",
    "    If the query asks for information not present in the context, please state that you don't have enough information to provide a complete answer,\n",
    "    and only respond based on the given context.\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    "    prompt_tmpl = PromptTemplate(prompt_str)\n",
    "    return prompt_tmpl\n",
    "\n",
    "def prompt_generation(state):\n",
    "    \"\"\"\n",
    "    Generate the prompt for the given search type, query, and reranking model.\n",
    "    \"\"\"\n",
    "    state = state\n",
    "    retriever_agent = Retriever(state)\n",
    "    reranked_documents = retriever_agent.retriever()\n",
    "\n",
    "    context = \"\\n\\n\".join(reranked_documents)\n",
    "    query = state.get('query')\n",
    "    prompt_templ = prompt_template().format(context_str=context, query_str=query)\n",
    "\n",
    "    return prompt_templ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the search and retrieval process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd84541930643ceb294349b7236d455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221f167ea43c4586b1b8912795bdae40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search and retrieval process completed\n",
      "Reranking of the retrieved documents is complete\n"
     ]
    }
   ],
   "source": [
    "state = {   'chunk_overlap': None,\n",
    "    'chunk_size': None,\n",
    "    'current_speaker': 'Concierge',\n",
    "    'embedding_model': None,\n",
    "    'input_dir': None,\n",
    "    'just_finished': False,\n",
    "    'query': 'what is self-RAG?',\n",
    "    'reranking_model': 'crossencoder',\n",
    "    'search_type': 'hybrid',\n",
    "    'session_token': None}\n",
    "\n",
    "prompt = prompt_generation(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    You are an AI assistant specializing in explaining complex topics related to Retrieval-Augmented Generation(RAG).\\n    Your task is to provide a clear, concise, and informative explanation based on the following context and query.\\n\\n    Context:\\n    ragnarÃ¶k a reusable rag framework and baselines for trec 2024 retrievalaugmented generation track conference17 july 2017 washington dc usa figure 4 webui dark mode showcasing the ragnarÃ¶k system arena for the user query on why have used car prices increased from trec2024 researchy with two different blinded pipelines the output tab displays the answers in humanreadable form figure 5 the responses tab for the example in figure 4 note that the responses tab reformats the final answers into the json format expected by the io definitions of the trec 2024 rag track\\n\\npreprint selfrag learning to retrieve  generate and critique through selfreflection akari asai zeqiu wu yizhong wang avirup sil hannaneh hajishirzi university of washingtonallen institute for aiibm research ai akarizeqiuwuyizhongwhannaneh cswashingtonedu aviusibmcom abstract despite their remarkable capabilities large language models llms often produce responses containing factual inaccuracies due to their sole reliance on the paramet ric knowledge they encapsulate retrievalaugmented generation rag an ad hoc approach that augments lms with retrieval of relevant knowledge decreases such issues however indiscriminately retrieving and incorporating a fixed number of retrieved passages regardless of whether retrieval is necessary or passages are relevant diminishes lm versatility or can lead to unhelpful response generation we introduce a new framework called selfreflective retrievalaugmented gen eration  selfragthat enhances an lms quality and factuality through retrieval and selfreflection our framework trains a single arbitrary lm that adaptively retrieves passages ondemand and generates and reflects on retrieved passages and its own generations using special tokens called reflection tokens generating reflection tokens makes the lm controllable during the inference phase enabling it to tailor its behavior to diverse task requirements experiments show that self rag7b and 13b parameters significantly outperforms stateoftheart llms and retrievalaugmented models on a diverse set of tasks specifically selfrag outperforms chatgpt and retrievalaugmented llama2chat on opendomain qa reasoning and fact verification tasks and it shows significant gains in improving factuality and citation accuracy for longform generations relative to these models1 1 i ntroduction stateoftheart llms continue to struggle with factual errors mallen et al 2023 min et al 2023 despite their increased model and data scale ouyang et al 2022 retrievalaugmented generation rag methods figure 1 left lewis et al 2020 guu et al 2020 augment the input of llms with relevant retrieved passages reducing factual errors in knowledgeintensive tasks ram et al 2023 asai et al 2023a however these methods may hinder the versatility of llms or introduce unnecessary or offtopic passages that lead to lowquality generations shi et al 2023 since they retrieve passages indiscriminately regardless of whether the factual grounding is helpful moreover the output is not guaranteed to be consistent with retrieved relevant passages gao et al 2023 since the models are not explicitly trained to leverage and follow facts from provided passages this work introduces selfreflective retrievalaugmented generation  selfragto improve an llms generation quality including its factual accuracy without hurting its versatility via ondemand retrieval and selfreflection we train an arbitrary lm in an endtoend manner to learn to reflect on its own generation process given a task input by generating both task output and intermittent special tokens ie reflection tokens  reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively figure 1 right in particular given an input prompt and preceding generations selfragfirst determines if augmenting the continued generation with retrieved passages would be helpful if so it outputs a retrieval token that calls a retriever model on demand step 1 subsequently selfragconcurrently processes multiple retrieved passages evaluating their relevance and then generating corresponding task outputs step 2 it then generates critique tokens to criticize its own output and choose best one step 3 in terms of factuality and overall quality this process differs from conventional rag figure 1 left which 1our code and trained models are available at httpsselfraggithubio  1arxiv231011511v1 cscl 17 oct 2023\\n\\nragnarÃ¶k a reusable rag framework and baselines for trec 2024 retrievalaugmented generation track ronak pradeep university of waterloo waterloo canadanandan thakur university of waterloo waterloo canadasahel sharifymoghaddam university of waterloo waterloo canadaeric zhang university of waterloo waterloo canada ryan nguyen university of waterloo waterloo canadadaniel campos snowflake inc new york usanick craswell microsoft seattle usajimmy lin university of waterloo waterloo canada abstract did you try out the new bing search or maybe you fiddled around with google ai overviews these might sound familiar because the modernday search stack has recently evolved to include retrieval augmented generation rag systems they allow searching and incorporating realtime data into large language models llms to provide a wellinformed attributed concise summary in contrast to the traditional search paradigm that relies on displaying a ranked list of documents therefore given these recent advancements it is crucial to have an arena to build test visualize and systematically evaluate ragbased search systems with this in mind we propose the trec 2024 rag track to foster innovation in evaluating rag systems in our work we lay out the steps weve made towards making this track a reality  we describe the details of our reusable framework ragnarÃ¶k explain the curation of the new ms marco v21 collection choice release the development topics for the track and standardize the io definitions which assist the end user next using ragnarÃ¶k we identify and provide key industrial baselines such as openais gpt4o or coheres command r further we introduce a webbased user interface for an interactive arena allow ing benchmarking pairwise rag systems by crowdsourcing we opensource our ragnarÃ¶k framework and baselines to achieve a unified standard for future rag systems httpsgithubcomcastoriniragnarok 1 introduction retrieval augmented generation rag  10212229 has emerged as a popular technique to augment large language model llm gen eration for knowledgeintensive tasks such as opendomain ques tion answering or fact verification  44 using the top ð‘˜retrieved segments from a suitable retrieval system rag systems output an answer summary grounded on the relevant context rag systems mitigate factual inconsistencies in llm outputs  19262934 and enhance interpretability  21 and generalization  20 thus facilitat ing a wider adoption across several domains like medicine  55 and finance 23 several companies provide endtoend rag frameworks such as bing search  39 or google gemini  5 most of these systems are either proprietary or offer limited user customization likewise the absence of a standardized rag framework makes implementing both authors contributed equally to the paper correspondence to ronak pradeep rpradeepuwaterlooca and nandan thakur nandanthakuruwaterloocarag at a large scale challenging implementing atop existing frame works requires custom code for multiple steps including retrieval reranking and generation to promote wider adoption of rag in academia we develop ragnarÃ¶k a userfriendly reusable end toend rag framework offering code for customizable retrievers rerankers and generation models ragnarÃ¶k comprises two key modules r retrieval and ag augmented generation the retrieval module incorporates both the retrieval and reranking stages to yield the top ð‘˜retrieved seg ments for an input user topic next the augmented generation module uses the userprovided topic and retrieved segments to produce an rag answer formatted into individual sentences cit ing the relevant information from the top ð‘˜retrieved segments ragnarÃ¶k is deeply integrated with existing python frameworks such as pyserini 31 and rank_llm 4647 and can be easily in stalled via pypi using  pip install pyragnarok  the framework offers easytouse rest apis and an integrated webui to enhance userfriendliness and improve the human evaluation experience the ragnarÃ¶k framework will be used for providing baselines in the upcoming trec 2024 retrieval augmented generation rag track1an ideal framework should include a sufficiently large doc ument collection covering diverse information and nonfactoid decompositional topics requiring longform answers in our frame work we deduplicate the existing ms marco v2 document col lection in addition we provide a segment collection using a slidingwindow chunking technique discussed in section 4 fur ther we release two sets of development topics i trecraggy 2024 a filtered subset of topics with longform answers from trec deep learning 202123  1517 and ii trecresearchy 2024 a subset of the researchy questions introduced in rosset et al 51 our ragnarÃ¶k framework supports a headtohead rag battle arena for the answer evaluation heavily inspired by recent work such as the chatbot arena  1358 we include key industrial base lines such as cohere command r  14 and openai gpt4o  41 and evaluate both the baselines using the retrieval setup involving bm25  49 and rankzephyr  47 with human preferences overall we observe gpt4o to provide more detailed answers over com mand r on the development set of topics discussed in section 6 finally we opensource ragnarÃ¶k and make it publicly available at the following url httpsgithubcomcastoriniragnarok in the future we will include a wider variety of llms as baselines and continue to improve our framework 1trec 2024 retrieval augmented generation rag track httpstrecraggithubioarxiv240616828v1 csir 24 jun 2024\\n\\nragnarÃ¶k a reusable rag framework and baselines for trec 2024 retrievalaugmented generation track conference17 july 2017 washington dc usa 4samuel joseph amouyal ohad rubin ori yoran tomer wolfson jonathan herzig and jonathan berant 2022 qampari  an opendomain question answering benchmark for questions with many answers from multiple para graphs corr abs220512665 2022 httpsdoiorg1048550arxiv220512665 arxiv220512665 5rohan anil sebastian borgeaud yonghui wu jeanbaptiste alayrac jiahui yu radu soricut johan schalkwyk andrew m dai anja hauth katie mil lican david silver slav petrov melvin johnson ioannis antonoglou julian schrittwieser amelia glaese jilin chen emily pitler timothy p lillicrap ange liki lazaridou orhan firat james molloy michael isard paul ronald barham tom hennigan benjamin lee fabio viola malcolm reynolds yuanzhong xu ryan doherty eli collins clemens meyer eliza rutherford erica moreira ka reem ayoub megha goel george tucker enrique piqueras maxim krikun iain barr nikolay savinov ivo danihelka becca roelofs anaÃ¯s white an ders andreassen tamara von glehn lakshman yagati mehran kazemi lu cas gonzalez misha khalman jakub sygnowski and et al 2023 gemini a family of highly capable multimodal models corr abs231211805 2023 httpsdoiorg1048550arxiv231211805 arxiv231211805 6negar arabzadeh and charles l a clarke 2024 a comparison of methods for evaluating generative ir corr abs240404044 2024 httpsdoiorg1048550 arxiv240404044 arxiv240404044 7akari asai zeqiu wu yizhong wang avirup sil and hannaneh hajishirzi 2023 selfrag learning to retrieve generate and critique through self reflection corr abs231011511 2023 httpsdoiorg1048550arxiv2310 11511 arxiv231011511 8payal bajaj daniel campos nick craswell li deng jianfeng gao xiaodong liu rangan majumder andrew mcnamara bhaskar mitra tri nguyen mir rosenberg xia song alina stoica saurabh tiwary and tong wang 2016 ms marco a human generated machine reading comprehension dataset corr abs161109268 2016 arxiv161109268 httparxivorgabs161109268 9valeria bolotova vladislav blinov falk scholer w bruce croft and mark sander son 2022 a nonfactoid questionanswering taxonomy in sigir 22 the 45th international acm sigir conference on research and development in information retrieval madrid spain july 11  15 2022  enrique amigÃ³ pablo castells julio gonzalo ben carterette j shane culpepper and gabriella kazai eds acm 11961207 httpsdoiorg10114534774953531926 10 sebastian borgeaud arthur mensch jordan hoffmann trevor cai eliza ruther ford katie millican george van den driessche jeanbaptiste lespiau bogdan damoc aidan clark diego de las casas aurelia guy jacob menick roman ring tom hennigan saffron huang loren maggiore chris jones albin cassirer andy brock michela paganini geoffrey irving oriol vinyals simon osindero karen simonyan jack w rae erich elsen and laurent sifre 2022 improving language models by retrieving from trillions of tokens in international con ference on machine learning icml 2022 1723 july 2022 baltimore maryland usa proceedings of machine learning research vol 162  kamalika chaudhuri stefanie jegelka le song csaba szepesvÃ¡ri gang niu and sivan sabato eds pmlr 22062240 httpsproceedingsmlrpressv162borgeaud22ahtml 11 andrei z broder 1997 on the resemblance and containment of documents incompression and complexity of sequences 1997 positano amalfitan coast salerno italy june 1113 1997 proceedings  bruno carpentieri alfredo de santis ugo vaccaro and james a storer eds ieee 2129 httpsdoiorg101109 sequen1997666900 12 harrison chase 2022 langchain  httpsgithubcomlangchainailangchain 13 weilin chiang lianmin zheng ying sheng anastasios nikolas angelopoulos tianle li dacheng li hao zhang banghua zhu michael i jordan joseph e gonzalez and ion stoica 2024 chatbot arena an open platform for evaluating llms by human preference corr abs240304132 2024 httpsdoiorg10 48550arxiv240304132 arxiv240304132 14 cohere 2024 introducing command r a scalable llm built for business  https coherecomblogcommandrplusmicrosoftazure 15 nick craswell bhaskar mitra emine yilmaz daniel campos and jimmy lin 2021 overview of the trec 2021 deep learning track in proceedings of the thirtieth text retrieval conference trec 2021 online november 1519 2021 nist special publication vol 500335  ian soboroff and angela ellis eds national institute of standards and technology nist httpstrecnistgovpubstrec30 papersoverviewdlpdf 16 nick craswell bhaskar mitra emine yilmaz daniel campos jimmy lin ellen m voorhees and ian soboroff 2022 overview of the trec 2022 deep learning track in proceedings of the thirtyfirst text retrieval conference trec 2022 online november 1519 2022 nist special publication vol 500338  ian soboroff and angela ellis eds national institute of standards and technology nist httpstrecnistgovpubstrec31papersoverview_deeppdf 17 nick craswell\\n\\nragnarÃ¶k a reusable rag framework and baselines for trec 2024 retrievalaugmented generation track conference17 july 2017 washington dc usa table 1 comparison of document and segment counts be tween versions v2 and v21 our version after removing near duplicates of the ms marco collection collection version v2 version v21 ours ms marco document 11959635 10960555 ms marco segment 124131414 113520750 4 document collection the ms marco v2 document collection earlier used in the trec dl tracks contains a substantial overlap of nearduplicates docu ments with sufficiently similar text information within the collec tion  1617 when left intact these nearduplicates degrade the downstream retrieval accuracy and reduce the diversity of the col lected documents potentially impacting the effectiveness of rag systems in addition chunking which breaks down a long verbose document into smaller compact representations is a key challenge in rag as the retrieved chunk representations correlate with the rag answer quality 34 ms marco v21 document collection we conduct a deduplica tion strategy in the ms marco v2 document collection to avoid nearduplicates in two stages in the first stage we establish an equivalence class of the documents using locality sensitive hashing lsh with minhash  11 and 9gram shingles next we selected a representative document for each equivalence class for our refined ms marco v21 document collection2reducing the duplicates in the original ms marco v2 document collection by 835 as shown in table 1 ms marco v21 segment collection we segment the ms marco v21 document collection into overlapping segments or chunks and develop the ms marco v21 segment collection3with more than 113 million text segments table 1 we utilize a sliding window technique to generate the segments by fixing the sliding window size of 10 sentences and a stride of 5 sentences to create each seg ment roughly on average between 5001000 characters long to easily map each segment back to the document every segment contains the document id within the segment id further two new fields start_char andend_char indicate the start and the end position character of where the segment begins and ends in the mapped ms marco v21 document collection 5 topic collection topics ie user queries are crucial for robust evaluation of rag systems traditionally popular retrieval and traditional qa bench marks primarily consist of factoid queries where answers are typi cally found within a single sentence or paragraph however these topics lack complexity leading to short answers that can be easily memorized by llms for instance ms marco  8 surprisingly con tains up to 55 factoid queries  951 to avoid shortform answers in rag we utilize two collections containing nonfactoid topics covering information about diverse topics and requiring longform answers we describe these collections below 2ms marco v21 document collection msmarco_v21_doctar 3ms marco v21 segment collection msmarco_v21_doc_segmentedtartable 2 trecraggy and trecresearchy 2024 topic dis tribution the table shows the top 5categories in topic clas sification for trecraggy intrinsic attributes for trec researchy and the first word in all topics definitions in more detail can be found in appendix a  b trecraggy 2024 trecresearchy 2024 topic category first word  intrinsic attributes first word  aggregation 242 what 375 knowledgeintensive 798 how 410 simple w cond 233 how 275 multifaceted 757 why 255 set 208 why 33 reasoningintensive 755 what 150 simple 100 is 25 subjective 485 is 52 comparison 67 when 17 assumptive 257 should 22 trecraggy 2024 we develop trecraggy 2024 a collection with topics filtered from trec deep learning 20212023 tracks  15 17 based on topic category and generatedanswer classification we classify each available topic into seven categories described in appendix a and filter out a subset of topics that either have a long form answer or require information aggregation across multiple sources of information out of the 210 original topics available we filter and include 120 topics 571 in the trecraggy 2024 topic collection4from table 2 we observe 242 of the topics included are aggregation indicating rag systems require to aggregate information from multiple retrieved segments to generate an accu rate longform answer similarly 65 of the topics start with what or how overall a majority of the topics are useful for evaluation containing diverse topic categories requiring a longform answer trecresearchy 2024 researchy questions introduced in ros set et al  51  contains 102k nonfactoid topics with longform answers these topics were curated from bing search logs and eval uated by gpt4 on a scale of 010 based on eight intrinsic attributes such as subjectivity and multifacetedness definitions provided in appendix b notably unlike trecraggy 2024 these queries lack relevance judgments to curate a smaller development subset for a faster evaluation of rag systems we employ a sampler designed to maximize diversity based on the eight intrinsic attributes this is achieved by iteratively selecting the query with the highest ð‘™1 norm in the intrinsic attribute space of all eight dimensions rela tive to the alreadysampled set the resultant topic set we dub as trecresearchy 20245from table 2 about 80 of the topics are knowledgeintensive and about 76 are multifaceted highlighting the need for effective rag systems additionally 665 of topics start with how or why emphasizing explanatory questions these distributions suggest that trecresearchy 2024 prioritizes complex and multidimensional topics 6 trec 2024 rag baselines retrieval our retrieval module integrates both firststage retriev ers and rerankers we use bm25 available in anserini  57 with the following default parameters  ð‘˜109andð‘04 to retrieve the top 100segments for a given topic next rankzephyr  47 a stateoftheart listwise reranker is used to rerank the top 100can didates we use rankzephyr ðœŒ a variant that reranks the candidates progressively ie makes three passes iteratively refining the final 4trecraggy 2024 topic collection topicsrag24raggydevtxt 5trecresearchy 2024 topic collection topicsrag24researchydevtxt\\n\\nconference17 july 2017 washington dc usa pradeep and thakur et al ranked candidate list to achieve better precision an easytouse implementation of rankzephyr is available via the rank_llm pack age along with various other rerankers like rankgpt  53 which we provide as secondary baselines finally the top 20reranked documents from the document collection are passed onto the next stage for rag generation augmented generation our generation module integrates two popular and commercially available llms i command r is coheres instruction following llm developed for complex rag pipelines  14 ii gpt4o is the latest gpt version from ope nai  41 given that command r cites in a span level we map the citations to their parent sentences for gpt4o we follow the zeroshot chatqa prompt template  35 and cite relevant segments within the text inline using the ieee format an example of the prompt template is shown in figure 2 in the appendix ragbench evaluation evaluating different rag answers is chal lenging as multiple factors within the output response are crucial for effectiveness evaluation to combat this recent works rely on an llmasajudge setup  58 where strong llm assessors judge the raggenerated output in a pairwise evaluation style sidebyside in a headon tournament in our work we briefly overview our baseline techniques using human evaluators a complete illustra tion can be found in table 4 in the appendix the command r baseline outputs shorter answers and cites more relevant segments whereas the gpt4o baseline outputs longer and more detailed answers and cites fewer segments therefore for topics in both trecraggy and trecresearchy 2024 gpt4o intuitively is the better choice for rag answer generation we leave it for future work to empirically compute the win rates in  between our baselines in the ragbench evaluation 61 ragnarÃ¶k system arena heavily inspired by the success of chatbot arena  1358 a crowd sourcing benchmark webui featuring anonymous battles we ex tend the concept to multistage configurable rag pipelines with ragnarÃ¶k in the arena users interact with two unblindedblinded rag systems simultaneously issuing the same topic to both the participants evaluate and select the pipeline that delivers their most preferred response with the identities of the modules in the endto end pipeline revealed after the voting process in the blinded case we leverage gradio  1 to build the webui for ragnarÃ¶k each step of the pipeline uses rest apis for intercommunication enabling easy module switching within the pipeline this modular design simplifies the integration of different retrieval and llm configura tions enhancing scalability and maintainability figure 3 in the appendix illustrates an example topic what inspired pink floyds the wall processed by two different pipelines pipeline a comprising bm25 rankzephyrgpt4o left and pipeline b comprising bm25 rankgpt4ocommand r right in the unblinded tab the outputs generated by each pipeline are compared allowing users to discern which system provided a more satisfactory answer note that when the user hovers the mouse over a citation they can preview the cited segment further in appendix c we discuss the blinded pairwise evaluation and the responses json output tab available in the webui for ragnarÃ¶k7 ongoing work ragnarÃ¶k is the first step for the ongoing work in the trec 2024 rag track by releasing the document collections development topics and baseline strategies for participants we will continue to update the pipelines to include more diverse retrieval models including stateoftheart dual encoders such as articembed  38 and effective pointwisepairwise rerankers  45 we plan to add additional support for more advanced rag techniques like self rag  7 and crag  56 for the trec 2024 rag track test topics we plan to conduct a new and fresh scrape of the bing search logs closer to the submission period this approach will compile a fresh and recent set of topics similar to rosset et al  51  thereby mini mizing the risk of data leakage and ensuring a fair evaluation with existing commercially available llms the next phase of our efforts will focus on finalizing the eval uation methodology using an automatic nuggetbased evaluation following earlier work in lin and demnerfushman 30 and first discussed in the trec rag 2024 presentation deck6the nugget based evaluation is recently gaining popularity  263748 and is becoming the de facto strategy for rag evaluation 8 conclusion the emergence of retrievalaugmented generation rag has rev olutionized modern search systems by allowing realtime data in corporation into large language models llms in our work we develop a reusable endtoend framework ragnarÃ¶k providing reproducible baselines and a webui serving a rag battle arena for retriever reranker and generation models we also introduce the ms marco v21 collection carefully curated topics from the trecdl 20212023 queries and researchy questions and io defi nitions to assist users in the rag paradigm additionally the paper identifies key industrial baselines such as coheres command r and openais gpt4o and includes a qualitative analysis of the baselines on the development topics by opensourcing this frame work we aim to standardize rag applications in preparation for the upcoming trec 2024 rag challenge acknowledgments we thank ian soboroff for the ms marco v2 document collection deduplication for our trec 2024 rag track cohere for providing us with the necessary credits to evaluate commandr and mi crosoft for providing azure credits to evaluate gpt4o additionally we thank corby rosset for the discussions surrounding researchy questions 51 references 1abubakar abid ali abdalla ali abid dawood khan abdulrahman alfozan and james y zou 2019 gradio hasslefree sharing and testing of ml models in the wild corr abs190602569 2019 arxiv190602569 httparxivorgabs 190602569 2marwah alaofi negar arabzadeh charles l a clarke and mark sanderson 2024 generative information retrieval evaluation corr abs240408137 2024 httpsdoiorg1048550arxiv240408137 arxiv240408137 3mohammad aliannejadi zahra abbasiantaeb shubham chatterjee jeffery dal ton and leif azzopardi 2024 trec ikat 2023 the interactive knowledge assistance track overview corr abs240101330 2024 httpsdoiorg10 48550arxiv240101330 arxiv240101330 6httpscsuwaterlooca jimmylinpublicationslin_etal_trec2023planningpdf\\n\\n    Query: what is self-RAG?\\n\\n    Please follow these guidelines in your response:\\n    1. Start with a brief overview of the concept mentioned in the query.\\n    2. Provide at least one concrete example or use case to illustrate the concept.\\n    3. If there are any limitations or challenges associated with this concept, briefly mention them.\\n    4. Conclude with a sentence about the potential future impact or applications of this concept.\\n\\n    Your explanation should be informative yet accessible, suitable for someone with a basic understanding of RAG.\\n    If the query asks for information not present in the context, please state that you don't have enough information to provide a complete answer,\\n    and only respond based on the given context.\\n\\n    Response:\\n    \""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import TreeSummarize\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.query_engine import CustomQueryEngine\n",
    "from retriever_agent import RetrieverAgent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core.response_synthesizers import BaseSynthesizer\n",
    "from llama_index.core.tools import FunctionTool\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def prompt_template():\n",
    "    \"\"\"\n",
    "    Define the prompt template for generating explanations based on the context and query.\n",
    "    \"\"\"\n",
    "    prompt_str = \"\"\"\n",
    "    You are an AI assistant specializing in explaining complex topics related to Retrieval-Augmented Generation(RAG).\n",
    "    Your task is to provide a clear, concise, and informative explanation based on the following context and query.\n",
    "\n",
    "    Context:\n",
    "    {context_str}\n",
    "\n",
    "    Query: {query_str}\n",
    "\n",
    "    Please follow these guidelines in your response:\n",
    "    1. Start with a brief overview of the concept mentioned in the query.\n",
    "    2. Provide at least one concrete example or use case to illustrate the concept.\n",
    "    3. If there are any limitations or challenges associated with this concept, briefly mention them.\n",
    "    4. Conclude with a sentence about the potential future impact or applications of this concept.\n",
    "\n",
    "    Your explanation should be informative yet accessible, suitable for someone with a basic understanding of RAG.\n",
    "    If the query asks for information not present in the context, please state that you don't have enough information to provide a complete answer,\n",
    "    and only respond based on the given context.\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    "    prompt_tmpl = PromptTemplate(prompt_str)\n",
    "    return prompt_tmpl\n",
    "\n",
    "def prompt_generation(state):\n",
    "    \"\"\"\n",
    "    Generate the prompt for the given search type, query, and reranking model.\n",
    "    \"\"\"\n",
    "    state = state\n",
    "    retriever_agent = Retriever(state)\n",
    "    reranked_documents = retriever_agent.retriever()\n",
    "\n",
    "    context = \"\\n\\n\".join(reranked_documents)\n",
    "    query = state.get('query')\n",
    "    prompt_templ = prompt_template().format(context_str=context, query_str=query)\n",
    "\n",
    "    return prompt_templ\n",
    "\n",
    "class RAGStringQueryEngine(CustomQueryEngine):\n",
    "    llm: OpenAI\n",
    "    response_synthesizer: BaseSynthesizer\n",
    "\n",
    "    def custom_query(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response for the given prompt using the LLM and response synthesizer.\n",
    "        \"\"\"\n",
    "        response = self.llm.complete(prompt)\n",
    "        summary = self.response_synthesizer.get_response(query_str=str(response), text_chunks=str(prompt))\n",
    "\n",
    "        return str(summary)\n",
    "    \n",
    "def create_query_engine(prompt: str):\n",
    "    \"\"\"\n",
    "    Create a query engine for generating responses based on the given prompt.\n",
    "    \"\"\"\n",
    "    llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "    response_synthesizer = TreeSummarize(llm=llm)\n",
    "\n",
    "    query_engine = RAGStringQueryEngine(\n",
    "        llm=llm,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "    )\n",
    "    response = query_engine.query(prompt)\n",
    "    return response.response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GenerationAgent(state: dict) -> OpenAIAgent:\n",
    "    \"\"\"\n",
    "    Define the GenerationAgent for generating explanations based on the user's query, search type, and reranking model.\n",
    "    \"\"\"\n",
    "\n",
    "    def generation(state):\n",
    "        \"\"\"\n",
    "        Generate an explanation based on the given search type, query, and reranking model.\n",
    "        \"\"\"\n",
    "        prompt = prompt_generation(state)\n",
    "        print(\"Passing the ReRanked documents to the LLM\")\n",
    "        response = create_query_engine(prompt)\n",
    "        print(\"Retrieved the summarized respopnse from LLMs\")\n",
    "        return str(response)\n",
    "\n",
    "    def done() -> None:\n",
    "        \"\"\"\n",
    "        Signal that the retrieval process is complete and update the state.\n",
    "        \"\"\"\n",
    "        print(\"Retrieval process is complete and updating the state\")\n",
    "        state[\"current_speaker\"] = None\n",
    "        state[\"just_finished\"] = True\n",
    "\n",
    "    tools = [\n",
    "        FunctionTool.from_defaults(fn=generation),\n",
    "        FunctionTool.from_defaults(fn=done),\n",
    "    ]\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a helpful assistant that is performing search and retrieval tasks for a retrieval-augmented generation (RAG) system.\n",
    "    Your task is to retrieve documents based on the user's query, search type, and reranking model.\n",
    "    To do this, you need to know the search type, query, and reranking model.\n",
    "    You can ask the user to supply these details.\n",
    "    If the user supplies the necessary information, then call the tool \"generation\" using the provided details to perform the search and retrieval process.\n",
    "    The current user state is:\n",
    "    {pprint.pformat(state, indent=4)}\n",
    "    When you have completed the retrieval process, call the tool \"done\" to signal that you are done.\n",
    "    If the user asks to do anything other than retrieve documents, call the tool \"done\" to signal that some other agent should help.\n",
    "    \"\"\"\n",
    "\n",
    "    return OpenAIAgent.from_tools(\n",
    "        tools,\n",
    "        llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "        system_prompt=system_prompt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    " state = {   'chunk_overlap': None,\n",
    "    'chunk_size': None,\n",
    "    'current_speaker': None,\n",
    "    'embedding_model': None,\n",
    "    'input_dir': None,\n",
    "    'just_finished': False,\n",
    "    'query': 'what is self-RAG?',\n",
    "    'reranking_model': None,\n",
    "    'search_type': 'hybrid',\n",
    "    'session_token': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the search and retrieval process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44736bb52a1a4c5ba340834084cc4dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6200e9631ed845aa9f6145d0c45f9df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search and retrieval process completed\n",
      "Reranking of the retrieved documents is complete\n",
      "Passing the ReRanked documents to the LLM\n",
      "Retrieved the summarized respopnse from LLMs\n",
      "Retrieval process is complete and updating the state\n",
      "If you need any more assistance or have any other queries, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "agent = GenerationAgent(state=state)\n",
    "response = agent.chat(\"I want to query what is a RagnarÃ¶k framework? Also can you use hybrid search along with crossencoder reranking model\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='I have completed the retrieval process.', sources=[ToolOutput(content='None', tool_name='generation', raw_input={'args': (), 'kwargs': {'state': {'query': 'what is self-RAG?', 'search_type': 'hybrid', 'reranking_model': 'crossencoder'}}}, raw_output=None, is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI \n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(search_type: str, query: str, reranking_model: str):\n",
    "        \"\"\"\n",
    "        Generate an explanation based on the given search type, query, and reranking model.\n",
    "        \"\"\"\n",
    "        #prompt_gen = prompt_template_generation()\n",
    "        prompt = prompt_generation(search_type, query, reranking_model)\n",
    "        print(\"Starting the search and retrieval process\")\n",
    "        response = create_query_engine(prompt)\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the search and retrieval process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9db7f4df6424ab1bacc0e5c438afe8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85c03c1f357484db155cb0649d96c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search and retrieval process completed\n",
      "Reranking of the retrieved documents is complete\n",
      "Starting the search and retrieval process\n"
     ]
    }
   ],
   "source": [
    "response = generation(search_type, query, reranking_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Self-RAG, or Self-Reflective Retrieval-Augmented Generation, is a framework that aims to enhance the quality and factuality of responses generated by large language models (LLMs) by incorporating retrieval of relevant knowledge and self-reflection during the generation process. This approach allows the model to adaptively retrieve passages on-demand and reflect on both the retrieved information and its own generated content using special tokens called reflection tokens. An example application of Self-RAG could be in open-domain question answering, where it dynamically retrieves information to generate accurate and coherent responses. However, a potential challenge of Self-RAG is the complexity of training and fine-tuning the model to balance retrieval, generation, and self-reflection processes, as well as ensuring the quality and relevance of retrieved passages for real-world applications. Despite these challenges, Self-RAG holds promise for enhancing various knowledge-intensive tasks like fact verification, reasoning, and long-form content generation, thereby improving the overall performance and reliability of AI systems.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerationAgent(state: dict) -> OpenAIAgent:\n",
    "    \"\"\"\n",
    "    Define the GenerationAgent for generating explanations based on the user's query, search type, and reranking model.\n",
    "    \"\"\"\n",
    "    def generation(search_type: str, query: str, reranking_model: str):\n",
    "        \"\"\"\n",
    "        Generate an explanation based on the given search type, query, and reranking model.\n",
    "        \"\"\"\n",
    "        #prompt_gen = prompt_template_generation()\n",
    "        prompt = prompt_generation(search_type, query, reranking_model)\n",
    "        print(\"Starting the search and retrieval process\")\n",
    "        response = create_query_engine(prompt)\n",
    "        print(\"Retrieved the respopnse from LLMs\")\n",
    "        return print(response)\n",
    "\n",
    "    def done() -> None:\n",
    "        \"\"\"\n",
    "        Signal that the retrieval process is complete and update the state.\n",
    "        \"\"\"\n",
    "        print(\"Retrieval process is complete and updating the state\")\n",
    "        state[\"current_speaker\"] = None\n",
    "        state[\"just_finished\"] = True\n",
    "\n",
    "    tools = [\n",
    "        FunctionTool.from_defaults(fn=generation),\n",
    "        FunctionTool.from_defaults(fn=done),\n",
    "    ]\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a helpful assistant that is performing search and retrieval tasks for a retrieval-augmented generation (RAG) system.\n",
    "    Your task is to retrieve documents based on the user's query, search type, and reranking model.\n",
    "    To do this, you need to know the search type, query, and reranking model.\n",
    "    You can ask the user to supply these details.\n",
    "    If the user supplies the necessary information, then call the tool \"generation\" using the provided details to perform the search and retrieval process.\n",
    "    The current user state is:\n",
    "    {pprint.pformat(state, indent=4)}\n",
    "    When you have completed the retrieval process, call the tool \"done\" to signal that you are done.\n",
    "    If the user asks to do anything other than retrieve documents, call the tool \"done\" to signal that some other agent should help.\n",
    "    \"\"\"\n",
    "\n",
    "    return OpenAIAgent.from_tools(\n",
    "        tools,\n",
    "        llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "        system_prompt=system_prompt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\pavan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from document_pre_processing_agent import DocumentPreprocessingAgent\n",
    "from indexing_agent import QdrantIndexingAgent\n",
    "from generation_agent import GenerationAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent(agent_name, state):\n",
    "    agents = {\n",
    "        \"Data_pre_processing\": DocumentPreprocessingAgent,\n",
    "        \"Indexing\": QdrantIndexingAgent,\n",
    "        \"Generation\": GenerationAgent,\n",
    "        #\"Concierge\": continuation_agent_factory,\n",
    "        # Add other agents here\n",
    "    }\n",
    "    return agents.get(agent_name, None)(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.agent.openai.base.OpenAIAgent at 0x21494f37a10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = {}\n",
    "agent = get_agent(\"Generation\", state=state)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response='I encountered an error while trying to finalize the process. I will attempt to resolve this issue.', sources=[ToolOutput(content=\"Error: 'OpenAIAgent' object has no attribute 'retriever'\", tool_name='generation', raw_input={'kwargs': {'search_type': 'Hybrid', 'query': 'what is self-RAG?', 'reranking_model': 'CrossEncoder'}}, raw_output=AttributeError(\"'OpenAIAgent' object has no attribute 'retriever'\"), is_error=False), ToolOutput(content=\"Error: 'OpenAIAgent' object has no attribute 'retriever'\", tool_name='generation', raw_input={'kwargs': {'search_type': 'Hybrid', 'query': 'what is self-RAG?', 'reranking_model': 'CrossEncoder'}}, raw_output=AttributeError(\"'OpenAIAgent' object has no attribute 'retriever'\"), is_error=False), ToolOutput(content=\"Error: name 'logging' is not defined\", tool_name='done', raw_input={'kwargs': {}}, raw_output=NameError(\"name 'logging' is not defined\"), is_error=False), ToolOutput(content=\"Error: 'OpenAIAgent' object has no attribute 'retriever'\", tool_name='generation', raw_input={'kwargs': {'search_type': 'Hybrid', 'query': 'what is self-RAG?', 'reranking_model': 'CrossEncoder'}}, raw_output=AttributeError(\"'OpenAIAgent' object has no attribute 'retriever'\"), is_error=False), ToolOutput(content=\"Error: 'OpenAIAgent' object has no attribute 'retriever'\", tool_name='generation', raw_input={'kwargs': {'search_type': 'Hybrid', 'query': 'what is self-RAG?', 'reranking_model': 'CrossEncoder'}}, raw_output=AttributeError(\"'OpenAIAgent' object has no attribute 'retriever'\"), is_error=False), ToolOutput(content=\"Error: name 'logging' is not defined\", tool_name='done', raw_input={'kwargs': {}}, raw_output=NameError(\"name 'logging' is not defined\"), is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#response = agent.chat(user_msg_str, chat_history=current_history)\n",
    "response = agent.chat(\"I want to query what is self-RAG? with hybrid search and following with CrossEncoder Reranking model\")\n",
    "response   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the search and retrieval process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d56ae9a33654b159f2d1288c6b9019a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7e712f52864f129bb65a1be8333abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search and retrieval process completed\n",
      "Reranking of the retrieved documents is complete\n",
      "Starting the search and retrieval process\n",
      "Retrieved the respopnse from LLMs\n",
      "Self-RAG, or Self-Reflective Retrieval-Augmented Generation, is a framework that aims to enhance the quality and factuality of large language models by incorporating retrieval of relevant knowledge and self-reflection during the generation process. This approach allows the model to adaptively retrieve passages on-demand, generate responses based on this retrieved knowledge, and reflect on both the retrieved information and its own generated content using special tokens called reflection tokens. The goal is to improve the accuracy and overall quality of the model's responses, particularly in tasks like open-domain question answering, by iteratively enhancing factual accuracy and response quality through self-reflection.\n",
      "I couldn't find any relevant information on \"self-RAG\" using the hybrid search with the crossencoder reranking model. If you have any other queries or need assistance with something else, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state = {}\n",
    "agent = GenerationAgent(state=state)\n",
    "response = agent.chat(\"Can you use hybrid search to query what is self-RAG? and with crossencoder Reranking model\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you need any more assistance or have any other queries, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "import pprint\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai import OpenAI \n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from document_pre_processing_agent import DocumentPreprocessingAgent\n",
    "from indexing_agent import QdrantIndexingAgent\n",
    "from generation_agent import GenerationAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Speaker(str, Enum):\n",
    "    Data_pre_processing = \"data_pre_processing\"\n",
    "    Indexing = \"indexing\"\n",
    "    Retriever = \"retriever\"\n",
    "    ReRanking = \"reranking\"\n",
    "    Generation = \"generation\"\n",
    "    Concierge = \"Concierge\"\n",
    "    ORCHESTRATOR = \"orchestrator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concierge_agent_factory(state: dict) -> OpenAIAgent:\n",
    "    def dummy_tool() -> bool:\n",
    "        \"\"\"A tool that does nothing.\"\"\"\n",
    "        print(\"Doing nothing.\")\n",
    "\n",
    "    tools = [\n",
    "        FunctionTool.from_defaults(fn=dummy_tool)\n",
    "    ]\n",
    "\n",
    "    system_prompt = (f\"\"\"\n",
    "        You are a helpful assistant that is helping a user navigate the process of querying and indexing their documents using this customizable RAG application.\n",
    "        Your job is to ask the user questions to figure out what they want to do, and give them the available scenario's.\n",
    "        That includes:\n",
    "        * pre-processing and indexing the documents/files into Qdrant vector database using user preferred chunking strategies and embedding models.\n",
    "        * generating a response to the user query using user preferred search type and reranking model.\n",
    "\n",
    "        The current state of the user is:\n",
    "        {pprint.pformat(state, indent=4)}\n",
    "    \"\"\")\n",
    "\n",
    "    return OpenAIAgent.from_tools(\n",
    "        tools,\n",
    "        llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "        system_prompt=system_prompt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuation_agent_factory(state: dict) -> OpenAIAgent:\n",
    "    def dummy_tool() -> bool:\n",
    "        \"\"\"A tool that does nothing.\"\"\"\n",
    "        print(\"Doing nothing.\")\n",
    "\n",
    "    tools = [\n",
    "        FunctionTool.from_defaults(fn=dummy_tool)\n",
    "    ]\n",
    "\n",
    "    system_prompt = (f\"\"\"\n",
    "        The current state of the user is:\n",
    "        {pprint.pformat(state, indent=4)}\n",
    "    \"\"\")\n",
    "\n",
    "    return OpenAIAgent.from_tools(\n",
    "        tools,\n",
    "        llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.4),\n",
    "        system_prompt=system_prompt,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def orchestration_agent_factory(state: dict) -> OpenAIAgent:\n",
    "    def has_input_dir() -> bool:\n",
    "        \"\"\"Useful for checking if the user has specified an input file directory.\"\"\"\n",
    "        print(\"Orchestrator checking if input file directory is specified\")\n",
    "        return (state[\"input_dir\"] is not None)\n",
    "\n",
    "    def has_chunk_size() -> bool:\n",
    "        \"\"\"Useful for checking if the user has specified a chunk size.\"\"\"\n",
    "        print(\"Orchestrator checking if chunk size is specified\")\n",
    "        return (state[\"chunk_size\"] is not None)\n",
    "\n",
    "    def has_chunk_overlap() -> bool:\n",
    "        \"\"\"Useful for checking if the user has specified a chunk overlap.\"\"\"\n",
    "        print(\"Orchestrator checking if chunk overlap is specified\")\n",
    "        return (state[\"chunk_overlap\"] is not None)\n",
    "\n",
    "    def has_embedding_model() -> bool:\n",
    "        \"\"\"Useful for checking if the user has specified an embedding model.\"\"\"\n",
    "        print(\"Orchestrator checking if embedding model is specified\")\n",
    "        return (state[\"embedding_model\"] is not None)\n",
    "\n",
    "    def has_reranking_model() -> bool:\n",
    "        \"\"\"Useful for checking if the user has specified a reranking model.\"\"\"\n",
    "        print(\"Orchestrator checking if reranking model is specified\")\n",
    "        return (state[\"reranking_model\"] is not None)\n",
    "\n",
    "    def has_search_type() -> bool:\n",
    "        \"\"\"Useful for checking if the user has specified a search type.\"\"\"\n",
    "        print(\"Orchestrator checking if search type is specified\")\n",
    "        return (state[\"search_type\"] is not None)    \n",
    "\n",
    "    def has_query() -> bool:\n",
    "        \"\"\"Useful for checking if the user has specified query.\"\"\"\n",
    "        print(\"Orchestrator checking if query is specified\")\n",
    "        return (state[\"query\"] is not None)  \n",
    "\n",
    "    tools = [\n",
    "        FunctionTool.from_defaults(fn=has_input_dir),\n",
    "        FunctionTool.from_defaults(fn=has_chunk_size),\n",
    "        FunctionTool.from_defaults(fn=has_chunk_overlap),\n",
    "        FunctionTool.from_defaults(fn=has_embedding_model),\n",
    "        FunctionTool.from_defaults(fn=has_reranking_model),\n",
    "        FunctionTool.from_defaults(fn=has_search_type),\n",
    "        FunctionTool.from_defaults(fn=has_query),\n",
    "    ]\n",
    "\n",
    "    system_prompt =  (f\"\"\"\n",
    "    You are the orchestration agent.\n",
    "    Your job is to decide which agent to run based on the current state of the user and what they've asked to do. Agents are identified by short strings.\n",
    "    What you do is return the name of the agent to run next. You do not do anything else.\n",
    "\n",
    "    The current state of the user is:\n",
    "    {pprint.pformat(state, indent=4)}\n",
    "\n",
    "    If a current_speaker is already selected in the state, simply output that value.\n",
    "\n",
    "    If there is no current_speaker value, look at the chat history and the current state and you MUST return one of these strings identifying an agent to run:\n",
    "    * \"{Speaker.Data_pre_processing.value}\" - if the user wants to pre-process the documents into nodes\n",
    "        * If they want to pre-process the documents, but they haven't specified an input file, chunk size, or chunk overlap, return \"{Speaker.Concierge.value}\" instead\n",
    "    * \"{Speaker.Indexing.value}\" - if the user wants to embed and index the nodes into a vector database\n",
    "         * If they want to embed and index the nodes, but there is no preprocessed data, return \"{Speaker.Data_pre_processing.value}\" instead\n",
    "        * If they want to embed and index the nodes, but they haven't specified an embedding model, return \"{Speaker.Concierge.value}\" instead\n",
    "    * \"{Speaker.Generation.value}\" - if the user wants to query the documents (requires query, search type, and reranking model)\n",
    "        * If they want to query the documents, but they haven't specified the query, search type, or reranking model, return \"{Speaker.Concierge.value}\" instead\n",
    "    * \"{Speaker.Concierge.value}\" - if the user wants to do something else, or hasn't said what they want to do, or you can't figure out what they want to do. Choose this by default.\n",
    "\n",
    "    Output one of these strings and ONLY these strings, without quotes.\n",
    "    NEVER respond with anything other than one of the above strings. DO NOT be helpful or conversational.\n",
    "    \"\"\")\n",
    "\n",
    "    return OpenAIAgent.from_tools(\n",
    "        tools,\n",
    "        llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.4),\n",
    "        system_prompt=system_prompt,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_state() -> dict:\n",
    "    return {\n",
    "        \"session_token\": None,\n",
    "        \"input_dir\": None,\n",
    "        \"chunk_size\": None,\n",
    "        \"chunk_overlap\": None,\n",
    "        \"embedding_model\": None,\n",
    "        \"reranking_model\": None,\n",
    "        \"search_type\": None,\n",
    "        \"query\": None,\n",
    "        \"current_speaker\": None,\n",
    "        \"just_finished\": False,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_agent(agent_name, state):\n",
    "    agents = {\n",
    "        \"Data_pre_processing\": DocumentPreprocessingAgent,\n",
    "        \"Indexing\": QdrantIndexingAgent,\n",
    "        \"Generation\": GenerationAgent,\n",
    "        \"Concierge\": continuation_agent_factory,\n",
    "        # Add other agents here\n",
    "    }\n",
    "    return agents.get(agent_name, None)(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run() -> None:\n",
    "    state = get_initial_state()\n",
    "\n",
    "    root_memory = ChatMemoryBuffer.from_defaults(token_limit=8000)\n",
    "\n",
    "    first_run = True\n",
    "    is_retry = False\n",
    "\n",
    "    while True:\n",
    "        if first_run:\n",
    "            # if this is the first run, start the conversation\n",
    "            user_msg_str = \"Hello there!\"\n",
    "            first_run = False\n",
    "        elif is_retry == True:\n",
    "            user_msg_str = \"That's not right, try again. Pick one agent.\"\n",
    "            is_retry = False\n",
    "        elif state[\"just_finished\"] == True:\n",
    "            print(\"Asking the continuation agent to decide what to do next\")\n",
    "            user_msg_str = str(continuation_agent_factory(state).chat(\"\"\"\n",
    "                Look at the chat history to date and figure out what the user was originally trying to do.\n",
    "                They might have had to do some sub-tasks to complete that task, but what we want is the original thing they started out trying to do.\n",
    "                Formulate a sentence as if written by the user that asks to continue that task.\n",
    "                If it seems like the user really completed their task, output \"no_further_task\" only.\n",
    "            \"\"\", chat_history=current_history))\n",
    "            print(f\"Continuation agent said {user_msg_str}\")\n",
    "            if user_msg_str == \"no_further_task\":\n",
    "                user_msg_str = input(\">> \").strip()\n",
    "            state[\"just_finished\"] = False\n",
    "        else:\n",
    "            # any other time, get user input\n",
    "            user_msg_str = input(\"> \").strip()\n",
    "\n",
    "        current_history = root_memory.get()\n",
    "\n",
    "        # who should speak next?\n",
    "        if (state[\"current_speaker\"]):\n",
    "            print(f\"There's already a speaker: {state['current_speaker']}\")\n",
    "            next_speaker = state[\"current_speaker\"]\n",
    "        else:\n",
    "            print(\"No current speaker, asking orchestration agent to decide\")\n",
    "            orchestration_response = orchestration_agent_factory(state).chat(user_msg_str, chat_history=current_history)\n",
    "            next_speaker = str(orchestration_response).strip()\n",
    "\n",
    "        print(f\"Next speaker: {next_speaker}\")\n",
    "\n",
    "        agent_class = get_agent(next_speaker, state)\n",
    "        if agent_class:\n",
    "            current_speaker = agent_class\n",
    "            state[\"current_speaker\"] = next_speaker\n",
    "        else:\n",
    "            print(\"Orchestration agent failed to return a valid speaker; ask it to try again\")\n",
    "            is_retry = True\n",
    "            continue\n",
    "\n",
    "        #pretty_state = pprint.pformat(state, indent=4)\n",
    "        \n",
    "        #print(f\"State: {pretty_state}\")\n",
    "\n",
    "        # chat with the current speaker\n",
    "        response = current_speaker.chat(user_msg_str, chat_history=current_history)\n",
    "        print(Fore.MAGENTA + str(response) + Style.RESET_ALL)\n",
    "\n",
    "        # update chat history\n",
    "        new_history = current_speaker.memory.get_all()\n",
    "        root_memory.set(new_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No current speaker, asking orchestration agent to decide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next speaker: Concierge\n",
      "State: {   'chunk_overlap': None,\n",
      "    'chunk_size': None,\n",
      "    'current_speaker': 'Concierge',\n",
      "    'embedding_model': None,\n",
      "    'input_dir': None,\n",
      "    'just_finished': False,\n",
      "    'query': None,\n",
      "    'reranking_model': None,\n",
      "    'search_type': None,\n",
      "    'session_token': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mHello! How can I assist you today?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from fastembed import SparseTextEmbedding, TextEmbedding\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.http.models import PointStruct, SparseVector\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qdrant_API_KEY = os.getenv('Qdrant_API_KEY')\n",
    "Qdrant_URL = os.getenv('Qdrant_URL')\n",
    "Collection_Name = os.getenv('collection_name')\n",
    "qdrant_client = QdrantClient(\n",
    "                            url=Qdrant_URL,\n",
    "                            api_key=Qdrant_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agentic-Automation-RAG'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Collection_Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nodes():\n",
    "        metadata = []\n",
    "        documents = []\n",
    "        payload_file = r'C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\nodes.json'\n",
    "\n",
    "        try:\n",
    "            with open(payload_file, 'r') as file:\n",
    "                nodes = json.load(file)\n",
    "\n",
    "            for node in nodes:\n",
    "                metadata.append(node['metadata'])\n",
    "                documents.append(node['text'])\n",
    "\n",
    "            print(f\"Loaded {len(nodes)} the nodes from JSON file\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading nodes from JSON file: {e}\")\n",
    "            raise\n",
    "\n",
    "        return documents, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_collection():\n",
    "    \"\"\"\n",
    "    Create a collection in Qdrant vector database.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not qdrant_client.collection_exists(collection_name=Collection_Name): \n",
    "        qdrant_client.create_collection(\n",
    "            collection_name= Collection_Name,\n",
    "            vectors_config={\n",
    "                    'dense': models.VectorParams(\n",
    "                        size=384,\n",
    "                        distance = models.Distance.COSINE,\n",
    "                    )\n",
    "            },\n",
    "            sparse_vectors_config={\n",
    "                \"sparse\": models.SparseVectorParams(\n",
    "                            index=models.SparseIndexParams(\n",
    "                            on_disk=False,              \n",
    "                        ),\n",
    "                    )\n",
    "                }\n",
    "        )\n",
    "        \n",
    "    print(f\"Created collection '{Collection_Name}' in Qdrant vector database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created collection 'Agentic-Automation-RAG' in Qdrant vector database.\n"
     ]
    }
   ],
   "source": [
    "client_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_vector(sparse_embedding_model, text):\n",
    "        \"\"\"\n",
    "        Create a sparse vector from the text using SPLADE.\n",
    "        \"\"\"\n",
    "        sparse_embedding_model = sparse_embedding_model\n",
    "        # Generate the sparse vector using SPLADE model\n",
    "        embeddings = list(sparse_embedding_model.embed([text]))[0]\n",
    "\n",
    "        # Check if embeddings has indices and values attributes\n",
    "        if hasattr(embeddings, 'indices') and hasattr(embeddings, 'values'):\n",
    "            sparse_vector = models.SparseVector(\n",
    "                indices=embeddings.indices.tolist(),\n",
    "                values=embeddings.values.tolist()\n",
    "            )\n",
    "            return sparse_vector\n",
    "        else:\n",
    "            raise ValueError(\"The embeddings object does not have 'indices' and 'values' attributes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_documents(embedding_model, documents, metadata):\n",
    "        points = []\n",
    "        embedding_model = TextEmbedding(model_name=embedding_model)\n",
    "        sparse_embedding_model = SparseTextEmbedding(model_name=\"Qdrant/bm42-all-minilm-l6-v2-attentions\")\n",
    "        for i, (doc, metadata) in enumerate(tqdm(zip(documents, metadata), total=len(documents))):\n",
    "            # Generate both dense and sparse embeddings\n",
    "            dense_embedding = list(embedding_model.embed([doc]))[0]\n",
    "            sparse_vector = create_sparse_vector(sparse_embedding_model, doc)\n",
    "\n",
    "            # Create PointStruct\n",
    "            point = models.PointStruct(\n",
    "                id=i,\n",
    "                vector={\n",
    "                    'dense': dense_embedding.tolist(),\n",
    "                    'sparse': sparse_vector,\n",
    "                },\n",
    "                payload={\n",
    "                    'text': doc,\n",
    "                    **metadata  # Include all metadata\n",
    "                }\n",
    "            )\n",
    "            points.append(point)\n",
    "\n",
    "        # Upsert points\n",
    "        qdrant_client.upsert(\n",
    "            collection_name=Collection_Name,\n",
    "            points=points\n",
    "        )\n",
    "\n",
    "        print(f\"Upserted {len(points)} points with dense and sparse vectors into Qdrant vector database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing(embedding_model) -> None:\n",
    "        \"\"\"\n",
    "        Index the documents into the Qdrant vector database.\n",
    "        \"\"\"\n",
    "        print(\"Starting to load the nodes from JSON file\")\n",
    "        documents, metadata = load_nodes()\n",
    "        client_collection()\n",
    "        print(\"Creation of the Qdrant Collection is Done\")\n",
    "        insert_documents(embedding_model, documents, metadata)\n",
    "        print(\"Indexing of the nodes is complete\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to load the nodes from JSON file\n",
      "Loaded 71 the nodes from JSON file\n",
      "Created collection 'Agentic-Automation-RAG' in Qdrant vector database.\n",
      "Creation of the Qdrant Collection is Done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac61c5a771c04770b6e2c6f181cc5925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fdc83ff1fd4c4b8c529dccc3d70534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71/71 [00:16<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 71 points with dense and sparse vectors into Qdrant vector database.\n",
      "Indexing of the nodes is complete\n"
     ]
    }
   ],
   "source": [
    "indexing(embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4ab00ada5b4226965c586d2d45bba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843b9f44cf74465e91edabd320b13fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qdrant_client.set_model(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# comment this line to use dense vectors only\n",
    "qdrant_client.set_sparse_model(\"prithivida/Splade_PP_en_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Local\\Temp\\ipykernel_15400\\4029956761.py:1: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant_client.recreate_collection(\n",
      "INFO:httpx:HTTP Request: DELETE https://c77ac75e-3a41-4acc-98d2-c9c3eb11b5ea.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Hybrid_RAG_Collection \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: PUT https://c77ac75e-3a41-4acc-98d2-c9c3eb11b5ea.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Hybrid_RAG_Collection \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_client.recreate_collection(\n",
    "    collection_name=\"Hybrid_RAG_Collection\",\n",
    "    vectors_config=qdrant_client.get_fastembed_vector_params(),\n",
    "    # comment this line to use dense vectors only\n",
    "    sparse_vectors_config=qdrant_client.get_fastembed_sparse_vector_params(),  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_file = r\"C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\nodes.json\"\n",
    "metadata = []\n",
    "documents = []\n",
    "\n",
    "with open(input_file, 'r') as file:\n",
    "        nodes = json.load(file)\n",
    "\n",
    "for node in nodes:\n",
    "    metadata.append(node['metadata'])\n",
    "    documents.append(node['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/71 [00:00<?, ?it/s]INFO:httpx:HTTP Request: GET https://c77ac75e-3a41-4acc-98d2-c9c3eb11b5ea.us-east4-0.gcp.cloud.qdrant.io:6333/collections/Hybrid_RAG_Collection \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "qdrant_client.add(\n",
    "    collection_name=\"Hybrid_RAG_Collection\",\n",
    "    documents=documents,\n",
    "    metadata=metadata,\n",
    "    ids=tqdm(range(len(documents))),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "Qdrant_API_KEY = os.getenv('Qdrant_API_KEY')\n",
    "Qdrant_URL = os.getenv('Qdrant_URL')\n",
    "Collection_Name = os.getenv('Collection_Name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def query_semantic_search(query, metadata_filter=None, limit=4):\n",
    "    # Load the dense embedding model\n",
    "    embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Initialize the Qdrant client\n",
    "    qdrant_client = QdrantClient(\n",
    "        url=Qdrant_URL,\n",
    "        api_key=Qdrant_API_KEY,\n",
    "        timeout=30\n",
    "    )\n",
    "\n",
    "    # Embed the query using the dense embedding model\n",
    "    dense_query = embedding_model.encode([query]).tolist()[0]\n",
    "\n",
    "    # Perform the semantic search\n",
    "    results = qdrant_client.search(\n",
    "        collection_name=Collection_Name,\n",
    "        query_vector=dense_query,\n",
    "        query_filter=metadata_filter,\n",
    "        limit=limit,\n",
    "    )\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query_semantic_search(query = \"can you explain what is SELF-RAG means?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexing(embedding_model = 'sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DocumentPreprocessingAgent(state: dict) -> OpenAIAgent:\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def done() -> None:\n",
    "        \"\"\"When you inserted the vetors into the Qdrant Cluster, call this tool.\"\"\"\n",
    "        print(\"Indexing of the nodes is complete\")\n",
    "        state[\"current_speaker\"] = None\n",
    "        state[\"just_finished\"] = True\n",
    "\n",
    "    tools = [\n",
    "        FunctionTool.from_defaults(fn=indexing),\n",
    "        FunctionTool.from_defaults(fn=done),\n",
    "    ]\n",
    "\n",
    "    system_prompt = (f\"\"\"\n",
    "    You are a helpful assistant that is indexing documents for a retrieval-augmented generation (RAG) system.\n",
    "    Your task is to index the documents into a Qdrant cluster.\n",
    "    To do this, you need to know the embedding model to use.\n",
    "    You can ask the user to supply this.\n",
    "    If the user supplies the embedding model, call the tool \"indexing\" with this parameter to index the documents into the Qdrant cluster.\n",
    "    The current user state is:\n",
    "    {pprint.pformat(state, indent=4)}\n",
    "    When you have indexed the documents into the Qdrant cluster, call the tool \"done\" to signal that you are done.\n",
    "    If the user asks to do anything other than index the documents, call the tool \"done\" to signal some other agent should help.\n",
    "    \"\"\")\n",
    "\n",
    "    return OpenAIAgent.from_tools(\n",
    "        tools,\n",
    "        llm=OpenAI(model=\"gpt-3.5-turbo\"),\n",
    "        system_prompt=system_prompt,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
