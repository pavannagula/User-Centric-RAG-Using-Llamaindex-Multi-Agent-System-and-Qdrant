[
    {
        "id_": "05a0020e-c7b8-4e72-9ee9-df1a298418bd",
        "embedding": null,
        "metadata": {
            "page_label": "1",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "3218514f-b470-48cb-ba2c-16382123c82c",
                "node_type": "4",
                "metadata": {
                    "page_label": "1",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "645538e3502d87f5553ab8868101351ad94f45b741fffc3b7f85343e2be3f54b",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "adaptiverag learning to adapt retrievalaugmented large language models through question complexity soyeong jeong1jinheon baek2sukmin cho1sung ju hwang12jong c park1 school of computing1graduate school of ai2 korea advanced institute of science and technology12 starsuzijinheonbaeknelllpicsjhwang82jongparkkaistackr abstract retrievalaugmented large language models llms which incorporate the nonparametric knowledge from external knowledge bases into llms have emerged as a promising approach to enhancing response accuracy in several tasks such as questionanswering qa however even though there are various approaches deal ing with queries of different complexities they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multistep queries yet not all user requests fall into only one of the sim ple or complex categories in this work we propose a novel adaptive qa framework that can dynamically select the most suitable strat egy for retrievalaugmented llms from the simplest to the most sophisticated ones based on the query complexity also this selec tion process is operationalized with a classi fier which is a smaller lm trained to predict the complexity level of incoming queries with automatically collected labels obtained from actual predicted outcomes of models and in herent inductive biases in datasets this ap proach offers a balanced strategy seamlessly adapting between the iterative and singlestep retrievalaugmented llms as well as the no retrieval methods in response to a range of query complexities we validate our model on a set of opendomain qa datasets cov ering multiple query complexities and show that ours enhances the overall efficiency and accuracy of qa systems compared to rele vant baselines including the adaptive retrieval approaches code is available at https githubcomstarsuziadaptiverag  1 introduction recent large language models llms brown et al 2020 openai 2023 touvron et al 2023 anil et al 2023 have shown overwhelming per formances across diverse tasks including question corresponding author 05 10 15 20 25 30 35 time per query4748495051performance f1 no retrieval singlestep approach adaptive retrieval multistep approach adaptiverag oursperformance vs time with gpt35figure 1 qa performance f1 and efficiency timequery for different retrievalaugmented generation approaches we use the gpt35turboinstruct as the base llm answering qa yang et al 2018 kwiatkowski et al 2019 however they still generate factu ally incorrect answers since their knowledge solely relies on their parametric memory kasai et al 2022 mallen et al 2023 meanwhile memoriz ing all the everchanging world knowledge may not be possible to address this problem retrieval augmented llms borgeaud et al 2022 izacard et al 2023 shi et al 2023 which incorporate nonparametric knowledge into llms with addi tional retrieval modules have gained much increas ing attention specifically these models access a knowledge base which serves as an extensive repository of information across various subjects and disciplines to retrieve information relevant to the given input and then incorporate the retrieved information into llms which enables them to stay accurate and current with the world knowledge a particularly salient application of retrieval augmented llms is to handling qa tasks whose goal is to provide correct answers in response to user queries especially those of high complexity early work on retrievalaugmented llms focuses primarily on singlehop queries lazaridou et al 2022 ram et al 2023 whose answers are typ ically found within a single document therefore this approach involves retrieving a relevant doc ument based on the query and subsequently inte grating this information into qa models to formu late a response however unlike this singlehop qa some queries require connecting and aggregat ing multiple documents which are furthermorearxiv240314403v2 cscl 28 mar 2024",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3959,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "b430899f-f6c8-4211-a5a0-9eb8abb41482",
        "embedding": null,
        "metadata": {
            "page_label": "2",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "9a3e12cf-8f69-4fb6-bf60-352d3d245af0",
                "node_type": "4",
                "metadata": {
                    "page_label": "2",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "96be05b2562603fdb56a1d3fa08f8bf0c7450ff5bfbd12e42ab55b2ddb5a79af",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "retrievalcomplex query what currency is in billy giles birthplacesimple query when is the birthday of michael f phelpsdocuments answer documents answer a single step approach inaccurate retrieval retrievalsimple query when is the birthday of michael f phelpsdocuments intermediate answers b multi step approach inefficient k times complex query what currency is in billy giles birthplacedocuments intermediate answers k timesstraightforward query paris is the capital of whatc our adaptive approach answer simple query when is the birthday of michael f phelpsdocuments answer complex query what currency is in billy giles birthplacedocuments intermediate answers k times classifier figure 2 a conceptual comparison of different retrievalaugmented llm approaches to question answering a in response to a query this singlestep approach retrieves relevant documents and then generates an answer however it may not be sufficient for complex queries that require multistep reasoning b this multistep approach iteratively retrieves documents and generates intermediate answers which is powerful yet largely inefficient for the simple query since it requires multiple accesses to both llms and retrievers c our adaptive approach can select the most suitable strategy for retrievalaugmented llms ranging from iterative to single to even no retrieval approaches based on the complexity of given queries determined by our classifier often not answerable through a singlestep pro cess of retrievalandresponse an example query is when did the people who captured malakoff come to the region where philipsburg is located which requires four reasoning steps to solve there fore to effectively handle such complex queries recent studies have concentrated largely on multi step and multireasoning qa which requires itera tive accesses to both llms and retrievers multiple times press et al 2023 trivedi et al 2023 at the cost of heavy computational overheads yet we should rethink in a realworld scenario are all the requests from users complex instead users might often ask simple and straightforward questions while only occasionally asking complex ones specifically a query such as paris is the capital of what is likely to be asked more fre quently compared to the aforementioned multi step query and this simpler query might also be easily answered by the llms themselves without accessing external knowledge in other words a multistep qa approach could give rise to unnec essary computational overhead for simple queries even though it would be vital for complex queries see figure 2 a on the other hand handling complex queries with singlestepretrieval or even nonretrieval strategies would be largely insuffi cient figure 2 b this suggests the need for an adaptive qa system which can dynamically adjust the operational strategies of retrievalaugmented llms based on the query complexity while some recent approaches are capable of doing this based on the frequency of entities in queries mallen et al 2023 or on the generated outputs from models for multistep qa trivedi et al 2023 they are still suboptimal the former methods are overly simplistic failing to consider multihop queries meanwhile the latter are excessively complex ter minating answer solving steps after several rounds of module accessin this work considering diverse complexity lev els of realworld queries we argue that previous onesizefitsall approaches might be inadequate to cover all of them instead we propose to select the most suitable strategy from a range of retrieval augmented llms each of which is tailored to the specific complexity of the input query notably a critical step in this process is predefining the query complexity which is instrumental in deter mining the most fitting model to it in this work we operationalize this process with a novel classi fier which is a smaller model trained to predict the complexity level of incoming queries see figure 2 c moreover we automatically collect its training datasets without human labeling by leveraging the predicted outcomes ie which models accurately respond to which queries as well as by capitalizing on the inherent biases in existing datasets ie sam ples in the datasets are designed either for single step or for multistep qa scenarios this proposed method can offer a robust middle ground among the iterative llm augmentation methods for complex queries singlestep methods for simpler queries and even noretrievalaugmented methods for the most straightforward queries answerable by llms themselves thus significantly enhancing the over all efficiency and accuracy as shown in figure 1 we refer to our framework as adaptive retrieval augmented generation adaptiverag we validate adaptiverag using benchmark opendomain qa datasets covering a wide range of query complexity from singlehop rajpurkar et al 2016 joshi et al 2017 kwiatkowski et al 2019 to multihop yang et al 2018 ho et al 2020 trivedi et al 2022b queries the exper imental results show that ours significantly im proves the overall accuracy and efficiency com pared to the prior adaptive strategies on multiple llms such as gpt35 brown et al 2020 and flant5 series chung et al 2022",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 5180,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "3fb8cf83-a480-44e7-9305-00a370b2309c",
        "embedding": null,
        "metadata": {
            "page_label": "3",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "f60d843c-1a62-41b4-8f81-9e04481d8065",
                "node_type": "4",
                "metadata": {
                    "page_label": "3",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "a4e42df0cd74e66b71afec38f5adb9b23a7ceff714ded5d1f9de24e49771c6d2",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "our contributions and findings are threefold we point out the realistic scenario of queries of varying complexities and find out that existing retrievalaugmented generation approaches tend to be overly simple or complex we adapt retrievalaugmented llms to the query complexity assessed by the classifier which en ables the utilization of the most suitable approach tailored to each query we show that our adaptiverag is highly effec tive and efficient balancing between the com plexity and the simplicity for diverse queries 2 related work opendomain qa opendomain qa is the task of accurately answering a query by sourcing for queryrelevant documents and then interpreting them to provide answers chen et al 2017 zhu et al 2021 which thus generally involves two modules a retriever karpukhin et al 2020 xiong et al 2021 and a reader yang et al 2019 izac ard and grave 2021 jeong et al 2023 along with the emergence of llms with superior rea soning capabilities thanks to their billionsized pa rameters wei et al 2022a a synergy between llms and retrievers has led to significant advance ments lazaridou et al 2022 ram et al 2023 specifically this integration has been shown to enhance opendomain qa by mitigating the hallu cination problem from llms through strengthened reasoning abilities of the reader as well as utiliz ing the retrieved external documents cho et al 2023 despite these advancements for singlehop retrievalaugmented llms however the complex ity of some queries needs a more complex strategy multihop qa multihop qa is an extension of conventional opendomain qa which addition ally requires the system to comprehensively gather and contextualize information from multiple docu ments often iteratively to answer more complex queries trivedi et al 2022a yang et al 2018 in the realm of multihop qa the approach to itera tively access both llms and the retrieval module is generally employed specifically khattab et al 2022 press et al 2023 pereira et al 2023 and khot et al 2023 proposed to first decom pose the multihop queries into simpler singlehop queries repeatedly access the llms and retriever to solve these subqueries and merge their solu tions to formulate a complete answer in contrastto this decompositionbased approach other re cent studies such as yao et al 2023 and trivedi et al 2023 explored the interleaving of chainof thought reasoning wei et al 2022b  a method where a logical sequence of thoughts is generated  with document retrieval repeatedly applying this process until the reasoning chain generates the an swer in addition jiang et al 2023 introduced an approach to repeatedly retrieving new documents if the tokens within generated sentences have low confidence however the aforementioned methods overlooked the fact that in realworld scenarios queries are of a wide variety of complexities there fore it would be largely inefficient to iteratively access llms and retrievers for every query which might be simple enough with a single retrieval step or even only with an llm itself adaptive retrieval to handle queries of varying complexities the adaptive retrieval strategy aims to dynamically decide whether to retrieve documents or not based on each querys complexity in this vein mallen et al 2023 proposed to decide the querys complexity level based on the frequency of its entities and suggested using the retrieval mod ules only when the frequency falls below a cer tain threshold however this approach focusing solely on the binary decision of whether to retrieve or not may not be sufficient for more complex queries that require multiple reasoning steps ad ditionally qi et al 2021 proposed an approach that performs a fixed set of operations retrieving reading and reranking multiple times until the an swer is derived for the given query which is built upon traditional bertlike lms however unlike our adaptiverag which predetermines the query complexity and adapts the operational behavior of any offtheshelf llms accordingly this approach applies the same fixed operations to every query regardless of its complexity but also necessitates additional specific training to lms concurrent to our work asai et al 2024 suggested training a so phisticated model to dynamically retrieve critique and generate the text nevertheless we argue that all the aforementioned adaptive retrieval methods that rely on a single model might be suboptimal in handling a variety of queries of a range of differ ent complexities since they tend to be either overly simple or complex for all the input queries which demands a new approach that can select the most suitable strategy of retrievalaugmented llms tai lored to the query complexity",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4677,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "7cd07a4e-78d6-4951-85b4-9a5c1a05243a",
        "embedding": null,
        "metadata": {
            "page_label": "4",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "ad2255f3-4d63-4af0-9c47-fd22974931b7",
                "node_type": "4",
                "metadata": {
                    "page_label": "4",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "c74e3e372f3a7d5294bbcd0d4a1534d4aee097136ee98db2972603426d3bac73",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "3 method in this section we describe our approach to adapt ing retrievalaugmented llms by predetermining the query complexity and then selecting the most fitting strategies for retrievalaugmented llms 31 preliminaries we begin with preliminaries formally introducing different strategies of retrievalaugmented llms non retrieval for qa let us first define an llm as a model llm which takes a sequence of tokens x x1 x2  x nas an input and then generates a sequence of tokens y y1 y2  y nas an out put which is formalized as follows yllmx then in our problem setup for qa xandybe come the input query  q from the user and the generated answer  a from the llm respectively qxanday also subsequently the most na\u00efve llmpowered qa model can be represented as follows allmq ideally ashould match the actual correct answer a this nonretrieval based qa method is highly efficient and could be a somewhat promising approach to handling easy queries as the size of llms becomes extremely large with its effect on storing a large amount of knowledge however this approach is largely prob lematic on queries that require precise or concur rent knowledge of specific people events or any subjects beyond the llms internal knowledge singlestep approach for qa to address the aforementioned scenarios where llmmay struggle with queries that are not answerable by llmitself we can utilize the external knowledge d which includes useful information for queries retrieved from the external knowledge source dthat could be an encyclopedia eg wikipedia consisting of millions of documents specifically to obtain suchdfromd a specific retrieval model is nec essary which returns documents based on their relevance with the given query this process can be formulated as follows dretriever qd where retriever is the retrieval model with d d  here we can use any offtheshelf re triever robertson et al 1994 karpukhin et al 2020 after the retrieval step is done we now have a pair of query qand its relevant documents d then in order to augment llms with this retrieved exter nal knowledge we can incorporate it into the input of llms represented as follows allmqdthis process allows llms to gain access to exter nal information contained in d which can provide the supplementary context that the internal knowl edge of llmlacks which can subsequently improve the accuracy and concurrency of llms for qa multistep approach for qa even though the aforementioned singlestep approach offers signif icant improvements over nonretrieval for qthat requires external knowledge it encounters notable limitations particularly when dealing with com plex queries that necessitate synthesizing informa tion from multiple source documents and reasoning over them this is where a multistep approach and reasoning for qa become essential in this multistep approach llminteracts with retriever in several rounds progressively refin ing its understanding of q until it formulates the fi nal answer from findings accumulated across these multiple steps specifically the process begins with the initial query q and at every retrieval step i new documents diare retrieved from dand then incorporated into the input of llms as follows aillmqdici where the additional context cican be composed of previous documents and outcomes d1d2 di1a1a2 ai1 and diretriever qcid1 we would like to note that this iterative multistep process enables llmto construct a more comprehensive and exten sive foundation to solve queries effectively specif ically adept at complex multihop queries where answers depend on interconnected pieces of infor mation however it is important to recognize that this multistep approach can be resourceintensive due to the repeated accesses to retriever andllm which entail substantial computational costs 32 adaptiverag adaptive retrievalaugmented generation we now introduce our adaptive retrievalaugmented llms which are built upon three different strate gies described in the previous section and which are designed to select the most suitable strategy according to the complexity of queries adapting retrievalaugmented llms note that in realworld scenarios not all qfrom users have the same level of complexity necessitating 1it is worth noting that implementations of the llm and retriever vary across different multistep retrievalaugmented llm approaches trivedi et al 2023 press et al 2023 yao et al 2023 therefore the context cimay incorporate none some or all of the previous documents and answers",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4477,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "74507880-6f00-4f6a-a176-5118371e120d",
        "embedding": null,
        "metadata": {
            "page_label": "5",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "d8350f87-2d1b-4271-a7ec-8f1a3fb40b4c",
                "node_type": "4",
                "metadata": {
                    "page_label": "5",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "86d7c0412a7c27ed3dbe454a674cabf6867c0fbe8ffe1774d32e60086d9235e2",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "tailored strategies for handling each query in other words employing the most basic nonretrieval based approach llmqto respond to the complex query qwould be also ineffective figure 2 a conversely using a more elaborate multistep ap proach llmqdcfor simple qwould be ineffi cient figure 2 b therefore our adaptive frame work is designed to dynamically adjust the query handling strategy of retrievalaugmented llms which is achieved by determining the complexity of each query before attempting a solution notably this framework can offer a robust middle ground with a range of solutions from the simplest ap proach for the most straightforward queries to the onestep approach for moderate queries and up to the most comprehensive and rigorous approach for complex queries in addition since the operations ofllmandretriever remain consistent regard less of inputs to them our method can seeming lessly go back and forth across queries of different complexities without changing the internal model architecture or parameters during adaption query complexity assessment to operational ize our adaptive retrievalaugmented llm frame work we should determine the query complexity and to achieve this we propose to model a com plexity classifier whose goal is to return the appro priate complexity level of the given query specif ically given the query q our classifier can be for mulated as follows oclassifier q where classifier is a smaller language model that is trained to classify one of three different complexity levels and ois its corresponding class label in our classifier design there are three class labels a b and c where a indicates that qis straight forward and answerable by llmqitself b in dicates that qhas the moderate complexity where at least a singlestep approach llmqdis needed and c indicates that qis complex requiring the most extensive solution llmqdc2 training strategy the remaining step is to train the smaller language model for classifier  to accurately predict its complexity oin response to the given query q yet there is no annotated dataset available for querycomplexity pairs hence we propose to automatically construct the training dataset with two particular strategies to be specific we first aim at labeling the query 2we consider three levels of query complexity and leave the exploration of more finegrained complexities as future workcomplexity based on the results from three different retrievalaugmented llm strategies in order to determine the label by its needs for example if the simplest nonretrievalbased approach correctly generates the answer the label for its corresponding query is assigned a also to break the tie between different models in providing the label to the query we provide a higher priority to a simpler model in other words if both singlestep and multistep approaches produce the same correct answer while the nonretrievalbased approach fails we assign label b to its corresponding query however this labeling strategy has a limita tion in that not all the queries are assigned labels since the three retrievalaugmented approaches may all fail to generate the correct answer on the other hand the benchmark datasets may al ready have meaningful inductive biases about the most appropriate retrievalaugmented llm strate gies for their queries considering the ways they are created eg qa datasets that require sequen tial reasoning usually necessitate a multistep ap proach while queries of those with labeled sin gle documents can be ideally answerable with the singlestep approach therefore for those queries that remain unlabeled after the first labeling step we assign b to queries in singlehop datasets and c to queries in multihop datasets finally we train classifier with these automatically collected querycomplexity pairs3 by using a cross entropy loss then at inference we can deter mine the complexity of the query which is one of a b c by forwarding it to classifier  oclassifier q 4 experimental setups in this section we explain datasets models met rics and implementation details we provide addi tional details in appendix a 41 datasets in order to simulate a realistic scenario where dif ferent queries have varying complexities we use both the singlehop and multihop qa datasets si multaneously in the unified experimental setting singlehop qa for simpler queries we use three benchmark singlehop qa datasets which consist 3as we automatically assign classifier labels there might be errors in labeling and might be more advanced strategies to automatically assign labels which we leave as future work",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4580,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "3ab2b202-c743-4c44-a750-bac822d03eda",
        "embedding": null,
        "metadata": {
            "page_label": "6",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "3ff68447-8c19-4367-93d5-08be7c17ee0a",
                "node_type": "4",
                "metadata": {
                    "page_label": "6",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "b01654ceb6e67b27e6d1a96853e0bf3d47d1621ab0999f4c8ca0c0fb5401aad1",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "table 1 averaged results on a collection of benchmark datasets for opendomain question answering including the singlehop and multihop queries with different llms selfragis trained with a different base llm namely llama2 touvron et al 2023 therefore we compare the results of flant5xl 3b with the results from selfrag with llama2 7b and the results of others with the results from selfrag with llama2 13b we emphasize our results in bold for easy comparisons flant5xl 3b flant5xxl 11b gpt35 turbo types methods em f1 acc step time em f1 acc step time em f1 acc step time simpleno retrieval 1487 2112 1597 000 011 1783 2514 1933 000 008 3577 4856 4427 000 071 singlestep approach 3483 4431 3887 100 100 3787 4763 4190 100 100 3473 4699 4527 100 100 adaptiveadaptive retrieval 2387 3224 2673 050 056 2693 3567 2973 050 054 3590 4820 4530 050 086 selfrag990 2079 3157 072 043 1087 2298 3413 074 023 1087 2298 3413 074 150 adaptiverag ours 3717 4694 4210 217 360 3890 4862 4377 135 200 3797 5091 4897 103 146 complex multistep approach 3900 4885 4370 469 881 4013 5009 4520 213 380 3813 5087 4970 281 333 oracle adaptiverag w oracle 4500 5628 4990 128 211 4717 5860 5220 084 110 4770 6280 5857 050 103 of queries and their associated documents contain ing answers namely 1 squad v11 rajpurkar et al 2016 2 natural questions kwiatkowski et al 2019 and 3 triviaqa joshi et al 2017 multihop qa to consider more complex query scenarios we use three benchmark multihop qa datasets which require sequential reasoning over multiple documents namely 1 musique trivedi et al 2022a 2 hotpotqa yang et al 2018 and3 2wikimultihopqa ho et al 2020 42 models we compare our adaptiverag against relevant models including three retrievalaugmented llm strategies in section 31 and the adaptive re trieval approaches mallen et al 2023 asai et al 2024 which can be grouped into one of three cat egories simple adaptive and complex specif ically simple approaches include the 1 no re trieval and2 singlestep approach based meth ods adaptive approaches include the 3 adaptive retrieval mallen et al 2023 4 selfrag asai et al 2024 and our 5 adaptiverag  which can adaptively perform retrieval based on the question complexity for the 6 multistep ap proach  we use the most sophisticated stateof theart method trivedi et al 2023 iteratively accessing both the retriever and llm with chain ofthought reasoning wei et al 2022b for every query note that models across different categories are not directly comparable yet in the ideal set ting adaptive approaches should be more effective than those in the simple category while simultane ously being more efficient than the complex one therefore we also report the performance in an ideal scenario 7 adaptiverag w oracle  using the oracle classifier with our adaptiverag 43 evaluation metrics when it comes to evaluating adaptive models it is essential to simultaneously consider both thetask performance and efficiency along with their tradeoffs thus we report the results with five metrics where three of them measure the effective ness and the other two measure the efficiency in particular for effectiveness we use f1 em and accuracy acc following the standard evaluation protocol mallen et al 2023 baek et al 2023 asai et al 2024 where f1 measures the number of overlapping words between the predicted an swer and the ground truth em measures whether they are the same and acc measures whether the predicted answer contains the groundtruth answer for efficiency we measure the number of retrieval andgenerate steps and the average time for answer ing each query relative to the onestep approach 44 implementation details for a fair comparison and following mallen et al 2023 and trivedi et al 2023 we use the same re triever a termbased sparse retrieval model known as bm25 robertson et al 1994 across all differ ent models for the external document corpus we use different sources depending on the dataset type the wikipedia corpus preprocessed by karpukhin et al 2020 for singlehop datasets and the pre processed corpus by trivedi et al 2023 for multi hop datasets regarding the llms that are used to generate answers we use the flant5 series models chung et al 2022 of xl with 3b pa rameters and xxl with 11b parameters and the gpt35 model gpt35turboinstruct for the retrievalaugmented llm design we follow the implementation details from trivedi et al 2023 which include input prompts instructions and the number of test samples for evaluation eg 500 samples per dataset in our adaptiverag for the querycomplexity classifier we use and train the t5large model raffel et al 2020 specifically the classifier is trained using the epoch that shows the best performance until 100 training iterations from the validation set with the learning rate of 3e",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4774,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "5dc0e50b-9fe5-4694-b6b4-3402f0355470",
        "embedding": null,
        "metadata": {
            "page_label": "7",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "66483dc5-4cf1-481a-a191-dc00e28c2ff7",
                "node_type": "4",
                "metadata": {
                    "page_label": "7",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "e31d213d67b6953a6ae56617a59bf4f2d4517c7bc5f28414ff6092beccfde0db",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "table 2 results on each of a collection of datasets with flant5xl 3b as the llm we emphasize our results in bold squad natural questions triviaqa data types methods em f1 acc step time em f1 acc step time em f1 acc step time singlestepsimpleno retrieval 360 1050 500 000 011 1420 1900 1560 000 013 2500 3180 2700 000 013 singlestep approach 2780 3930 3400 100 100 3780 4730 4460 100 100 5360 6240 6020 100 100 adaptiveadaptive retrieval 1340 2310 1760 050 055 2820 3600 3300 050 056 3840 4690 4260 050 056 selfrag220 1120 1840 063 050 3140 3900 3360 063 017 1280 2930 5700 068 045 adaptiverag ours 2680 3830 3300 137 202 3780 4730 4460 100 100 5220 6070 5820 123 154 complex multistep approach 2440 3560 2960 452 903 3860 4780 4420 504 1018 5380 6240 6020 528 922 oracle adaptiverag w oracle 3200 4560 3820 124 160 4740 5710 5360 110 155 6160 7020 6640 079 110 musique hotpotqa 2wikimultihopqa data types methods em f1 acc step time em f1 acc step time em f1 acc step time multistepsimpleno retrieval 240 1070 320 000 011 1660 2271 1720 000 011 2740 3204 2780 000 010 singlestep approach 1380 2280 1520 100 100 3440 4615 3640 100 100 4160 4790 4280 100 100 adaptiveadaptive retrieval 640 1580 800 050 055 2360 3222 2500 050 055 3320 3944 3420 050 055 selfrag160 810 1200 073 051 680 1753 2960 073 045 460 1959 3880 093 049 adaptiverag ours 2360 3180 2600 322 661 4200 5382 4440 355 599 4060 4975 4640 263 468 complex multistep approach 2300 3190 2580 360 758 4460 5654 4700 553 938 4960 5885 5540 417 737 oracle adaptiverag w oracle 2480 3850 2700 198 399 5120 6400 5480 159 277 5300 6230 5940 101 169 f1102030405060adaptive retrieval selfrag adaptiverag ours classifier acc3540455055flant5xl f1102030405060adaptive retrieval selfrag adaptiverag ours classifier acc3540455055flant5xxl no one multino one multi031 047 022 01 066 023 003 031 065 020406confusion matrix figure 3 performance on qa and querycomplexity assessment of different adaptive approaches for retrievalaugmented llms with flant5 xl left and xxl center for labeling the complexity of queries we use the silver data annotated from the prediction outcomes of models described in section 32 we also provide the confusion matrix across three labels right 5 and the adamw loshchilov and hutter 2019 as an optimizer regarding its training data we sample and annotate 400 queries from 6 datasets based on its inductive bias singlehop for onestep approach and multihop for multistep in addition we use predicted outcomes of three different strate gies over 400 queries sampled from each dataset note that those queries used for classifier training do not overlap with the testing queries for qa 5 experimental results and analyses in this section we show the overall experimental results and offer indepth analyses of our method main results first of all table 1 shows our main results averaged over all considered datasets which corroborate our hypothesis that simple retrieval augmented strategies are less effective than the complex strategy while the complex one is sig nificantly more expensive than the simple ones in addition we report the more granular results with flant5xl on each of the singlehop and multi hop datasets in table 2 and more with different llms in table 7 and table 8 of appendix which are consistent with the results observed in table 1 however in a realworld scenario not all users ask queries with the same level of complexity which emphasizes the importance of the need for adaptive strategies note that among the adaptive strategies our adaptiverag shows remarkableeffectiveness over the competitors table 1 this indicates that merely focusing on the decision of whether to retrieve or not is suboptimal also as shown in table 2 such simple adaptive strategies are particularly inadequate for handling complex queries in multihop datasets which require ag gregated information and reasoning over multiple documents meanwhile our approach can consider a more finegrained query handling strategy by fur ther incorporating an iterative module for complex queries furthermore in a realistic setting we should take into account not only effectiveness but also efficiency as shown in table 1 compared to the complex multistep strategy our proposed adap tive strategy is significantly more efficient across all model sizes this is meaningful in this era of llms where the cost of accessing them is a critical factor for practical applications and scalability fi nally to see the upper bound of our adaptiverag we report its performances with the oracle classifier where the classification performance is perfect as shown in table 1 and table 2 we observe that it achieves the best performance while being much more efficient than our adaptiverag without the oracle classifier these results support the valid ity and significance of our proposal for adapting retrievalaugmented llm strategies based on query complexity and further suggest the direction to de velop more improved classifiers to achieve optimal performance",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 5004,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "ba865e69-2cc3-4cf7-86b9-92b450241c19",
        "embedding": null,
        "metadata": {
            "page_label": "8",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e88f5317-1744-43a9-99a9-9eeac22858c3",
                "node_type": "4",
                "metadata": {
                    "page_label": "8",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "8c74be189d44c0b51932ffedfcc16a170cb4ad992198fe3207f99fdc3a5d79a1",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "table 3 the exact elapsed time per query and the percentage of the predicted labels from the classifier over all samples labels timequery sec percentage  no a 035 860 one b 308 5333 multi c 2718 3807 classifier performance to understand how the proposed classifier works we analyze its perfor mance across different complexity labels as fig ure 3 left and center shows the classification accuracy of our adaptiverag is better than those of the other adaptive retrieval baselines which leads to overall qa performance improvements in other words this result indicates that our adaptive rag is capable of more accurately classifying the complexity levels with various granularities which include not performing retrieval performing re trieval only once and performing retrieval multiple times in addition to the true positive performance of our classifier averaged over all those three la bels in figure 3 left and center we further re port its confusion matrix in figure 3 right we note that the confusion matrix reveals some notable trends c multi is sometimes misclassified as b one about 31 and b one as c multi about 23 a no is misclassified often as b one about 47 and less frequently as c multi about 22 while the overall results in figure 3 show that our classifier effectively cate gorizes the three labels further refining it based on such misclassification would be a meaningful direction for future work analyses on efficiency for classifier while ta ble 1 shows the relative elapsed time for each of the three different rag strategies we further provide the exact elapsed time per query for our adaptive rag and the distribution for predicted labels from our querycomplexity classifier in table 3 similar to the results of the elapsed time in table 1 relative time table 3 exact time shows that efficiency can be substantially improved by identifying sim ple or straightforward queries analyses on training data for classifier we have shown that the classifier plays an important role in adaptive retrieval here we further analyze the different strategies for training the classifier by ablating our full training strategy which includes two approaches generating silver data from pre dicted outcomes of models and utilizing inductivetable 4 results on qa and complexity classification with varying the data annotation strategies for training the classifier qa classifier accuracy training strategies f1 step all no one multi adaptiverag ours 4694 1084 5452 3052 6628 6545 wo binary 4343 640 6030 6219 6570 3955 wo silver 4879 1464 4000 000 5398 7591 bias in datasets see section 32 as table 4 shows compared to the training strategy relying solely on the data derived from inductive bias ours is sig nificantly more efficient this efficiency is partly because ours also takes into account the case that does not consider any documents at all as also implied by the classification accuracy meanwhile queries in the existing datasets do not capture the information on whether the retrieval is required or not on the other hand in the case of only using the silver data annotated from the correct predictions while its overall classification accuracy is high the overall qa performance implies that relying on the silver data may not be optimal this may be because this silver data does not cover complex ity labels over incorrectly predicted queries which leads to lower generalization effect on queries rel evant to them meanwhile by also incorporating complexity labels from dataset bias singlehop vs multihop the classifier becomes more accurate in predicting multihop queries leading to the better performance it is worth noting that our automatic labeling strategies are two particular instantiations for training the classifier and that there could be other instantiations which we leave as future work analyses on classifier size to investigate the sensitivity of our classifier according to its varying sizes we conducted further experiments as shown in table 6 we observe no significant performance differences among classifiers of various sizes even with reduced complexity and fewer parameters in smaller classifiers this indicates that our proposed classifier can contribute to resourceefficient set tings in realuse cases with smaller sizes without compromising the performance case study we conduct a case study to qualita tively compare our adaptiverag against adap tive retrieval table 5 shows the classified com plexity and the query handling patterns for both simple and complex questions first for the simple singlehop question our adaptiverag identifies that it is answerable by only using the llms para metric knowledge about google by contrast",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4675,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "994ac015-5ace-4861-ab4b-1afb70b9a44c",
        "embedding": null,
        "metadata": {
            "page_label": "9",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "a46f2e3c-bfd1-4636-b4c1-0d7669d7eb58",
                "node_type": "4",
                "metadata": {
                    "page_label": "9",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "aefd33ea565cdfe55c3319ab934d0b5676c74bdd5b8cf86f273d33c2fed89e1f",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "table 5 case study with gpt35 turbo where we represent the factual error in red and the accurate information in blue dataset question adaptive retrieval adaptiverag ours nq singlehopwhich famous corpo rate logo changed to a flat colourcolor sans serif font in its first ma jor change since 1999query type b singlestep approach reasoning the famous corporate logo that changed to a flat color sansserif font in its first major change since 1999 is microsoft microsoft changed its logo in 2012 replacing the italicized multicolored font with a simple flat grey font so the answer is microsoftquery type a non retrieval reasoning the famous corporate logo that changed to a flat color sans serif font in its first ma jor change since 1999 is google google changed its logo in 2015 so the answer is google musique multihopwho is the child of the italian navigator who explored the eastern coast of the continent c\u00e9sar gaytan was born in for the englishquery type a non retrieval reasoning c\u00e9sar gaytan was born in the conti nent of north america the italian navigator who explored the eastern coast of north america for the english was giovanni caboto also known as john cabot so the answer is giovanni cabotojohn cabotquery type c multistep approach reasoning c\u00e9sarr gaytan was born in guadalajara jalisco mexico the italian navigator who explored the eastern coast of the continent for the english is john cabot john cabots son is sebastian cabot so the answer is sebastian cabot table 6 results with varying model sizes for classifiers qa classifier accuracy sizes f1 step all no one multi small 60m 4583 964 5348 2665 7062 5318 base 223m 4597 983 5341 2642 6946 5682 large 770m 4694 1084 5452 3052 6628 6545 adaptive retrieval fetches additional documents leading to longer processing times and occasion ally producing incorrect responses due to the inclu sion of partially irrelevant information about mi crosoft meanwhile faced with a complex ques tion adaptiverag seeks out relevant information including details like a son of john cabot which may not have been stored in llms while adaptive retrieval fails to request such information from external sources resulting in inaccurate answers 6 conclusion in this work we proposed the adaptive retrieval augmented generation framework referred to as adaptiverag to handle queries of various complexities specifically adaptiverag is de signed to dynamically adjust its query handling strategies in the unified retrievalaugmented llm based on the complexity of queries that they en counter which spans across a spectrum of the non retrievalbased approach for the most straightfor ward queries to the singlestep approach for the queries of moderate complexity and finally to the multistep approach for the complex queries the core step of our adaptiverag lies in determin ing the complexity of the given query which is instrumental in selecting the most suitable strat egy for its answer to operationalize this process we trained a smaller language model with query complexity pairs which are automatically anno tated from the predicted outcomes and the inductive biases in datasets we validated our adaptiveragon a collection of opendomain qa datasets cover ing the multiple query complexities including both the single and multihop questions the results demonstrate that our adaptiverag enhances the overall accuracy and efficiency of qa systems al locating more resources to handle complex queries while efficiently handling simpler queries com pared to the existing onesizefitsall approaches that tend to be either minimalist or maximalist over varying query complexities limitations while our adaptiverag shows clear advantages in effectiveness and efficiency by determining the query complexity and then leveraging the most suitable approach for tackling it it is important to recognize that there still exist potential avenues for improving the classifier from the perspectives of its training datasets and architecture specifi cally as there are no available datasets for training the querycomplexity classifier we automatically create new data based on the model prediction out comes and the inductive dataset biases however our labeling process is one specific instantiation of labeling the query complexity and it may have the potential to label queries incorrectly despite its effectiveness therefore future work may create new datasets that are annotated with a diverse range of query complexities in addition to the labels of questionanswer pairs also as the performance gap between the ideal classifier in table 1 and the current classifier in figure 3 indicates there is still room to improve the effectiveness of the classifier in other words our classifier design based on the smaller lm is the initial simplest instantiation for classifying the query complexity and based upon it future work may improve the classifier archi tecture and its performance which will positively contribute to the overall qa performance",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4981,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "7b9fee7c-d346-402c-be69-4258126ec4ea",
        "embedding": null,
        "metadata": {
            "page_label": "10",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e02e8351-4cc4-4073-86b2-b919786e52b8",
                "node_type": "4",
                "metadata": {
                    "page_label": "10",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "ff14540c479a957ac1d9b4abe9fe6fe86b588fb0b596e1a2f2ab7d5da834c4bf",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "ethics statement the experimental results on adaptiverag vali date its applicability in realistic scenarios where a wide range of diverse user queries exist nonethe less given the potential diversity of realworld user inputs it is crucial to also consider scenarios where these inputs might be offensive or harmful we should be aware that such inputs could lead to the retrieval of offensive documents and the genera tion of inappropriate responses by the retrieval augmented llms to address this challenge de veloping methods to detect and manage offensive or inappropriate content in both user inputs and re trieved documents within the retrievalaugmented framework is essential we believe that this is a critical area for future work acknowledgements this work was supported by institute for informa tion and communications technology promotion iitp grant funded by the korea government no 2018000582 prediction and augmentation of the credibility distribution via linguistic analysis and automated evidence document collection basic science research program through the national research foundation of korea nrf funded by the ministry of education rs202300275747 and the artificial intelligence industrial convergence cluster development project funded by the ministry of science and ict msit korea  gwangju metropolitan city references rohan anil andrew m dai orhan firat melvin john son dmitry lepikhin alexandre passos siamak shakeri emanuel taropa paige bailey zhifeng chen eric chu jonathan h clark laurent el shafey yanping huang kathy meierhellstern gau rav mishra erica moreira mark omernick kevin robinson sebastian ruder yi tay kefan xiao yuanzhong xu yujing zhang gustavo hern\u00e1ndez \u00e1brego junwhan ahn jacob austin paul barham jan a botha james bradbury siddhartha brahma kevin brooks michele catasta yong cheng colin cherry christopher a choquettechoo aakanksha chowdhery cl\u00e9ment crepy shachi dave mostafa dehghani sunipa dev jacob devlin mark d\u00edaz nan du ethan dyer vladimir feinberg fangxi aoyu feng vlad fienber markus freitag xavier garcia sebastian gehrmann lucas gonzalez and et al 2023 palm 2 technical report arxiv preprint arxiv230510403  akari asai zeqiu wu yizhong wang avirup sil andhannaneh hajishirzi 2024 selfrag learning to retrieve generate and critique through selfreflection inthe twelfth international conference on learning representations  jinheon baek soyeong jeong minki kang jong park and sung ju hwang 2023 knowledgeaugmented language model verification in proceedings of the 2023 conference on empirical methods in natural language processing emnlp 2023 singapore de cember 610 2023  pages 17201736 association for computational linguistics sebastian borgeaud arthur mensch jordan hoffmann trevor cai eliza rutherford katie millican george van den driessche jeanbaptiste lespiau bogdan damoc aidan clark diego de las casas aurelia guy jacob menick roman ring tom hennigan saffron huang loren maggiore chris jones albin cassirer andy brock michela paganini geoffrey irving oriol vinyals simon osindero karen si monyan jack w rae erich elsen and laurent sifre 2022 improving language models by retrieving from trillions of tokens in international conference on machine learning icml 2022 1723 july 2022 bal timore maryland usa  volume 162 of proceedings of machine learning research  pages 22062240 pmlr tom b brown benjamin mann nick ryder melanie subbiah jared kaplan prafulla dhariwal arvind neelakantan pranav shyam girish sastry amanda askell sandhini agarwal ariel herbertv oss gretchen krueger tom henighan rewon child aditya ramesh daniel m ziegler jeffrey wu clemens winter christopher hesse mark chen eric sigler mateusz litwin scott gray benjamin chess jack clark christopher berner sam mccandlish alec radford ilya sutskever and dario amodei 2020 language models are fewshot learners in ad vances in neural information processing systems 33 annual conference on neural information process ing systems 2020 neurips 2020 december 612 2020 virtual  danqi chen adam fisch jason weston and antoine bordes 2017 reading wikipedia to answer open domain questions in proceedings of the 55th annual meeting of the association for computational lin guistics acl 2017 vancouver canada july 30  august 4 volume 1 long papers  pages 18701879 association for computational linguistics sukmin cho jeongyeon seo soyeong jeong and jong c park 2023 improving zeroshot reader by reducing distractions from irrelevant documents in opendomain question answering in findings of the association for computational linguistics emnlp 2023 singapore december 610 2023  pages 3145 3157 association for computational linguistics hyung won chung le hou shayne longpre barret zoph yi tay william fedus eric li xuezhi wang mostafa dehghani siddhartha brahma albert web son shixiang shane gu zhuyun dai mirac suz gun xinyun chen aakanksha chowdhery sharan",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4873,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "b30dd03c-aca2-44d0-bd64-e977fe8bae52",
        "embedding": null,
        "metadata": {
            "page_label": "11",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "bf492d4d-53d4-44a0-8f30-290d58cc693d",
                "node_type": "4",
                "metadata": {
                    "page_label": "11",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "e4010287de509fa3fcd6d8154d63f66727272600dece0d450a45f9a791e9a93a",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "narang gaurav mishra adams yu vincent y  zhao yanping huang andrew m dai hongkun yu slav petrov ed h chi jeff dean jacob devlin adam roberts denny zhou quoc v  le and jason wei 2022 scaling instructionfinetuned language models arxiv preprint arxiv221011416  xanh ho anhkhoa duong nguyen saku sugawara and akiko aizawa 2020 constructing a multihop qa dataset for comprehensive evaluation of reason ing steps in proceedings of the 28th international conference on computational linguistics coling 2020 barcelona spain online december 813 2020  pages 66096625 international committee on computational linguistics gautier izacard and edouard grave 2021 leveraging passage retrieval with generative models for open do main question answering in proceedings of the 16th conference of the european chapter of the associ ation for computational linguistics main volume eacl 2021 online april 19  23 2021  pages 874 880 association for computational linguistics gautier izacard patrick s h lewis maria lomeli lucas hosseini fabio petroni timo schick jane dwivediyu armand joulin sebastian riedel and edouard grave 2023 atlas fewshot learning with retrieval augmented language models j mach learn res  24251125143 soyeong jeong jinheon baek sukmin cho sung ju hwang and jong park 2023 testtime selfadaptive small language models for question answering in findings of the association for computational lin guistics emnlp 2023 singapore december 610 2023  pages 1545915469 association for computa tional linguistics zhengbao jiang frank f xu luyu gao zhiqing sun qian liu jane dwivediyu yiming yang jamie callan and graham neubig 2023 active retrieval augmented generation in emnlp 2023  mandar joshi eunsol choi daniel s weld and luke zettlemoyer 2017 triviaqa a large scale distantly supervised challenge dataset for reading comprehen sion in proceedings of the 55th annual meeting of the association for computational linguistics acl 2017 vancouver canada july 30  august 4 volume 1 long papers  pages 16011611 association for computational linguistics vladimir karpukhin barlas oguz sewon min patrick s h lewis ledell wu sergey edunov danqi chen and wentau yih 2020 dense passage retrieval for opendomain question answering in proceedings of the 2020 conference on empirical methods in natu ral language processing emnlp 2020 november 1620 2020  association for computational linguis tics jungo kasai keisuke sakaguchi yoichi takahashi ro nan le bras akari asai xinyan yu dragomir r radev noah a smith yejin choi and kentaro inui2022 realtime qa whats the answer right now arxiv preprint arxiv220713332  omar khattab keshav santhanam xiang lisa li david hall percy liang christopher potts and matei zaharia 2022 demonstratesearch predict composing retrieval and language mod els for knowledgeintensive nlp arxiv preprint arxiv221214024  abs221214024 tushar khot harsh trivedi matthew finlayson yao fu kyle richardson peter clark and ashish sab harwal 2023 decomposed prompting a modular approach for solving complex tasks in the eleventh international conference on learning representa tions iclr 2023 kigali rwanda may 15 2023  openreviewnet tom kwiatkowski jennimaria palomaki olivia red field michael collins ankur parikh chris alberti danielle epstein illia polosukhin jacob devlin ken ton lee kristina toutanova llion jones matthew kelcey mingwei chang andrew m dai jakob uszkoreit quoc le and slav petrov 2019 natu ral questions a benchmark for question answering research transactions of the association for compu tational linguistics  7452466 angeliki lazaridou elena gribovskaya wojciech stokowiec and nikolai grigorev 2022 internet augmented language models through fewshot prompting for opendomain question answering arxiv preprint arxiv220305115  belinda z li sewon min srinivasan iyer yashar mehdad and wentau yih 2020 efficient onepass endtoend entity linking for questions in proceed ings of the 2020 conference on empirical methods in natural language processing emnlp 2020 online november 1620 2020  pages 64336441 associa tion for computational linguistics ilya loshchilov and frank hutter 2019 decoupled weight decay regularization in 7th international conference on learning representations iclr 2019 new orleans la usa may 69 2019  openre viewnet alex mallen akari asai victor zhong rajarshi das daniel khashabi and hannaneh hajishirzi 2023 when not to trust language models investigating effectiveness of parametric and nonparametric mem ories in proceedings of the 61st annual meeting of the association for computational linguistics vol ume 1 long papers acl 2023 toronto canada july 914 2023  pages 98029822 association for computational linguistics openai 2023 gpt4 technical report arxiv preprint arxiv230308774  adam paszke sam gross francisco massa adam lerer james bradbury gregory chanan trevor killeen zeming lin natalia gimelshein luca antiga alban desmaison andreas k\u00f6pf edward z",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4905,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "61a82d29-9a6e-4121-9b30-90c5b4cfba42",
        "embedding": null,
        "metadata": {
            "page_label": "12",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e4664c73-479e-4f00-8926-2abe5d4d4327",
                "node_type": "4",
                "metadata": {
                    "page_label": "12",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "1aa8d17eb20c0dfb8dfb4e6b1e848e4c26273621c5cd4e4582b97aa16460c6f8",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "yang zachary devito martin raison alykhan te jani sasank chilamkurthy benoit steiner lu fang junjie bai and soumith chintala 2019 pytorch an imperative style highperformance deep learning li brary in advances in neural information processing systems 32 annual conference on neural informa tion processing systems 2019  pages 80248035 jayr alencar pereira robson do nascimento fidalgo roberto de alencar lotufo and rodrigo frassetto nogueira 2023 visconde multidocument qa with gpt3 and neural reranking in advances in informa tion retrieval  45th european conference on infor mation retrieval ecir 2023 dublin ireland april 26 2023 proceedings part ii  volume 13981 of lecture notes in computer science  pages 534543 springer ofir press muru zhang sewon min ludwig schmidt noah a smith and mike lewis 2023 measuring and narrowing the compositionality gap in language models in findings of the association for computa tional linguistics emnlp 2023  peng qi haejun lee tg sido and christopher d man ning 2021 answering opendomain questions of varying reasoning steps from text in proceedings of the 2021 conference on empirical methods in natural language processing emnlp 2021 vir tual event  punta cana dominican republic 711 november 2021  pages 35993614 association for computational linguistics colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu 2020 exploring the limits of transfer learning with a unified texttotext trans former j mach learn res  21140114067 pranav rajpurkar jian zhang konstantin lopyrev and percy liang 2016 squad 100 000 questions for machine comprehension of text in proceedings of the 2016 conference on empirical methods in natural language processing emnlp 2016 austin texas usa november 14 2016  pages 23832392 the association for computational linguistics ori ram yoav levine itay dalmedigos dor muhlgay amnon shashua kevin leytonbrown and yoav shoham 2023 incontext retrievalaugmented lan guage models transactions of the association for computational linguistics  stephen e robertson steve walker susan jones micheline hancockbeaulieu and mike gatford 1994 okapi at trec3 in proceedings of the third text retrieval conference trec 1994 gaithers burg maryland usa november 24 1994  volume 500225 of nist special publication  pages 109 126 national institute of standards and technology nist weijia shi sewon min michihiro yasunaga min joon seo rich james mike lewis luke zettle moyer and wentau yih 2023 replug retrievalaugmented blackbox language models arxiv preprint arxiv230112652  hugo touvron louis martin kevin stone peter al bert amjad almahairi yasmine babaei nikolay bashlykov soumya batra prajjwal bhargava shruti bhosale dan bikel lukas blecher cristian canton ferrer moya chen guillem cucurull david esiobu jude fernandes jeremy fu wenyin fu brian fuller cynthia gao vedanuj goswami naman goyal an thony hartshorn saghar hosseini rui hou hakan inan marcin kardas viktor kerkez madian khabsa isabel kloumann artem korenev punit singh koura marieanne lachaux thibaut lavril jenya lee di ana liskovich yinghai lu yuning mao xavier mar tinet todor mihaylov pushkar mishra igor moly bog yixin nie andrew poulton jeremy reizen stein rashi rungta kalyan saladi alan schelten ruan silva eric michael smith ranjan subrama nian xiaoqing ellen tan binh tang ross tay lor adina williams jian xiang kuan puxin xu zheng yan iliyan zarov yuchen zhang angela fan melanie kambadur sharan narang aur\u00e9lien ro driguez robert stojnic sergey edunov and thomas scialom 2023 llama 2 open foundation and fine tuned chat models arxiv preprint arxiv230709288  harsh trivedi niranjan balasubramanian tushar khot and ashish sabharwal 2022a musique multi hop questions via singlehop question composition trans assoc comput linguistics  10539554 harsh trivedi niranjan balasubramanian tushar khot and ashish sabharwal 2022b musique multi hop questions via singlehop question composition transactions of the association for computational linguistics  10539554 harsh trivedi niranjan balasubramanian tushar khot and ashish sabharwal 2023 interleaving retrieval with chainofthought reasoning for knowledge intensive multistep questions in proceedings of the 61st annual meeting of the association for com putational linguistics volume 1 long papers acl 2023 toronto canada july 914 2023  pages 1001410037 association for computational lin guistics jason wei yi tay rishi bommasani colin raffel barret zoph sebastian borgeaud dani yogatama maarten bosma denny zhou donald metzler ed h chi tatsunori hashimoto oriol vinyals percy liang jeff dean and william fedus 2022a emer gent abilities of large language models trans mach learn res  2022 jason wei xuezhi wang dale schuurmans maarten bosma brian ichter fei xia ed h chi quoc v  le and denny zhou 2022b chainofthought prompt ing elicits reasoning in large language models in neurips  thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi pier ric cistac tim rault r\u00e9mi louf morgan funtowicz",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 5048,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "f4c61957-59a3-45fe-8a99-87283a513329",
        "embedding": null,
        "metadata": {
            "page_label": "13",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "51d7c203-3e53-4231-b080-a61c8caf4680",
                "node_type": "4",
                "metadata": {
                    "page_label": "13",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "dedf1388ea9caff06959df009a59657cfdac6325428d98f1f2ca624b61bcef36",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "joe davison sam shleifer patrick von platen clara ma yacine jernite julien plu canwen xu teven le scao sylvain gugger mariama drame quentin lhoest and alexander m rush 2020 transform ers stateoftheart natural language processing in proceedings of the 2020 conference on empirical methods in natural language processing system demonstrations emnlp 2020  demos  pages 38 45 association for computational linguistics lee xiong chenyan xiong ye li kwokfung tang jialin liu paul n bennett junaid ahmed and arnold overwijk 2021 approximate nearest neigh bor negative contrastive learning for dense text re trieval in 9th international conference on learning representations iclr 2021 virtual event austria may 37 2021  openreviewnet wei yang yuqing xie aileen lin xingyu li luchen tan kun xiong ming li and jimmy lin 2019 endtoend opendomain question answering with bertserini in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies naaclhlt 2019 minneapolis mn usa june 27 2019 demonstrations  pages 7277 association for computational linguistics zhilin yang peng qi saizheng zhang yoshua bengio william cohen ruslan salakhutdinov and christo pher d manning 2018 hotpotqa a dataset for diverse explainable multihop question answering inproceedings of the 2018 conference on empiri cal methods in natural language processing  pages 23692380 brussels belgium association for com putational linguistics shunyu yao jeffrey zhao dian yu nan du izhak shafran karthik r narasimhan and yuan cao 2023 react synergizing reasoning and acting in language models in the eleventh international conference on learning representations iclr 2023 kigali rwanda may 15 2023  openreviewnet fengbin zhu wenqiang lei chao wang jianming zheng soujanya poria and tatseng chua 2021 retrieving and reading a comprehensive survey on opendomain question answering arxiv preprint arxiv210100774",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 1949,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "3f02dbcd-1257-4a5e-96f2-7d2bf8eb2ae4",
        "embedding": null,
        "metadata": {
            "page_label": "14",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "28c6e323-af97-4a1c-8b7b-b977b90db54c",
                "node_type": "4",
                "metadata": {
                    "page_label": "14",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "923f424689cb42448651cbd7ef42231ea66d413a578d06e9b6731d1361ea69e6",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "0 2 4 6 8 10 time per query20304050performance f1 no retrieval singlestep approach adaptive retrieval multistep approach adaptiverag oursperformance vs time with flant5xlfigure 4 qa performance f1 and efficiency timequery for different retrievalaugmented generation approaches we use the flant5xl 3b as the base llm a additional experimental setups a1 datasets we use publicly open datasets for both single hop and multihop qa datasets referring to as karpukhin et al 2020 and trivedi et al 2023 respectively we describe the characteristics of each dataset 1 squad v11 rajpurkar et al 2016 is created through a process where annotators write questions based on the documents they read 2 natural questions kwiatkowski et al 2019 is constructed by real user queries on google search 3 triviaqa joshi et al 2017 comprises trivia questions sourced from various quiz websites 4 musique trivedi et al 2022a is collected by compositing multiple singlehop queries to form queries spanning 24 hops 5 hotpotqa yang et al 2018 is constructed by having annotators create questions that link multi ple wikipedia articles 6 2wikimultihopqa ho et al 2020 is derived from wikipedia and its associated knowledge graph path needing 2hops a2 models we describe the details of models as follows 1 no retrieval this approach uses only the llm itself to generate the answer to the given query 2 singlestep approach this approach first re trieves the relevant knowledge with the given query from the external knowledge sources and then aug ments the llm with this retrieved knowledge to generate the answer which iterates only once 3 adaptive retrieval this baseline mallen et al 2023 adaptively augments the llm with the re trieval module only when the entities appearing in queries are less popular to extract entities we use the available entitylinking method li et al 2020 namely blink for questions 4 selfrag this baseline asai et al 2024 00 05 10 15 20 25 30 35 40 time per query304050performance f1 no retrieval singlestep approach adaptive retrieval multistep approach adaptiverag oursperformance vs time with flant5xxlfigure 5 qa performance f1 and efficiency timequery for different retrievalaugmented generation approaches we use the flant5xxl 11b as the base llm trains the llm to adaptively perform retrieval and generation where the retrieval is conducted once it predicts the special retrieval token above a certain threshold and the answer generation follows 5 adaptiverag this is our model that adap tively selects the retrievalaugmented generation strategy smoothly oscillating between the non retrieval singlestep approach and multistep ap proaches4without architectural changes based on the query complexity assessed by the classifier 6 multistep approach this approach trivedi et al 2023 is the multistep retrievalaugmented llm which iteratively accesses both the retriever and llm with interleaved chainofthought rea soning wei et al 2022b repeatedly until it derives the solution or reaches the maximum step number 7 adaptiverag w oracle this is an ideal sce nario of our adaptiverag equipped with an or acle classifier that perfectly categorizes the query complexity a3 implementation details for computing resources we use a100 gpus with 80gb memory in addition due to the sig nificant costs associated with evaluating retrieval augmented generation models we perform experi ments with a single run finally we implemented models using pytorch paszke et al 2019 and transformers library wolf et al 2020 b additional experimental results performance vs time we further provide a com parison of different retrievalaugmented genera tion approaches with flant5xl and flant5 xxl models in figure 4 and figure 5 respectively in the context of performance and efficiency trade offs similar to the observation made from the gpt 35 model in figure 1 our proposed adaptiverag is significantly more effective as well as efficient 4for the multistep approach we use the stateoftheart question answering strategy from ircot trivedi et al 2023",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4030,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "22afd7b6-c359-492b-8e13-d5cc33e881e7",
        "embedding": null,
        "metadata": {
            "page_label": "15",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "94b90e09-6436-4580-bb20-72f787ab09b3",
                "node_type": "4",
                "metadata": {
                    "page_label": "15",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "3c61f9256bfb3e4a4fa369ef4dccb672903f9cbefbbcda5f2b9d731d24dcff3b",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "49e254d5-4234-463f-9173-4aa8fdd1f927",
                "node_type": "1",
                "metadata": {},
                "hash": "647a16a58564af2b74b9e0cd69088e79a3d667cd7aa2b3a0a7ef78fb48e3bd2d",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "table 7 results on each of a collection of datasets with flant5xxl 11b as the llm we emphasize our results in bold squad natural questions triviaqa data types methods em f1 acc step time em f1 acc step time em f1 acc step time singlestepsimpleno retrieval 700 1440 840 000 008 1880 2550 2040 000 008 3280 3920 3540 000 008 singlestep approach 2880 4080 3500 100 100 4140 5120 4760 100 100 5600 6470 6180 100 100 adaptiveadaptive retrieval 1560 2560 2000 050 054 3100 3970 3500 050 054 4480 5220 4860 050 054 selfrag160 1190 2080 059 031 3920 4710 4240 075 009 1460 3370 6020 076 022 adaptiverag ours 2780 3980 3400 117 150 4120 5100 4740 100 100 5200 6030 5720 103 133 complex multistep approach 2460 3690 3020 213 383 3960 4960 4640 216 394 5260 6110 5940 217 403 oracle adaptiverag w oracle 3280 4690 3820 085 094 5120 6100 5700 071 091 6340 7130 6820 051 060 musique hotpotqa 2wikimultihopqa data types methods em f1 acc step time em f1 acc step time em f1 acc step time multistepsimpleno retrieval 420 1340 540 000 008 1740 2544 1840 000 009 2680 3293 2800 000 008 singlestep approach 1680 2570 1920 100 100 3760 4927 3960 100 100 4660 5413 4820 100 100 adaptiveadaptive retrieval 840 1780 1020 050 054 2660 3601 2780 050 054 3520 4268 3680 050 054 selfrag120 820 1180 068 027 560 1786 3060 076 026 300 1914 3900 090 025 adaptiverag ours 2060 2850 2320 189 312 4420 5478 4680 158 253 4760 5736 5400 146 255 complex multistep approach 1940 2750 2180 209 366 4700 5781 4940 208 373 5760 6765 6400 217 363 oracle adaptiverag w oracle 2420 3720 2660 122 171 5220 6480 5460 092 133 5920 7040 6860 082 114 table 8 results on each of a collection of datasets with gpt35 turbo as the llm we emphasize our results in bold squad natural questions triviaqa data types methods em f1 acc step time em f1 acc step time em f1 acc step time singlestepsimpleno retrieval 1600 2920 2380 000 062 3980 5570 5500 000 056 6400 7560 7580 000 068 singlestep approach 1800 3380 2920 100 100 3240 4680 5480 100 100 5520 6650 6580 100 100 adaptiveadaptive retrieval 1540 3000 2440 050 081 3640 5120 5660 050 078 6200 7190 7220 050 084 selfrag160 1190 2080 059 191 3920 4710 4240 075 052 1460 3370 6020 076 159 adaptiverag ours 1980 3440 3000 087 121 3680 5200 5660 068 086 6240 7380 7380 022 079 complex multistep approach 1740 3150 2620 250 324 3560 4970 5780 258 379 5480 6710 6800 230 265 oracle adaptiverag w oracle 2800 4590 3940 054 093 5000 6540 6700 028 08 7080 8100 8000 011 073 musique hotpotqa 2wikimultihopqa data types methods em f1 acc step time em f1 acc step time em f1 acc step time multistepsimpleno retrieval 2040 3130 2440 000 081 3740 5104 4320 000 074 3700 4850 4340 000 090 singlestep approach 1640 2670 2360 100 100 3960 5044 4560 100 100 4680 5769 5260 100 100 adaptiveadaptive retrieval 1880 3030 2480 050 090 3860 5070 4320 050 087 4420 5511 5060 050 095 selfrag120 820 1180 068 166 560 1786 3060 076 167 300 1914 3900 090 181 adaptiverag ours 2180 3260 2960 190 229 4040 5256 4700 093 148 4660 6009 5680 159 223 complex multistep approach 2300 3250 3160 341 361 4580 5836 5220 273 318 5220 6608 6240 336 335 oracle adaptiverag w oracle 2960 4470 3560 090 145 5560 6990 6280 054 108 5220 6990 6660 065 121 performance per dataset in addition to detail ing the performance of each dataset with the flan t5xl model as shown in table 2 we also present the results for each dataset with the flant5 xxl and gpt35 models in table 2",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3429,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "49e254d5-4234-463f-9173-4aa8fdd1f927",
        "embedding": null,
        "metadata": {
            "page_label": "15",
            "file_name": "Adaptive-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 577416,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "94b90e09-6436-4580-bb20-72f787ab09b3",
                "node_type": "4",
                "metadata": {
                    "page_label": "15",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "3c61f9256bfb3e4a4fa369ef4dccb672903f9cbefbbcda5f2b9d731d24dcff3b",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "22afd7b6-c359-492b-8e13-d5cc33e881e7",
                "node_type": "1",
                "metadata": {
                    "page_label": "15",
                    "file_name": "Adaptive-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Adaptive-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 577416,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "7b0d8f317422ae69ea97229d202c64810aa466ed2d065b5be81ebd1093f3b035",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "076 167 300 1914 3900 090 181 adaptiverag ours 2180 3260 2960 190 229 4040 5256 4700 093 148 4660 6009 5680 159 223 complex multistep approach 2300 3250 3160 341 361 4580 5836 5220 273 318 5220 6608 6240 336 335 oracle adaptiverag w oracle 2960 4470 3560 090 145 5560 6990 6280 054 108 5220 6990 6660 065 121 performance per dataset in addition to detail ing the performance of each dataset with the flan t5xl model as shown in table 2 we also present the results for each dataset with the flant5 xxl and gpt35 models in table 2 and table 8 respectively the experimental results show that our adaptiverag consistently balances between efficiency and accuracy it is worth noting that while the gpt35 model performs effectively in addressing straightforward queries even without document retrieval it benefits significantly from our adaptiverag in terms of effectiveness when solving complex multihop queries",
        "mimetype": "text/plain",
        "start_char_idx": 2901,
        "end_char_idx": 3807,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "682cd89f-b8fd-4091-9f25-8f51bea00bfa",
        "embedding": null,
        "metadata": {
            "page_label": "1",
            "file_name": "RAFT.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
            "file_type": "application/pdf",
            "file_size": 784351,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "d9eda127-52ae-41eb-ac24-4947f9caaff9",
                "node_type": "4",
                "metadata": {
                    "page_label": "1",
                    "file_name": "RAFT.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
                    "file_type": "application/pdf",
                    "file_size": 784351,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "15aad394e7c3cc06f67386d4cec3b1974f9d4dada4c5c110952e40f503671c94",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint under review raft adapting language model to domain specific rag tianjun zhang department of computer science uc berkeley berkeley ca 94720 usa tianjunzberkeleyedushishir g patil naman jain sheng shen department of computer science uc berkeley berkeley ca 94720 usa shishirpatilnaman_jainshengsberkeleyedu matei zaharia ion stoica joseph e gonzalez department of computer science uc berkeley berkeley ca 94720 usa mateiistoicajegonzalberkeleyedu abstract pretraining large language models llms on large corpora of textual data is now a standard paradigm when using these llms for many downstream applications it is common to additionally incorporate new in formation into the pretrained model either through ragbasedprompting or finetuning however the best methodology to incorporate information remains an open question in this paper we present retrieval augmented fine tuning raft a training recipe which improves the models ability to answer questions in openbook indomain settings in training raft given a question and a set of retrieved documents we train the model to ignore those documents that dont help in answering the question which we call distractor documents raft accomplishes this by citing verbatim the right sequence from the relevant document to help answer the question this coupled with rafts chainofthoughtstyle response helps improve the models ability to reason in domain specific rag raft consistently improves the models performance across pubmed hotpotqa and gorilla datasets presenting a posttraining recipe to improve pretrained llms to indomain rag 1 introduction trained on vast quantities of public data large language models llms have achieved significant advances in a wide range of general knowledge reasoning tasks brown et al 2020 wei et al 2022 however increasingly llms are being employed in specialized domains to support tasks ranging from code completion for specific software frameworks to question answering on specific document collections eg legal or medical documents in these settings general knowledge reasoning is less critical and instead the primary goal is to maximize accuracy based on a given set of documents indeed adapting llms to the specialized domains eg recent news enterprise private documents or program resources constructed after the training cutoff is essential to many emerging applications vu et al 2023 lazaridou et al 2022 and is the focus of this work this paper studies the following question  how do we adapt pretrained llms for retrieval augmented generation rag in specialized domains when it comes to adapting llms to specialized domains we consider the following two candidates incontext learning through retrievalaugmented generation rag and super vised finetuning rag based methods allow the llm to reference the documents when corresponding author personal website tianjunzgithubio 1arxiv240310131v2 cscl 5 jun 2024",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2904,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "927c97bf-8a05-4159-a68c-0e25384f1704",
        "embedding": null,
        "metadata": {
            "page_label": "2",
            "file_name": "RAFT.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
            "file_type": "application/pdf",
            "file_size": 784351,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "2826868d-b40a-45a6-ad6b-21d441c5fd8b",
                "node_type": "4",
                "metadata": {
                    "page_label": "2",
                    "file_name": "RAFT.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
                    "file_type": "application/pdf",
                    "file_size": 784351,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "995fea9eb73a3baffa6ea3e44c6f2a6bc5bea457a2f2a248de92170d56e5de33",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint under review open book query answerclosed book query answerbake in knowledge at train timemodel can use external docs at test raft proposed query answerteach model to use external docs at test figure 1 how best to prepare for an exam a finetuning based approaches implement studying by either directly memorizing the input documents or answering practice qa without referencing the documents b alternatively incontext retrieval methods fail to leverage the learning opportunity afforded by the fixed domain and are equivalent to taking an openbook exam without studying in contrast our approach c raft leverages finetuning with questionanswer pairs while referencing the documents in a simulated imperfect retrieval setting  thereby effectively preparing for the openbook exam setting answering questions however rag based incontext learning methods fail to leverage the learning opportunity afforded by the fixed domain setting and early access to the test documents alternatively supervised finetuning offers the opportunity to learn more general patterns in the documents and better align to end tasks and user preferences zhou et al 2023 however existing finetuning based approaches either fail to leverage the documents at test time dont incorporate rag or fail to account for the imperfections in retrieval process during training we can draw an analogy to an openbook exam existing incontext retrieval methods are equivalent to taking an openbook exam without studying alternatively existing fine tuning based approaches implement studying by either directly memorizing xiong et al 2023 the input documents or answering practice questions wang et al 2022 without referencing the documents while these approaches leverage indomain learning they fail to prepare for the openbook nature of the test setting in this paper we study how to combine instruction finetuning ift with retrieval aug mented generation rag we propose a novel adaptation strategy  retrievalaugmented fine tuning raft raft specifically addresses the challenge of finetuning llms to both incorporate domain knowledge while also improving indomain rag performance raft aims to not only enable models to learn domainspecific knowledge through finetuning but also to ensure robustness against distracting retrieved information this is achieved by training the models to understand the dynamics between the question prompt the domainspecific documents retrieved and the right answer going back to our analogy to the open book exam our approach is analogous to studying for an openbook exam by recognizing relevant and irrelevant retrieved documents in raft we train the model to answer the question q from documents d to generate answer a where a includes chainofthought reasoning wei et al 2022 anthropic 2023 and in the presence of distractor documents  dk we explain the methodology in section 3 and analyze the sensitivity to the number of distractor documents  k at train and test time in section 5 raft consistently outperforms supervisedfinetuning both with and without rag across pubmed dernoncourt  lee 2017 hotpot qa yang et al 2018 and huggingface hub torch hub and tensorflow hub gorilla datasets patil et al 2023 presenting a novel yet simple technique to improve pretrained llms for indomain rag our code is available at httpsgithubcomshishirpatilgorilla  2 llms for openbook exam to understand our goal better we expand on our analogy between training an llm with the realworld setting of prepararing for an exam closedbook exam a closed book exam often refers to the scenario where the llms do not have access to any additional documents or references to answer the questions during 2",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3679,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "cbc80c33-8c5a-4e18-8b75-69fe2ca28b3f",
        "embedding": null,
        "metadata": {
            "page_label": "3",
            "file_name": "RAFT.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
            "file_type": "application/pdf",
            "file_size": 784351,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "85aa6c01-108e-42b2-9b55-b9df4cc8ef51",
                "node_type": "4",
                "metadata": {
                    "page_label": "3",
                    "file_name": "RAFT.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
                    "file_type": "application/pdf",
                    "file_size": 784351,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "f4e7e2e2f147d22cf726549d5335b88789409b19ecac0d07f7dafdfc523a65e0",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint under review figure 2 overview of our raft method the topleft figure depicts our approach of adapting llms to reading solution from a set of positive and distractor documents in contrast to standard rag setup where models are trained based on the retriever outputs which is a mixture of both memorization and reading at test time all methods follow the standard rag setting provided with a topk retrieved documents in the context the exam for llms this is equivalent to the scenario for example in which the llm is used as a chatbot in this scenario the llm draws from the knowledge baked in during pretraining and supervisedfinetuning to respond to the users prompt open book exam in contrast we liken the openbook exam setting to the scenario in which the llm can refer to external sources of information eg a website or a book chapter in such scenarios typically the llm is paired with retriever which retrieves k documents or specific segments of the document which are appended to the users prompt it is only through these documents retrieved that the llm gains access to domainspecific information as a result we argue that the llms performance in these settings where it is trained as a generalpurpose llm is largely dependent on the quality of the retriever and how accurately the retriever can identify the most relevant piece of information domainspecific openbook exam in this paper we focus on the narrower but increas ingly popular domain than the general open book exam which we call the domainspecific openbook exam here we know apriori the domain in which the llm will be tested the llm can respond to the users prompt using use any and all information from this specific domain which it has been finetuned on examples of domain specific examples include enterprise documents code repositories belonging to an organization etc in all these scenarios the llm will be used to respond to the questions whose answers can be found within a collection of documents the retrieval technique itself has little to noimpact on the mechanism though it may impact the accuracy this paper studies the domainspecific openbook setting and how to adapt a pretrained llm to this specific domain including how to make it more robust to a varying number of retrieved documents and distractors 3 raft in this section we present raft a novel way of training llms for domainspecific open book exams we first introduce the classical technique of supervised finetuning followed with the key takeaways from our experiments then we introduce raft  a modified version of general instruction tuning lastly we provide an overview of the experiments to expect in the later sections supervised finetuning consider the supervised finetuning sft setting for a questionanswer dataset the formulation consists of the dataset  d from which a set of question  q and corresponding answer  a pairs are derived or already available in the classical sft setting the model is trained to improve its ability to answer the questions based on its knowledge  obtained either during pretraining or during the sft training phase the model so trained can also 3",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3136,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "22e5d93c-07e4-4fd6-b212-d9250e82358d",
        "embedding": null,
        "metadata": {
            "page_label": "4",
            "file_name": "RAFT.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
            "file_type": "application/pdf",
            "file_size": 784351,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "76a5fcaf-b9b4-4f3d-b40e-5466ac7e92fd",
                "node_type": "4",
                "metadata": {
                    "page_label": "4",
                    "file_name": "RAFT.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
                    "file_type": "application/pdf",
                    "file_size": 784351,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "5a2eac64027431a897d8788098f6a9230002f367b6794fc68dba856f3c467c2a",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint under review be used at testtime with retrieval augmented generation rag setting where additional documents can be introduced in the prompt to help the model answer the question this can be represented as follows train qa 0shot inference qa rag inference qda raft retrieval augmented finetuning raft presents a novel recipe to prepare fine tuning data to tailor the models for domainspecific openbook setting equivalent to in domain rag in raft we prepare the training data such that each data point contains a question  q a set of documents  dk and a corresponding chainofthough style answer a generated from one of the document  d we differentiate between two types of documents golden documents  d ie the documents from which the answer to the question can be deduced and distractor documents  di that do not contain answer relevant information as an implementation detail the golden document doesnt need to be a single document but can be more than one document as is the case in hotpotqa yang et al 2018 then for pfraction of the questions  qi in the dataset we retain the golden document  d i along with distractor documents  dk1 for 1pfraction of the questions qi in the dataset we include no golden document and only include distractor documents dk we then finetune the language model using standard supervised training sft technique training it to generate answers from the provided documents and question fig 2 illustrates the highlevel design principal for raft  we demonstrate that our rag approach trains the model to perform better rag on the set of documents it is trained on ie indomain  by removing the golden documents in some instances we are compelling the model to memorize answers instead of deriving them from the context the training data for raft is as follows and an example training data can be seen in fig 3 p of data qdd1d2     dka 1p  of data qd1d2     dka subsequently for the test scenario the model is provided with the q and topk documents retrieved by the rag pipeline note that raft is independent of the retriever used a key factor in enhancing training quality is the generation of a reasoning process such as chainofthought to explain the provided answers raft approach is similar we demonstrate that creating a full reasoning chain and inaddition clearly citing sources enhances the models accuracy in answering questions in fig 3 we illustrate this set up generating the training data in this fashion involves presenting the model with a question context and verified answers and then requesting it to form a reasoning chain that appropriately references the original context for all the datasets in our experiments we generate the answers using the technique described above note that the gorilla apibench dataset already includes reasoning in the answers we provide an example of the generation step in fig 3 the detailed reasoning answer includes a citation from the original context inside begin_quote and end_quote as well as the detailed explanation on how to reach the conclusion based on the citations we demonstrate that adding detailed reasoning paragraphs can help boost the models performance in our experiment section 4 evaluation we design our experiments to study how well raft performs compared to various base lines we find that the raft7b model a finetuned version of llama2 is better at reading and extracting information from indomain documents than domainspecific finetuned model and generalpurpose model with rag as an ablation we also demonstrate how important it is for the model to learn with chainofthought responses in this section we will first introduce all the datasets we used in the experiments then all the baseline modelfinetuning techniques that we benchmark against 4",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3753,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "a2e974bf-352f-48f1-9692-d953dd68d091",
        "embedding": null,
        "metadata": {
            "page_label": "5",
            "file_name": "RAFT.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
            "file_type": "application/pdf",
            "file_size": 784351,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "ac214962-a4a0-478f-8b99-fcb0a078ff2a",
                "node_type": "4",
                "metadata": {
                    "page_label": "5",
                    "file_name": "RAFT.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
                    "file_type": "application/pdf",
                    "file_size": 784351,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "f3cdfa39451820042725b22cf798b7101908667e80de20659cd5f6ca05dd4cf1",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint under review question the oberoi family is part of a hotel company that has a head office in what city context the oberoi family is an indian family that is famous for its involvement in hotels namely through the oberoi groupit is located in city center of jakarta near mega kuningan adjacent to the sister jw marriott hotel it is operated by the ritzcarlton hotel company the complex has two towers that comprises a hotel and the airlangga apartment respectivelythe oberoi group is a hotel company with its head office in delhi instruction given the question context and answer above provide a logical reasoning for that answer please use the format of reason reason answer answer  cot answer reason the document begin_quote the oberoi family is an indian family that is famous for its involvement in hotels namely through the oberoi group end_quote establishes that the oberoi family is involved in the oberoi group and the document begin_quote the oberoi group is a hotel company with its head office in delhi end_quote establishes the head office of the oberoi group therefore the oberoi family is part of a hotel company whose head office is in delhi answer delhi figure 3 raft prompt to help llm evaluate its own generated reasoning and answers contrasting them with the correct reasoning and answers the llm is prompted to identify errors in its reasoning and extract key insights for improvement this figure specifically represents the generateexplanation step in the raft algorithm section 3 table 1 raft improves rag performance for all specialized domains  across pubmed hotpot huggingface torch hub and tensorflow hub we see that domainspecific fine tuning improves significantly of the performance of the base model raft consistently outperforms the existing domainspecific finetuning method with or without rag this suggests the need to train the model with context we compare our model with llama finetuning receipes and provide gpt35 for reference pubmed hotpot huggingface torch hub tensorflow gpt35  rag 7160 415 2908 6021 6559 llama27b 565 054 022 0 0 llama27b  rag 588 003 2643 0860 4306 dsf 597 638 6106 8494 8656 dsf  rag 716 441 4259 8280 6029 raft llama27b 7330 3528 7400 8495 8686 datasets in our experiments we use the following datasets to evaluate our model and all baselines we selected these datasets to represent both popular and diverse domains including wikipedia codingapi documents and questionanswering on medical docu ments natural questions nq kwiatkowski et al 2019 trivia qa joshi et al 2017 and hotpotqa yang et al 2018 are the opendomain questionanswers based on wikipedia mainly focused on common knowledge eg movies sports etc huggingface torch hub and tensorflow hub are from the apibench patil et al 2023 proposed in the gorilla paper these benchmarks measure how to generate the correct functional and executable api calls based on the documentation pubmed qa jin et al 2019 is a questionanswering dataset tailored only for biomedicalresearch questionanswering it mainly focuses on answering medical and biology questions based on a given set of documents we would 5",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3122,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "cbb2cbba-b26e-4b98-8d4f-4b8d58e08d44",
        "embedding": null,
        "metadata": {
            "page_label": "6",
            "file_name": "RAFT.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
            "file_type": "application/pdf",
            "file_size": 784351,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "5d0920d3-7be5-4a73-93c5-9d0f56f9982b",
                "node_type": "4",
                "metadata": {
                    "page_label": "6",
                    "file_name": "RAFT.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
                    "file_type": "application/pdf",
                    "file_size": 784351,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "9bb024aa8dbaaa7ae9e998f2905691d89b2769e3c4c839ec91d2a4352dda9e68",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint under review like to highlight that nq trivia qa and hotpotqa are relatively general domain whereas the latter two domains are on domainspecific documents baselines we consider the following baselines for our experiments llama27bchat model with 0shot prompting this is the commonly used instructionfinetuned model for qa tasks where we provide clearly written instruc tions but no reference documentation llama27bchat model with rag llama2  rag similar to the previous setting except here we include reference documents this is a popular technique when dealing with domainspecific qa tasks domainspecific finetuning with 0shot prompting dsf standard supervised finetuning without documents in context we find that its mostly useful to align the answering style of the model as well as get familiar with the domain context domainspecific finetuning with rag dsf  rag equip a domainspecific finetunedmodel with external knowledge using rag so for the knowledge the model does not know it can still refer to the context 41 results using the above datasets and baselines we evaluate our model raft and demonstrate the effectiveness of raft in tab 1 we see that raft consistently and significantly outperforms the baselines compared with the base llama2 instructiontuned model raft with rag does much better in terms of extracting information as well as being robust towards distractors the gain can be as big as 3525 on hotpot qa and 7635 on torch hub evaluation compared with dsf on the specific dataset our model does better at relying on the provided context to solve the problem raft does much better on the tasks like hotpot and huggingface datasets 3087 on hotpot and 3141 on huggingface note that for pubmed qa since it is a binary yesno question we dont observe significant gains when we compare our model with dsf  rag even compared with a much larger and better model gpt35 raft demonstrates significant advantages overall the llama7b model both with and without the rag performs poorly due to its answering style not aligning with the ground truth by applying domainspecific tuning we significantly enhance its performance this process enables the model to learn and adopt the appropriate style of answering however introducing rag to a domainspecifically finetuned dsf model doesnt invariably lead to better outcomes this might indicate that the model lacks training in context processing and extracting useful information from it by incorporating our method raft  we train the model not only to match its answering style with that required but also to improve its document processing capabilities consequently our approach outperforms all others 42 effect of cot we also conduct an analysis to evaluate the effectiveness of the chainofthought approach in enhancing the models performance as indicated in table 2 simply providing the answer to a question may not always be adequate this approach can lead to a rapid decrease in loss resulting in the model beginning to overfit incorporating a reasoning chain that not only guides the model to the answer but also enriches the models understanding can improve the overall accuracy and prevent overfitting to concise answers in our experiments integrating the chainofthought significantly enhances training robustness we employ gpt41106 to generate our chainofthought prompts and include an example of the prompt we used in figure 3 43 qualitative analysis to illustrate the potential advantages of raft over the domainspecifically finetuned dsf approach we present a comparative example in figure 4 this example qualitatively 6",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3593,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "53325750-6f51-4d0e-b06f-a86206e79b01",
        "embedding": null,
        "metadata": {
            "page_label": "7",
            "file_name": "RAFT.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
            "file_type": "application/pdf",
            "file_size": 784351,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "2c55480e-9574-4648-b69c-0d00b9912acc",
                "node_type": "4",
                "metadata": {
                    "page_label": "7",
                    "file_name": "RAFT.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
                    "file_type": "application/pdf",
                    "file_size": 784351,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "fc13c617f3dc7974643f77a435faa61d98f2b5d73b6c0896a95a6b3667e1faff",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint under review table 2 ablation on chainofthought  the numbers of raft and raft without cot results on various datasets show that adding cot can significantly improve the performance of the finetuned model with a gains of 966 and 1493 in the hotpot qa and huggingface datasets respectively pubmed hotpotqa huggingface torch hub tensorflow raft wo cot 6830 2562 5907 8656 8321 raft 7330 3528 7400 8495 8686 demonstrates a scenario where the dsf model becomes confused by a question asking for the identity of a screenwriter instead of providing the correct name it mistakenly cites one of the films written by the screenwriter in contrast the raft model accurately answers the question this discrepancy suggests that training a model solely with questionanswer pairs may impair its ability to derive relevant context from provided documents the comparison underscores the importance of incorporating both standard instructional tuning and context comprehension into the training dataset to preserve and enhance the models ability to process text effectively hotpot qa question what screenwriter with credits for evolution01879e a film starring nicolas cage and t\u00e9a leoni documents   david weiss man is a screen writer and director his film credits include the family man 2000 evo lution 2001 and when in rome 2010    the family man is a 2000 american romantic comedydrama film directed by brett ratner written by david diamond and david weissman and starring nico lascage and t\u00e9aleoni      raft reason the screenwriter with credits for the film evolution starring nicolas cage and t\u00e9a leoni is david weissman this information is provided in the reference documents where it mentions david weissman as a screenwriter with film credits including the family man 2000 evolution 2001 and when in rome 2010 therefore the screenwriter for evolution is david weissman answer david weiss man dsf the fam ilyman figure 4 comparison of raft and dsf  on the hotpot qa dataset we can see that dsf model extracts the wrong information from the context when the question is asking who is the screen writer and it answers a film name raft manages to get the accurate results  44 should we train the llm always with the golden context for rag in our exploration of whether large language models llms should always be trained with the golden context for retrievalaugmented generation rag we address a key question what proportion p of the training data should include golden documents intuitively one might assume that for effective training in reading and extracting information from context eg rag tasks the golden document should always be included during training p  100 however our findings challenge this assumption incorporating a portion of the training data without the golden document in the context p  80 appears to enhance the models performance on rag tasks 7",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2860,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "5aa413d1-699b-4273-83fa-50d938313287",
        "embedding": null,
        "metadata": {
            "page_label": "8",
            "file_name": "RAFT.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
            "file_type": "application/pdf",
            "file_size": 784351,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "f0f5461b-d9da-4456-a198-d66a7e7e5ad5",
                "node_type": "4",
                "metadata": {
                    "page_label": "8",
                    "file_name": "RAFT.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
                    "file_type": "application/pdf",
                    "file_size": 784351,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "c7c885bbd1674bbfa47ccf92a3711e2691bd8ba349bacd8518c6260bd64fbfc9",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint under review 0 20 40 60 80 100 p  golden retrieved context at training025030035040045final accuracy t est domain nq 0 20 40 60 80 100  golden retrieved context at training050055060065final accuracy t est domain tqa 0 20 40 60 80 100 p  golden retrieved context at training040045050055060final accuracy t est domain hopo figure 5 how many golden documents to involve we study the hyperparameter p where it indicates how much portion of training data is with golden document results on nq tqa and hotpotqa suggest that mixing some amount of data that the golden document is not put in the context is helpful for indomain rag figure 5 presents our investigation into the hyperparameter p which represents the percentage of training instances that should include golden documents we find that the optimal proportion varies across datasets with p ranging from 40 60 and 100 this indicates that training your llm without the correct corresponding context at times can be beneficial for the downstream task of answering questions related to the documents in our training setup we include four distractor documents alongside the golden document and at test time we maintain this format by providing the golden document with four distractors our findings suggest that for domainspecific rag tasks including a certain percentage of training data without the golden documents in the context proves to be advantageous 5 raft generalizes to topk rag we now study another important problem how does the number of distractor documents in raft affect the models performance when augmented with topk rag results during evaluation previous research has highlighted the vulnerability of llms to irrelevant text see studies shi et al 2023a weston  sukhbaatar 2023 liu et al 2023 this issue is particularly critical for llms  rag since topk rag is frequently employed at test time to ensure high recall such a scenario necessitates the model to have the ability to discern and disregard irrelevant content focusing solely on pertinent information 51 making model robust to topk rag to tackle the challenge of enhancing large language models llms ability to sift through irrelevant text within the retrieval pipeline our analysis revealed that training solely with golden highly relevant documents can inadvertently diminish the models ability to dis cern and disregard irrelevant information to address this our algorithm raft  adopts a strategy that integrates golden documents with a mix of irrelevant ones this method ology prompts us to investigate the ideal fraction of distractor irrelevant documents to incorporate throughout the training process and to assess how well this training approach adapts to different volumes of documents encountered by the retrievalaugmented gen eration rag during the test phase our aim is to refine the balance between relevant and irrelevant information to strenghten the models efficiency in identifying and utilizing pertinent content notice that sec 44 looked what what p of training data should include distractors while in this section we study testtime scenarios training with distractor documents to enhance the robustness of llms against irrelevant text in retrieved documents we adopted a finetuning approach that incorporates both golden highly relevant documents and distractor irrelevant documents the model was trained with varying numbers of distractor documents but consistently evaluated using the top3 documents obtained from the retriever  not to be confused with p our findings detailed in fig 6 reveal that finetuning with only the golden document frequently results in inferior performance compared to configurations that include a greater number of distractor documents as we can see in the figure the better performance for natural questions is 8",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3797,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "cdff7336-96b7-4886-a0a7-a80e29b4a4e5",
        "embedding": null,
        "metadata": {
            "page_label": "9",
            "file_name": "RAFT.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
            "file_type": "application/pdf",
            "file_size": 784351,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "b3fce14a-8daf-4916-9e03-381931fa2e5a",
                "node_type": "4",
                "metadata": {
                    "page_label": "9",
                    "file_name": "RAFT.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
                    "file_type": "application/pdf",
                    "file_size": 784351,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "f00f6518e0bfa10ecffb4e5ccf59d95eda79f7215f17f80d72f411665ad591ff",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint under review 2 4 6 8 10  t est documents t opk022024026028030032final accuracy natural questions train d train d  1d train d  2d train d  3d 2 4 6 8 10  t est documents t opk012501500175020002250250final accuracy hotpot qa train d train d  1d train d  2d train d  3d figure 6 testtime documents varying  to analyze how robust raft is to varying number of testtime documents we study three domains  nq trivia qa and hotpot qa in nq we find that training with 4 documents leads to optimal performance and this changes to 3 and 2 for for trivia qa and hotpot qa respectively however we see that training with only golden documents leads to poor performance training with d3dand it is d1ddocuments with hotpot qa this insight has been particularly beneficial for our algorithm raft  in our experiments we consistently employ a training setup consisting of one golden document alongside four distractor documents generalization to a variable number of testtime documents we extended our research to examine the impact of different quantities of testtime documents on the models per formance specifically our experiments focused on assessing how models trained with varying numbers of distractor documents respond to changes in the number of documents presented at test time the results illustrated in fig 6 confirm that the inclusion of distrac tor documents during training indeed makes the model more resilient to fluctuations in the number of documents encountered during testing this ability to maintain consistent perfor mance despite variations in testtime document numbers further validates the robustness of our approach raft  this finding underscores the importance of a wellcalibrated training environment to prepare the model for a range of scenarios it may encounter in realworld 6 related works retrievalaugmented language models retrievalaugmented language models ralms enhance llms by integrating a retrieval module that sources relevant information from external knowledge bases significantly improving performance across various nlp tasks including language modeling guu et al 2020 borgeaud et al 2022 khandelwal et al 2019 shi et al 2023d lin et al 2023b shi et al 2023c asai et al 2023 xu et al 2023 wang et al 2023 and opendomain question answering izacard et al 2023 lewis et al 2020 for instance atlas izacard et al 2023 finetunes t5 models with the retriever treating documents as latent variables while retro borgeaud et al 2022 modifies the decoderonly architecture to include retrieved texts and conducts pretraining from scratch knnlm khandelwal et al 2019 interpolates between the lms next token distribution and distributions computed from retrieved tokens at inference shi et al 2023d ram et al 2023 assume blackbox access to an llm combining it with either offtheshelf or finetuned retriever memorization a key question around large neural language models is whether they truly understand text feldman 2020 power et al 2022 or simply rely on surface pattern memorization carlini et al 2019 t\u00e4nzer et al 2022 feldman 2020 carlini et al 2019 2022 develop methodologies to quantify the extent of memorization in neural models brown et al 2020 power et al 2022 liu et al 2022 further explored how memorization impacts the models generalization capabilities carlini et al 2021 shi et al 2023b demonstrated the ability of language models to memorize and regurgitate training data raising significant privacy concerns kandpal et al 2022 pan et al 2020 finetuning for rag more recently several papers have been exploring the idea of fine tuning a pretrained llm to be better at rag tasks lin et al 2023a wang et al 2023 xu 9",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3651,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "e3115b27-c047-4802-8b8f-44fe00ae37c0",
        "embedding": null,
        "metadata": {
            "page_label": "10",
            "file_name": "RAFT.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
            "file_type": "application/pdf",
            "file_size": 784351,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "93a5175a-95b3-4f81-8841-aa2f4e926903",
                "node_type": "4",
                "metadata": {
                    "page_label": "10",
                    "file_name": "RAFT.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
                    "file_type": "application/pdf",
                    "file_size": 784351,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "5a0f49774b9d4a2a38a8d3efe83883aaace11ae582ffc0fd6ae48bef09a61fe0",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint under review et al 2023 liu et al 2024 these works focus on constructing a combination of finetuning dataset for rag and train a model to perform well on these tasks in particular in their settings at test time the domain or documents can be different than the training time whereas our paper studies a slightly opposite scenario where we only care about testing the llm on the same set of documents 7 conclusion raft is a training strategy designed to enhance the models performance in answering questions within a specific domain in openbook settings we highlight several crucial design decisions such as training the model alongside distractor documents organizing the dataset so a portion lacks golden documents in their context and formulating answers in a chainofthought manner with direct quotations from the relevant text our evaluations on pubmed hotpotqa and gorilla api bench underline rafts significant potential references anthropic prompt engineering for claudes long context window 2023 asai a wu z wang y sil a and hajishirzi h selfrag learning to retrieve generate and critique through selfreflection arxiv preprint arxiv231011511  2023 borgeaud s mensch a hoffmann j cai t rutherford e millican k van den driess che g b lespiau jb damoc b clark a et al improving language models by retrieving from trillions of tokens in international conference on machine learning  pp 22062240 pmlr 2022 brown t mann b ryder n subbiah m kaplan j d dhariwal p  neelakantan a shyam p  sastry g askell a et al language models are fewshot learners advances in neural information processing systems  3318771901 2020 carlini n liu c erlingsson \u00fa kos j and song d the secret sharer evaluating and testing unintended memorization in neural networks in 28th usenix security symposium usenix security 19  pp 267284 2019 carlini n tramer f wallace e jagielski m herbertvoss a lee k roberts a brown t song d erlingsson u et al extracting training data from large language models in 30th usenix security symposium usenix security 21  pp 26332650 2021 carlini n ippolito d jagielski m lee k tramer f and zhang c quantifying memorization across neural language models in the eleventh international conference on learning representations  2022 dernoncourt f and lee j y pubmed 200k rct a dataset for sequential sentence classification in medical abstracts arxiv preprint arxiv171006071  2017 feldman v  does learning require memorization a short tale about a long tail in proceedings of the 52nd annual acm sigact symposium on theory of computing  pp 954959 2020 guu k lee k tung z pasupat p  and chang m retrieval augmented language model pretraining in international conference on machine learning  pp 39293938 pmlr 2020 izacard g lewis p  lomeli m hosseini l petroni f schick t dwivediyu j joulin a riedel s and grave e atlas fewshot learning with retrieval augmented language models journal of machine learning research  24251143 2023 url http jmlrorgpapersv24230037html  jin q dhingra b liu z cohen w w and lu x pubmedqa a dataset for biomedical research question answering arxiv preprint arxiv190906146  2019 10",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3113,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "361feeff-e3bb-450c-b5fc-449e66d439ac",
        "embedding": null,
        "metadata": {
            "page_label": "11",
            "file_name": "RAFT.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
            "file_type": "application/pdf",
            "file_size": 784351,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "af8985a6-44de-4fa9-97ff-51c9a0492f2a",
                "node_type": "4",
                "metadata": {
                    "page_label": "11",
                    "file_name": "RAFT.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
                    "file_type": "application/pdf",
                    "file_size": 784351,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "ce26ddb7975be5efbc28bdaa85f509431a3cacf2db8cefc77b6d98242ccb7a07",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint under review joshi m choi e weld d s and zettlemoyer l triviaqa a large scale distantly supervised challenge dataset for reading comprehension arxiv preprint arxiv170503551  2017 kandpal n wallace e and raffel c deduplicating training data mitigates privacy risks in language models in international conference on machine learning  pp 1069710707 pmlr 2022 khandelwal u levy o jurafsky d zettlemoyer l and lewis m general ization through memorization nearest neighbor language models arxiv preprint arxiv191100172  2019 kwiatkowski t palomaki j redfield o collins m parikh a alberti c epstein d polosukhin i devlin j lee k et al natural questions a benchmark for question answering research transactions of the association for computational linguistics  7453466 2019 lazaridou a gribovskaya e stokowiec w and grigorev n internetaugmented language models through fewshot prompting for opendomain question answering arxiv preprint arxiv220305115  2022 lewis p  perez e piktus a petroni f karpukhin v  goyal n k\u00fcttler h lewis m yih wt rockt\u00e4schel t et al retrievalaugmented generation for knowledgeintensive nlp tasks advances in neural information processing systems  3394599474 2020 lin x v  chen x chen m shi w lomeli m james r rodriguez p  kahn j szilvasy g lewis m et al radit retrievalaugmented dual instruction tuning arxiv preprint arxiv231001352  2023a lin x v  chen x chen m shi w lomeli m james r rodriguez p  kahn j szilvasy g lewis m et al radit retrievalaugmented dual instruction tuning arxiv preprint arxiv231001352  2023b liu n f lin k hewitt j paranjape a bevilacqua m petroni f and liang p  lost in the middle how language models use long contexts arxiv preprint arxiv230703172  2023 liu z kitouni o nolte n s michaud e tegmark m and williams m towards understanding grokking an effective theory of representation learning advances in neural information processing systems  353465134663 2022 liu z ping w roy r xu p  shoeybi m and catanzaro b chatqa building gpt4 level conversational qa models arxiv preprint arxiv240110225  2024 pan x zhang m ji s and yang m privacy risks of generalpurpose language models in2020 ieee symposium on security and privacy sp  pp 13141331 ieee 2020 patil s g zhang t wang x and gonzalez j e gorilla large language model connected with massive apis arxiv preprint arxiv230515334  2023 power a burda y edwards h babuschkin i and misra v  grokking generalization beyond overfitting on small algorithmic datasets arxiv preprint arxiv220102177  2022 ram o levine y dalmedigos i muhlgay d shashua a leytonbrown k and shoham y incontext retrievalaugmented language models arxiv preprint arxiv230200083  2023 shi f chen x misra k scales n dohan d chi e h sch\u00e4rli n and zhou d large language models can be easily distracted by irrelevant context in international conference on machine learning  pp 3121031227 pmlr 2023a shi w ajith a xia m huang y liu d blevins t chen d and zettlemoyer l detecting pretraining data from large language models arxiv preprint arxiv231016789  2023b 11",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3029,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "ea7e1e61-c802-4ce4-ac5d-c4e2a40fd12c",
        "embedding": null,
        "metadata": {
            "page_label": "12",
            "file_name": "RAFT.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
            "file_type": "application/pdf",
            "file_size": 784351,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "784c120a-8e83-413e-824d-e0f89c1d6f82",
                "node_type": "4",
                "metadata": {
                    "page_label": "12",
                    "file_name": "RAFT.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\RAFT.pdf",
                    "file_type": "application/pdf",
                    "file_size": 784351,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "cfa20dff6ddba769682d1d372d8b3ee56ae71d341b05cb9c0ee1b67361813b0c",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint under review shi w min s lomeli m zhou c li m lin v  smith n a zettlemoyer l yih s and lewis m incontext pretraining language modeling beyond document boundaries arxiv preprint arxiv231010638  2023c shi w min s yasunaga m seo m james r lewis m zettlemoyer l and yih wt replug retrievalaugmented blackbox language models arxiv preprint arxiv230112652  2023d t\u00e4nzer m ruder s and rei m memorisation versus generalisation in pretrained lan guage models in proceedings of the 60th annual meeting of the association for computational linguistics volume 1 long papers  pp 75647578 2022 vu t iyyer m wang x constant n wei j wei j tar c sung yh zhou d le q et al freshllms refreshing large language models with search engine augmentation arxiv preprint arxiv231003214  2023 wang b ping w mcafee l xu p  li b shoeybi m and catanzaro b instructretro instruction tuning post retrievalaugmented pretraining arxiv preprint arxiv231007713  2023 wang y kordi y mishra s liu a smith n a khashabi d and hajishirzi h selfinstruct aligning language models with selfgenerated instructions arxiv preprint arxiv221210560  2022 wei j wang x schuurmans d bosma m xia f chi e le q v  zhou d et al chainofthought prompting elicits reasoning in large language models advances in neural information processing systems  352482424837 2022 weston j and sukhbaatar s system 2 attention is something you might need too arxiv preprint arxiv231111829  2023 xiong w liu j molybog i zhang h bhargava p  hou r martin l rungta r sankararaman k a oguz b et al effective longcontext scaling of foundation models arxiv preprint arxiv230916039  2023 xu p  ping w wu x mcafee l zhu c liu z subramanian s bakhturina e shoeybi m and catanzaro b retrieval meets long context large language models arxiv preprint arxiv231003025  2023 yang z qi p  zhang s bengio y cohen w w salakhutdinov r and manning c d hotpotqa a dataset for diverse explainable multihop question answering arxiv preprint arxiv180909600  2018 zhou c liu p  xu p  iyer s sun j mao y ma x efrat a yu p  yu l et al lima less is more for alignment arxiv preprint arxiv230511206  2023 12",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2113,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "347ac25d-8f34-4656-a648-73f139ec1e32",
        "embedding": null,
        "metadata": {
            "page_label": "1",
            "file_name": "Ragnar\u00f6k.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
            "file_type": "application/pdf",
            "file_size": 1681358,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e1b20f6e-dc98-48d6-bd13-1ecc4f824e79",
                "node_type": "4",
                "metadata": {
                    "page_label": "1",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "bedfb4f1057bd71bbde2e62d1b3ed70f7723942763ecedbb8ed38de6895427cb",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "ragnar\u00f6k a reusable rag framework and baselines for trec 2024 retrievalaugmented generation track ronak pradeep university of waterloo waterloo canadanandan thakur university of waterloo waterloo canadasahel sharifymoghaddam university of waterloo waterloo canadaeric zhang university of waterloo waterloo canada ryan nguyen university of waterloo waterloo canadadaniel campos snowflake inc new york usanick craswell microsoft seattle usajimmy lin university of waterloo waterloo canada abstract did you try out the new bing search or maybe you fiddled around with google ai overviews these might sound familiar because the modernday search stack has recently evolved to include retrieval augmented generation rag systems they allow searching and incorporating realtime data into large language models llms to provide a wellinformed attributed concise summary in contrast to the traditional search paradigm that relies on displaying a ranked list of documents therefore given these recent advancements it is crucial to have an arena to build test visualize and systematically evaluate ragbased search systems with this in mind we propose the trec 2024 rag track to foster innovation in evaluating rag systems in our work we lay out the steps weve made towards making this track a reality  we describe the details of our reusable framework ragnar\u00f6k explain the curation of the new ms marco v21 collection choice release the development topics for the track and standardize the io definitions which assist the end user next using ragnar\u00f6k we identify and provide key industrial baselines such as openais gpt4o or coheres command r further we introduce a webbased user interface for an interactive arena allow ing benchmarking pairwise rag systems by crowdsourcing we opensource our ragnar\u00f6k framework and baselines to achieve a unified standard for future rag systems httpsgithubcomcastoriniragnarok 1 introduction retrieval augmented generation rag  10212229 has emerged as a popular technique to augment large language model llm gen eration for knowledgeintensive tasks such as opendomain ques tion answering or fact verification  44 using the top \ud835\udc58retrieved segments from a suitable retrieval system rag systems output an answer summary grounded on the relevant context rag systems mitigate factual inconsistencies in llm outputs  19262934 and enhance interpretability  21 and generalization  20 thus facilitat ing a wider adoption across several domains like medicine  55 and finance 23 several companies provide endtoend rag frameworks such as bing search  39 or google gemini  5 most of these systems are either proprietary or offer limited user customization likewise the absence of a standardized rag framework makes implementing both authors contributed equally to the paper correspondence to ronak pradeep rpradeepuwaterlooca and nandan thakur nandanthakuruwaterloocarag at a large scale challenging implementing atop existing frame works requires custom code for multiple steps including retrieval reranking and generation to promote wider adoption of rag in academia we develop ragnar\u00f6k a userfriendly reusable end toend rag framework offering code for customizable retrievers rerankers and generation models ragnar\u00f6k comprises two key modules r retrieval and ag augmented generation the retrieval module incorporates both the retrieval and reranking stages to yield the top \ud835\udc58retrieved seg ments for an input user topic next the augmented generation module uses the userprovided topic and retrieved segments to produce an rag answer formatted into individual sentences cit ing the relevant information from the top \ud835\udc58retrieved segments ragnar\u00f6k is deeply integrated with existing python frameworks such as pyserini 31 and rank_llm 4647 and can be easily in stalled via pypi using  pip install pyragnarok  the framework offers easytouse rest apis and an integrated webui to enhance userfriendliness and improve the human evaluation experience the ragnar\u00f6k framework will be used for providing baselines in the upcoming trec 2024 retrieval augmented generation rag track1an ideal framework should include a sufficiently large doc ument collection covering diverse information and nonfactoid decompositional topics requiring longform answers in our frame work we deduplicate the existing ms marco v2 document col lection in addition we provide a segment collection using a slidingwindow chunking technique discussed in section 4 fur ther we release two sets of development topics i trecraggy 2024 a filtered subset of topics with longform answers from trec deep learning 202123  1517 and ii trecresearchy 2024 a subset of the researchy questions introduced in rosset et al 51 our ragnar\u00f6k framework supports a headtohead rag battle arena for the answer evaluation heavily inspired by recent work such as the chatbot arena  1358 we include key industrial base lines such as cohere command r  14 and openai gpt4o  41 and evaluate both the baselines using the retrieval setup involving bm25  49 and rankzephyr  47 with human preferences overall we observe gpt4o to provide more detailed answers over com mand r on the development set of topics discussed in section 6 finally we opensource ragnar\u00f6k and make it publicly available at the following url httpsgithubcomcastoriniragnarok in the future we will include a wider variety of llms as baselines and continue to improve our framework 1trec 2024 retrieval augmented generation rag track httpstrecraggithubioarxiv240616828v1 csir 24 jun 2024",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 5495,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "ec3267e5-d817-429c-a5ca-d776067557a9",
        "embedding": null,
        "metadata": {
            "page_label": "2",
            "file_name": "Ragnar\u00f6k.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
            "file_type": "application/pdf",
            "file_size": 1681358,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "0cdabdb2-eff3-4e2a-849e-ff269cf5fe2e",
                "node_type": "4",
                "metadata": {
                    "page_label": "2",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "7b8349a96ad033111ad25932561c1b33865bec135b689fc648a1bf7ea5f20363",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "conference17 july 2017 washington dc usa pradeep and thakur et al r retrieval module bm25 rankzephyr user topic how often should you take your toddler to the potty when potty training  referencesdoc0 doc1  doc19  response_length 728 answer  text  the frequency with which you should take your toddler to the potty depends on their readiness for potty training  citations 0 1 12 13 19   text  if they are reluctant to use the potty dont force them citations 6 8    doc0 how often should i take my toddler to the potty  doc1 how to potty train your kid  doc19 potty training how to get the job done topk segments ag augmented generation module command r gpt4orag output json prompting  llm retrieval  rerank rag retrievalaugmented generation module figure 1 schematic diagram of the ragnar\u00f6k framework given a user topic left the process consists of two steps 1 r retrieval  rerank where the topic yields the top \ud835\udc58relevant segments from our document collection eg potty training articles and 2 ag augmentedgeneration where the retrieved segments with a suitable prompt template is fed to the large language model llm to generate the postprocessed answer response json containing individual sentencelevel citations 2 related work rag frameworks existing rag systems are primarily closed source and difficult to reproduce opensource frameworks such as langchain  12 and llamaindex  33 while available are not researchfriendly and lack proper evaluation and benchmarking flashrag  24 a concurrent work is a similarly motivated toolkit to improve the rag experience for researchers while the frame work is extensive and designed for pipeline flexibility ragnar\u00f6k offers a few additional capabilities  a webui serving a rag battle arena easytouse rest apis a standardized io definition working with sentencelevel citations and a tight integration with popular retrieval  reranking frameworks like pyserini  31 and rankllm collection selection current rag datasets are constructed using the english wikipedia as the document collection however their scale is limited to provide rich and comprehensive information to support rag systems in addition clueweb22  42 offers an exten sive collection of 22 billion curated web pages previously utilized in trec tracks such as the trec conversational assistance track cast  43 and the forthcoming trec interactive knowledge as sistance track ikat  3 another alternative is the ms marco v2 document collection used in the trec deep learning dl track topic selection recently there has been a surge in datasets pro viding topics with longform answers for evaluating rag systems asqa  52 eli5  18 and qampari  4 were utilized for evaluation in the automatic llms citation evaluation alce framework  19 similarly related longform qa datasets include aquamuse  27 expertqa  36 and truthfulqa  32 another recently introduced dataset is clapnq  50 created from the subset of natural ques tions nq  28 and hagrid  25 built on a subset of ms marco dev  8 almost all previous datasets are built on english wikipedia in contrast our work deliberately avoids english wikipedia to pre vent the overfitting commonly seen in existing benchmarks  4054 in our work we reutilize topics from previous trec tracks such as the deep learning dl track because human judgments are available on the ms marco v2 corpora and researchy questions 51 as it covers a wide range of topics based on clueweb22 423 our framework ragnar\u00f6k is an opensource reproducible and reusable framework implementing an endtoend retrievalaugmented generation rag pipeline comprising two modules applied sequentially 1 r re trieval and 2 ag augmented generation through the ragnar\u00f6k framework we will provide several baselines to all participants in the upcoming trec 2024 rag track an overview of the frame work is provided in figure 1 we first describe both modules and expand on the io specifications in our framework retrieval module this module retrieves the relevant segments for a user topic as the input it supports i firststage lexical re trieval models such as bm25  49 and ii reranking models such as rankzephyr  47 the retrieval system searches for relevant segments in the document collection and retrieves the top 100seg ments further reranked by the reranker model to filter out the top20relevant segments for the next stage augmented generation module this module takes in the user topic and the top 20retrieved segments from the retrieval module as the input and a prompting strategy to the large language model llm to generate the answer response with incontext citations for the topic the answer response is divided into individual sentences each sentence within the answer contains text and is grounded on retrieved documents provided as references 31 rag inputoutput definitions rag input the input specifications are straightforward as the user can formulate any question they wish to ask provide the user topic and call our ragnar\u00f6k restapi framework rag output the user receives a json output in response to their topic from the ragnar\u00f6k framework the first key in the output json schema references  provides an ordered list of the top 20 ranked segment ids from our retrieval module next answer  pro vides the llmgenerated rag answer to the user topic presented as a toptobottom list of sentencelevel texts with corresponding seg ment citations all citations are zerobased indexed indicating the exact position of the segment id from the references list finally response_length  provides the total count of the text characters present in the output rag answer",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 5569,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "82873a76-7026-47c9-acef-ea2148c96e37",
        "embedding": null,
        "metadata": {
            "page_label": "3",
            "file_name": "Ragnar\u00f6k.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
            "file_type": "application/pdf",
            "file_size": 1681358,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "16b7960a-b0a1-44ec-91b6-0d4b82270730",
                "node_type": "4",
                "metadata": {
                    "page_label": "3",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "df6d62bdc1ac54c66ee89730d52d40061162f9d442878a154d7bc8b7f7919a49",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "ragnar\u00f6k a reusable rag framework and baselines for trec 2024 retrievalaugmented generation track conference17 july 2017 washington dc usa table 1 comparison of document and segment counts be tween versions v2 and v21 our version after removing near duplicates of the ms marco collection collection version v2 version v21 ours ms marco document 11959635 10960555 ms marco segment 124131414 113520750 4 document collection the ms marco v2 document collection earlier used in the trec dl tracks contains a substantial overlap of nearduplicates docu ments with sufficiently similar text information within the collec tion  1617 when left intact these nearduplicates degrade the downstream retrieval accuracy and reduce the diversity of the col lected documents potentially impacting the effectiveness of rag systems in addition chunking which breaks down a long verbose document into smaller compact representations is a key challenge in rag as the retrieved chunk representations correlate with the rag answer quality 34 ms marco v21 document collection we conduct a deduplica tion strategy in the ms marco v2 document collection to avoid nearduplicates in two stages in the first stage we establish an equivalence class of the documents using locality sensitive hashing lsh with minhash  11 and 9gram shingles next we selected a representative document for each equivalence class for our refined ms marco v21 document collection2reducing the duplicates in the original ms marco v2 document collection by 835 as shown in table 1 ms marco v21 segment collection we segment the ms marco v21 document collection into overlapping segments or chunks and develop the ms marco v21 segment collection3with more than 113 million text segments table 1 we utilize a sliding window technique to generate the segments by fixing the sliding window size of 10 sentences and a stride of 5 sentences to create each seg ment roughly on average between 5001000 characters long to easily map each segment back to the document every segment contains the document id within the segment id further two new fields start_char andend_char indicate the start and the end position character of where the segment begins and ends in the mapped ms marco v21 document collection 5 topic collection topics ie user queries are crucial for robust evaluation of rag systems traditionally popular retrieval and traditional qa bench marks primarily consist of factoid queries where answers are typi cally found within a single sentence or paragraph however these topics lack complexity leading to short answers that can be easily memorized by llms for instance ms marco  8 surprisingly con tains up to 55 factoid queries  951 to avoid shortform answers in rag we utilize two collections containing nonfactoid topics covering information about diverse topics and requiring longform answers we describe these collections below 2ms marco v21 document collection msmarco_v21_doctar 3ms marco v21 segment collection msmarco_v21_doc_segmentedtartable 2 trecraggy and trecresearchy 2024 topic dis tribution the table shows the top 5categories in topic clas sification for trecraggy intrinsic attributes for trec researchy and the first word in all topics definitions in more detail can be found in appendix a  b trecraggy 2024 trecresearchy 2024 topic category first word  intrinsic attributes first word  aggregation 242 what 375 knowledgeintensive 798 how 410 simple w cond 233 how 275 multifaceted 757 why 255 set 208 why 33 reasoningintensive 755 what 150 simple 100 is 25 subjective 485 is 52 comparison 67 when 17 assumptive 257 should 22 trecraggy 2024 we develop trecraggy 2024 a collection with topics filtered from trec deep learning 20212023 tracks  15 17 based on topic category and generatedanswer classification we classify each available topic into seven categories described in appendix a and filter out a subset of topics that either have a long form answer or require information aggregation across multiple sources of information out of the 210 original topics available we filter and include 120 topics 571 in the trecraggy 2024 topic collection4from table 2 we observe 242 of the topics included are aggregation indicating rag systems require to aggregate information from multiple retrieved segments to generate an accu rate longform answer similarly 65 of the topics start with what or how overall a majority of the topics are useful for evaluation containing diverse topic categories requiring a longform answer trecresearchy 2024 researchy questions introduced in ros set et al  51  contains 102k nonfactoid topics with longform answers these topics were curated from bing search logs and eval uated by gpt4 on a scale of 010 based on eight intrinsic attributes such as subjectivity and multifacetedness definitions provided in appendix b notably unlike trecraggy 2024 these queries lack relevance judgments to curate a smaller development subset for a faster evaluation of rag systems we employ a sampler designed to maximize diversity based on the eight intrinsic attributes this is achieved by iteratively selecting the query with the highest \ud835\udc591 norm in the intrinsic attribute space of all eight dimensions rela tive to the alreadysampled set the resultant topic set we dub as trecresearchy 20245from table 2 about 80 of the topics are knowledgeintensive and about 76 are multifaceted highlighting the need for effective rag systems additionally 665 of topics start with how or why emphasizing explanatory questions these distributions suggest that trecresearchy 2024 prioritizes complex and multidimensional topics 6 trec 2024 rag baselines retrieval our retrieval module integrates both firststage retriev ers and rerankers we use bm25 available in anserini  57 with the following default parameters  \ud835\udc58109and\ud835\udc4f04 to retrieve the top 100segments for a given topic next rankzephyr  47 a stateoftheart listwise reranker is used to rerank the top 100can didates we use rankzephyr \ud835\udf0c a variant that reranks the candidates progressively ie makes three passes iteratively refining the final 4trecraggy 2024 topic collection topicsrag24raggydevtxt 5trecresearchy 2024 topic collection topicsrag24researchydevtxt",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 6185,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "a557a241-8c1b-475c-b6a8-6a09eb10f6cb",
        "embedding": null,
        "metadata": {
            "page_label": "4",
            "file_name": "Ragnar\u00f6k.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
            "file_type": "application/pdf",
            "file_size": 1681358,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "237d1826-53a9-45e3-aa8f-0a3a730cc948",
                "node_type": "4",
                "metadata": {
                    "page_label": "4",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "2cf7f167f2a096468e29097dfff45a01fc682338bbf240776d07924c3e4b9663",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "conference17 july 2017 washington dc usa pradeep and thakur et al ranked candidate list to achieve better precision an easytouse implementation of rankzephyr is available via the rank_llm pack age along with various other rerankers like rankgpt  53 which we provide as secondary baselines finally the top 20reranked documents from the document collection are passed onto the next stage for rag generation augmented generation our generation module integrates two popular and commercially available llms i command r is coheres instruction following llm developed for complex rag pipelines  14 ii gpt4o is the latest gpt version from ope nai  41 given that command r cites in a span level we map the citations to their parent sentences for gpt4o we follow the zeroshot chatqa prompt template  35 and cite relevant segments within the text inline using the ieee format an example of the prompt template is shown in figure 2 in the appendix ragbench evaluation evaluating different rag answers is chal lenging as multiple factors within the output response are crucial for effectiveness evaluation to combat this recent works rely on an llmasajudge setup  58 where strong llm assessors judge the raggenerated output in a pairwise evaluation style sidebyside in a headon tournament in our work we briefly overview our baseline techniques using human evaluators a complete illustra tion can be found in table 4 in the appendix the command r baseline outputs shorter answers and cites more relevant segments whereas the gpt4o baseline outputs longer and more detailed answers and cites fewer segments therefore for topics in both trecraggy and trecresearchy 2024 gpt4o intuitively is the better choice for rag answer generation we leave it for future work to empirically compute the win rates in  between our baselines in the ragbench evaluation 61 ragnar\u00f6k system arena heavily inspired by the success of chatbot arena  1358 a crowd sourcing benchmark webui featuring anonymous battles we ex tend the concept to multistage configurable rag pipelines with ragnar\u00f6k in the arena users interact with two unblindedblinded rag systems simultaneously issuing the same topic to both the participants evaluate and select the pipeline that delivers their most preferred response with the identities of the modules in the endto end pipeline revealed after the voting process in the blinded case we leverage gradio  1 to build the webui for ragnar\u00f6k each step of the pipeline uses rest apis for intercommunication enabling easy module switching within the pipeline this modular design simplifies the integration of different retrieval and llm configura tions enhancing scalability and maintainability figure 3 in the appendix illustrates an example topic what inspired pink floyds the wall processed by two different pipelines pipeline a comprising bm25 rankzephyrgpt4o left and pipeline b comprising bm25 rankgpt4ocommand r right in the unblinded tab the outputs generated by each pipeline are compared allowing users to discern which system provided a more satisfactory answer note that when the user hovers the mouse over a citation they can preview the cited segment further in appendix c we discuss the blinded pairwise evaluation and the responses json output tab available in the webui for ragnar\u00f6k7 ongoing work ragnar\u00f6k is the first step for the ongoing work in the trec 2024 rag track by releasing the document collections development topics and baseline strategies for participants we will continue to update the pipelines to include more diverse retrieval models including stateoftheart dual encoders such as articembed  38 and effective pointwisepairwise rerankers  45 we plan to add additional support for more advanced rag techniques like self rag  7 and crag  56 for the trec 2024 rag track test topics we plan to conduct a new and fresh scrape of the bing search logs closer to the submission period this approach will compile a fresh and recent set of topics similar to rosset et al  51  thereby mini mizing the risk of data leakage and ensuring a fair evaluation with existing commercially available llms the next phase of our efforts will focus on finalizing the eval uation methodology using an automatic nuggetbased evaluation following earlier work in lin and demnerfushman 30 and first discussed in the trec rag 2024 presentation deck6the nugget based evaluation is recently gaining popularity  263748 and is becoming the de facto strategy for rag evaluation 8 conclusion the emergence of retrievalaugmented generation rag has rev olutionized modern search systems by allowing realtime data in corporation into large language models llms in our work we develop a reusable endtoend framework ragnar\u00f6k providing reproducible baselines and a webui serving a rag battle arena for retriever reranker and generation models we also introduce the ms marco v21 collection carefully curated topics from the trecdl 20212023 queries and researchy questions and io defi nitions to assist users in the rag paradigm additionally the paper identifies key industrial baselines such as coheres command r and openais gpt4o and includes a qualitative analysis of the baselines on the development topics by opensourcing this frame work we aim to standardize rag applications in preparation for the upcoming trec 2024 rag challenge acknowledgments we thank ian soboroff for the ms marco v2 document collection deduplication for our trec 2024 rag track cohere for providing us with the necessary credits to evaluate commandr and mi crosoft for providing azure credits to evaluate gpt4o additionally we thank corby rosset for the discussions surrounding researchy questions 51 references 1abubakar abid ali abdalla ali abid dawood khan abdulrahman alfozan and james y zou 2019 gradio hasslefree sharing and testing of ml models in the wild corr abs190602569 2019 arxiv190602569 httparxivorgabs 190602569 2marwah alaofi negar arabzadeh charles l a clarke and mark sanderson 2024 generative information retrieval evaluation corr abs240408137 2024 httpsdoiorg1048550arxiv240408137 arxiv240408137 3mohammad aliannejadi zahra abbasiantaeb shubham chatterjee jeffery dal ton and leif azzopardi 2024 trec ikat 2023 the interactive knowledge assistance track overview corr abs240101330 2024 httpsdoiorg10 48550arxiv240101330 arxiv240101330 6httpscsuwaterlooca jimmylinpublicationslin_etal_trec2023planningpdf",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 6383,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "3aa72450-49b6-435e-9823-dc12b12dcd37",
        "embedding": null,
        "metadata": {
            "page_label": "5",
            "file_name": "Ragnar\u00f6k.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
            "file_type": "application/pdf",
            "file_size": 1681358,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "edccc31c-7205-445b-861c-5a6076f724f9",
                "node_type": "4",
                "metadata": {
                    "page_label": "5",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "1add71be1a60801ea59aab34c92cab969e46044d125e0c68c4f31dea8f838083",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "bb20572f-e837-44a7-8da9-7b6f44db745f",
                "node_type": "1",
                "metadata": {},
                "hash": "f8592af8718ae1fd9c1dd37d50dc4c967a34a1117643ba1765806e6da549d0d9",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "ragnar\u00f6k a reusable rag framework and baselines for trec 2024 retrievalaugmented generation track conference17 july 2017 washington dc usa 4samuel joseph amouyal ohad rubin ori yoran tomer wolfson jonathan herzig and jonathan berant 2022 qampari  an opendomain question answering benchmark for questions with many answers from multiple para graphs corr abs220512665 2022 httpsdoiorg1048550arxiv220512665 arxiv220512665 5rohan anil sebastian borgeaud yonghui wu jeanbaptiste alayrac jiahui yu radu soricut johan schalkwyk andrew m dai anja hauth katie mil lican david silver slav petrov melvin johnson ioannis antonoglou julian schrittwieser amelia glaese jilin chen emily pitler timothy p lillicrap ange liki lazaridou orhan firat james molloy michael isard paul ronald barham tom hennigan benjamin lee fabio viola malcolm reynolds yuanzhong xu ryan doherty eli collins clemens meyer eliza rutherford erica moreira ka reem ayoub megha goel george tucker enrique piqueras maxim krikun iain barr nikolay savinov ivo danihelka becca roelofs ana\u00efs white an ders andreassen tamara von glehn lakshman yagati mehran kazemi lu cas gonzalez misha khalman jakub sygnowski and et al 2023 gemini a family of highly capable multimodal models corr abs231211805 2023 httpsdoiorg1048550arxiv231211805 arxiv231211805 6negar arabzadeh and charles l a clarke 2024 a comparison of methods for evaluating generative ir corr abs240404044 2024 httpsdoiorg1048550 arxiv240404044 arxiv240404044 7akari asai zeqiu wu yizhong wang avirup sil and hannaneh hajishirzi 2023 selfrag learning to retrieve generate and critique through self reflection corr abs231011511 2023 httpsdoiorg1048550arxiv2310 11511 arxiv231011511 8payal bajaj daniel campos nick craswell li deng jianfeng gao xiaodong liu rangan majumder andrew mcnamara bhaskar mitra tri nguyen mir rosenberg xia song alina stoica saurabh tiwary and tong wang 2016 ms marco a human generated machine reading comprehension dataset corr abs161109268 2016 arxiv161109268 httparxivorgabs161109268 9valeria bolotova vladislav blinov falk scholer w bruce croft and mark sander son 2022 a nonfactoid questionanswering taxonomy in sigir 22 the 45th international acm sigir conference on research and development in information retrieval madrid spain july 11  15 2022  enrique amig\u00f3 pablo castells julio gonzalo ben carterette j shane culpepper and gabriella kazai eds acm 11961207 httpsdoiorg10114534774953531926 10 sebastian borgeaud arthur mensch jordan hoffmann trevor cai eliza ruther ford katie millican george van den driessche jeanbaptiste lespiau bogdan damoc aidan clark diego de las casas aurelia guy jacob menick roman ring tom hennigan saffron huang loren maggiore chris jones albin cassirer andy brock michela paganini geoffrey irving oriol vinyals simon osindero karen simonyan jack w rae erich elsen and laurent sifre 2022 improving language models by retrieving from trillions of tokens in international con ference on machine learning icml 2022 1723 july 2022 baltimore maryland usa proceedings of machine learning research vol 162  kamalika chaudhuri stefanie jegelka le song csaba szepesv\u00e1ri gang niu and sivan sabato eds pmlr 22062240 httpsproceedingsmlrpressv162borgeaud22ahtml 11 andrei z broder 1997 on the resemblance and containment of documents incompression and complexity of sequences 1997 positano amalfitan coast salerno italy june 1113 1997 proceedings  bruno carpentieri alfredo de santis ugo vaccaro and james a storer eds ieee 2129 httpsdoiorg101109 sequen1997666900 12 harrison chase 2022 langchain  httpsgithubcomlangchainailangchain 13 weilin chiang lianmin zheng ying sheng anastasios nikolas angelopoulos tianle li dacheng li hao zhang banghua zhu michael i jordan joseph e gonzalez and ion stoica 2024 chatbot arena an open platform for evaluating llms by human preference corr abs240304132 2024 httpsdoiorg10 48550arxiv240304132 arxiv240304132 14 cohere 2024 introducing command r a scalable llm built for business  https coherecomblogcommandrplusmicrosoftazure 15 nick craswell bhaskar mitra emine yilmaz daniel campos and jimmy lin 2021 overview of the trec 2021 deep learning track in proceedings of the thirtieth text retrieval conference trec 2021 online november 1519 2021 nist special publication vol 500335  ian soboroff and angela ellis eds national institute of standards and technology nist httpstrecnistgovpubstrec30 papersoverviewdlpdf 16 nick craswell bhaskar mitra emine yilmaz daniel campos jimmy lin ellen m voorhees and ian soboroff 2022 overview of the trec 2022 deep learning track in proceedings of the thirtyfirst text retrieval conference trec 2022 online november 1519 2022 nist special publication vol 500338  ian soboroff and angela ellis eds national institute of standards and technology nist httpstrecnistgovpubstrec31papersoverview_deeppdf 17 nick craswell",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4847,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "bb20572f-e837-44a7-8da9-7b6f44db745f",
        "embedding": null,
        "metadata": {
            "page_label": "5",
            "file_name": "Ragnar\u00f6k.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
            "file_type": "application/pdf",
            "file_size": 1681358,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "edccc31c-7205-445b-861c-5a6076f724f9",
                "node_type": "4",
                "metadata": {
                    "page_label": "5",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "1add71be1a60801ea59aab34c92cab969e46044d125e0c68c4f31dea8f838083",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "3aa72450-49b6-435e-9823-dc12b12dcd37",
                "node_type": "1",
                "metadata": {
                    "page_label": "5",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "74ca3643ae15a22223f9db7fd3fdb7a28b8e6e8de052b6fb8079f289d27683c0",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "1bd1e9a9-6e47-4c80-87ec-2dcf2c64da04",
                "node_type": "1",
                "metadata": {},
                "hash": "65cf89cac13ee655fadab85d4f6f66a8c2838dcfefd6a42951ee29e278141a8f",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "lin 2021 overview of the trec 2021 deep learning track in proceedings of the thirtieth text retrieval conference trec 2021 online november 1519 2021 nist special publication vol 500335  ian soboroff and angela ellis eds national institute of standards and technology nist httpstrecnistgovpubstrec30 papersoverviewdlpdf 16 nick craswell bhaskar mitra emine yilmaz daniel campos jimmy lin ellen m voorhees and ian soboroff 2022 overview of the trec 2022 deep learning track in proceedings of the thirtyfirst text retrieval conference trec 2022 online november 1519 2022 nist special publication vol 500338  ian soboroff and angela ellis eds national institute of standards and technology nist httpstrecnistgovpubstrec31papersoverview_deeppdf 17 nick craswell bhaskar mitra emine yilmaz hossein a rahmani daniel cam pos jimmy lin ellen m voorhees and ian soboroff 2024 overview of the trec 2023 deep learning track in text retrieval conference trec  nist trec httpswwwmicrosoftcomenusresearchpublicationoverviewofthe trec2023deeplearningtrack18 angela fan yacine jernite ethan perez david grangier jason weston and michael auli 2019 eli5 long form question answering in proceedings of the 57th conference of the association for computational linguistics acl 2019 florence italy july 28 august 2 2019 volume 1 long papers  anna korhonen david r traum and llu\u00eds m\u00e0rquez eds association for computational lin guistics 35583567 httpsdoiorg1018653v1p191346 19 tianyu gao howard yen jiatong yu and danqi chen 2023 enabling large language models to generate text with citations in proceedings of the 2023 conference on empirical methods in natural language processing emnlp 2023 singapore december 610 2023  houda bouamor juan pino and kalika bali eds association for computational linguistics 64656488 httpsdoiorg1018653 v12023emnlpmain398 20 yunfan gao yun xiong xinyu gao kangxiang jia jinliu pan yuxi bi yi dai jiawei sun qianyu guo meng wang and haofen wang 2023 retrievalaugmented generation for large language models a survey corr abs231210997 2023 httpsdoiorg1048550arxiv231210997 arxiv231210997 21 kelvin guu kenton lee zora tung panupong pasupat and mingwei chang 2020 retrieval augmented language model pretraining in proceedings of the 37th international conference on machine learning icml 2020 1318 july 2020 virtual event proceedings of machine learning research vol 119  pmlr 39293938 httpproceedingsmlrpressv119guu20ahtml 22 gautier izacard and edouard grave 2021 leveraging passage retrieval with generative models for open domain question answering in proceedings of the 16th conference of the european chapter of the association for computational linguistics main volume eacl 2021 online april 19  23 2021  paola merlo j\u00f6rg tiedemann and reut tsarfaty eds association for computational linguistics 874880 httpsdoiorg1018653v12021eaclmain74 23 antonio jimenoyepes yao you jan milczek sebastian laverde and renyu li 2024 financial report chunking for effective retrieval augmented genera tion corr abs240205131 2024 httpsdoiorg1048550arxiv240205131 arxiv240205131 24 jiajie jin yutao zhu xinyu yang chenghao zhang and zhicheng dou 2024 flashrag a modular toolkit for efficient retrievalaugmented generation research corr abs240513576 2024 arxiv240513576 httpsarxivorgabs 240513576 25 ehsan kamalloo aref jafari xinyu zhang nandan thakur and jimmy lin 2023 hagrid a humanllm collaborative dataset for generative information seeking with attribution corr abs230716883 2023 httpsdoiorg1048550 arxiv230716883 arxiv230716883 26 urvashi khandelwal omer levy dan jurafsky luke zettlemoyer and mike lewis 2020 generalization through memorization nearest neighbor language models in 8th international conference on learning representations iclr 2020 addis ababa ethiopia april 2630 2020  openreviewnet httpsopenreview netforumidhklbjcekvh 27 sayali kulkarni sheide chammas wan zhu fei sha and eugene ie 2020 aqua muse automatically generating datasets for querybased multidocument summarization corr abs201012694 2020 arxiv201012694 httpsarxivorg abs201012694 28 tom kwiatkowski jennimaria palomaki olivia redfield michael collins ankur p parikh chris alberti danielle epstein illia polosukhin jacob de vlin kenton lee kristina toutanova llion jones matthew kelcey mingwei chang andrew m dai jakob uszkoreit quoc le and slav petrov 2019 natural questions a benchmark for question answering research trans assoc comput linguistics 7 2019 452466 httpsdoiorg101162tacl_a_00276 29 patrick s h lewis ethan perez aleksandra piktus fabio petroni vladimir karpukhin naman goyal heinrich k\u00fcttler mike lewis wentau yih tim rockt\u00e4schel sebastian riedel and douwe kiela 2020 retrievalaugmented generation for knowledgeintensive nlp tasks in advances in neural in formation processing systems 33 annual conference on neural information processing systems 2020 neurips 2020 december 612 2020 virtual  hugo larochelle marcaurelio ranzato raia hadsell mariaflorina balcan and hsuantien lin eds httpsproceedingsneuripsccpaper2020hash 6b493230205f780e1bc26945df7481e5abstracthtml 30 jimmy lin and dina demnerfushman 2006 methods for automatically eval",
        "mimetype": "text/plain",
        "start_char_idx": 4091,
        "end_char_idx": 9223,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "1bd1e9a9-6e47-4c80-87ec-2dcf2c64da04",
        "embedding": null,
        "metadata": {
            "page_label": "5",
            "file_name": "Ragnar\u00f6k.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
            "file_type": "application/pdf",
            "file_size": 1681358,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "edccc31c-7205-445b-861c-5a6076f724f9",
                "node_type": "4",
                "metadata": {
                    "page_label": "5",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "1add71be1a60801ea59aab34c92cab969e46044d125e0c68c4f31dea8f838083",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "bb20572f-e837-44a7-8da9-7b6f44db745f",
                "node_type": "1",
                "metadata": {
                    "page_label": "5",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "d8b75a8e9ad3be35f18ce68470a44db587a844cbbce0e253ac8697c5c9f682c3",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "29 patrick s h lewis ethan perez aleksandra piktus fabio petroni vladimir karpukhin naman goyal heinrich k\u00fcttler mike lewis wentau yih tim rockt\u00e4schel sebastian riedel and douwe kiela 2020 retrievalaugmented generation for knowledgeintensive nlp tasks in advances in neural in formation processing systems 33 annual conference on neural information processing systems 2020 neurips 2020 december 612 2020 virtual  hugo larochelle marcaurelio ranzato raia hadsell mariaflorina balcan and hsuantien lin eds httpsproceedingsneuripsccpaper2020hash 6b493230205f780e1bc26945df7481e5abstracthtml 30 jimmy lin and dina demnerfushman 2006 methods for automatically eval uating answers to complex questions inf retr 9 5 2006 565587 https doiorg101007s1079100690037 31 jimmy lin xueguang ma shengchieh lin jhenghong yang ronak pradeep and rodrigo nogueira 2021 pyserini a python toolkit for reproducible informa tion retrieval research with sparse and dense representations in proceedings of the 44th annual international acm sigir conference on research and development in information retrieval sigir 2021  23562362 32 stephanie lin jacob hilton and owain evans 2022 truthfulqa measuring how models mimic human falsehoods in proceedings of the 60th annual meeting of the association for computational linguistics volume 1 long papers acl 2022 dublin ireland may 2227 2022  smaranda muresan preslav nakov and aline villavicencio eds association for computational linguistics 32143252 httpsdoiorg1018653v12022acllong229",
        "mimetype": "text/plain",
        "start_char_idx": 8564,
        "end_char_idx": 10070,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "c90563a3-7710-4ae8-b21d-a54043886883",
        "embedding": null,
        "metadata": {
            "page_label": "6",
            "file_name": "Ragnar\u00f6k.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
            "file_type": "application/pdf",
            "file_size": 1681358,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "3ee25dc0-0ce3-458c-a242-e07b846fe0e2",
                "node_type": "4",
                "metadata": {
                    "page_label": "6",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "05114c7f8aeb2da619eb3fa978960004ec67a8b1aec7429c4432800d5f333daa",
                "class_name": "RelatedNodeInfo"
            },
            "3": {
                "node_id": "f9fef093-f54a-4c1e-99d5-72f616f9f868",
                "node_type": "1",
                "metadata": {},
                "hash": "857a050bc031171988069793554b08f82d61fcd1046738fda54cfe8d2ad87560",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "conference17 july 2017 washington dc usa pradeep and thakur et al 33 jerry liu 2022 llamaindex  httpswwwllamaindexai 34 nelson f liu kevin lin john hewitt ashwin paranjape michele bevilacqua fabio petroni and percy liang 2024 lost in the mid dle how language models use long contexts transactions of the association for computational linguistics 12 02 2024 157173 httpsdoiorg101162tacl_a_00638 arxivhttpsdirectmitedutaclarticle pdfdoi101162tacl_a_006382336043tacl_a_00638pdf 35 zihan liu wei ping rajarshi roy peng xu chankyu lee mohammad shoeybi and bryan catanzaro 2024 chatqa building gpt4 level conversational qa models corr abs240110225 2024 httpsdoiorg1048550arxiv240110225 arxiv240110225 36 chaitanya malaviya subin lee sihao chen elizabeth sieber mark yatskar and dan roth 2023 expertqa expertcurated questions and attributed an swers corr abs230907852 2023 httpsdoiorg1048550arxiv230907852 arxiv230907852 37 james mayfield eugene yang dawn lawrie sean macavaney paul mcnamee douglas w oard luca soldaini ian soboroff orion weller efsun kayi kate sanders marc mason and noah hibbler 2024 on the evaluation of machine generated reports in proceedings of the 47th international acm sigir conference on research and development in information retrieval  38 luke merrick danmei xu gaurav nuti and daniel campos 2024 arcticembed scalable efficient and accurate text embedding models arxiv240505374 cscl 39 microsoft 2023 reinventing search with a new aipowered microsoft bing and edge your copilot for the web  httpsblogsmicrosoftcomblog20230207reinventing searchwithanewaipoweredmicrosoftbingandedgeyourcopilotfor theweb 40 niklas muennighoff nouamane tazi lo\u00efc magne and nils reimers 2023 mteb massive text embedding benchmark in proceedings of the 17th conference of the european chapter of the association for computational linguistics eacl 2023 dubrovnik croatia may 26 2023  andreas vlachos and isabelle augenstein eds association for computational linguistics 20062029 httpsdoiorg10 18653v12023eaclmain148 41 openai 2024 hello gpt4o  httpsopenaicomindexhellogpt4o 42 arnold overwijk chenyan xiong and jamie callan 2022 clueweb22 10 billion web documents with rich information in sigir 22 the 45th international acm sigir conference on research and development in information retrieval madrid spain july 11  15 2022  enrique amig\u00f3 pablo castells julio gonzalo ben carterette j shane culpepper and gabriella kazai eds acm 33603362 httpsdoiorg10114534774953536321 43 paul owoicho jeff dalton mohammad aliannejadi leif azzopardi johanne r trippas and svitlana vakulenko 2022 trec cast 2022 going beyond user ask and system retrieve with initiative and response generation in proceedings of the thirtyfirst text retrieval conference trec 2022 online november 1519 2022 nist special publication vol 500338  ian soboroff and angela ellis eds national institute of standards and technology nist httpstrecnistgov pubstrec31papersoverview_castpdf 44 fabio petroni aleksandra piktus angela fan patrick s h lewis majid yaz dani nicola de cao james thorne yacine jernite vladimir karpukhin jean maillard vassilis plachouras tim rockt\u00e4schel and sebastian riedel 2021 kilt a benchmark for knowledge intensive language tasks in proceedings of the 2021 conference of the north american chapter of the association for compu tational linguistics human language technologies naaclhlt 2021 online june 611 2021  kristina toutanova anna rumshisky luke zettlemoyer dilek hakkanit\u00fcr iz beltagy steven bethard ryan cotterell tanmoy chakraborty and yichao zhou eds association for computational linguistics 25232544 httpsdoiorg1018653v12021naaclmain200 45 ronak pradeep rodrigo nogueira and jimmy lin 2021 the expandomono duo design pattern for text ranking with pretrained sequencetosequence models arxiv210105667 2021 46 ronak pradeep sahel sharifymoghaddam and jimmy lin 2023 rankvicuna zeroshot listwise document reranking with opensource large language models corr abs230915088 2023 httpsdoiorg1048550arxiv230915088 arxiv230915088 47 ronak pradeep sahel sharifymoghaddam and jimmy lin 2023 rankzephyr ef fective and robust zeroshot listwise reranking is a breeze corr abs231202724 2023 httpsdoiorg1048550arxiv231202724 arxiv231202724 48 vatsal raina and mark gales 2024 questionbased retrieval using atomic units for enterprise rag arxiv240512363 cscl 49 stephen e robertson and hugo zaragoza 2009 the probabilistic relevance framework bm25 and beyond found trends inf retr 3 4 2009 333389 httpsdoiorg1015611500000019 50 sara rosenthal avirup sil radu florian and salim roukos 2024 clapnq cohesive longform answers from passages in natural questions for rag systems corr abs240402103 2024 httpsdoiorg1048550arxiv240402103 arxiv24040210351 corby rosset holam chung guanghui qin ethan c chau zhuo feng ahmed awadallah jennifer neville and nikhil rao 2024 researchy ques tions a dataset of multiperspective decompositional questions for llm web agents corr abs240217896 2024",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4960,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "f9fef093-f54a-4c1e-99d5-72f616f9f868",
        "embedding": null,
        "metadata": {
            "page_label": "6",
            "file_name": "Ragnar\u00f6k.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
            "file_type": "application/pdf",
            "file_size": 1681358,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "3ee25dc0-0ce3-458c-a242-e07b846fe0e2",
                "node_type": "4",
                "metadata": {
                    "page_label": "6",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "05114c7f8aeb2da619eb3fa978960004ec67a8b1aec7429c4432800d5f333daa",
                "class_name": "RelatedNodeInfo"
            },
            "2": {
                "node_id": "c90563a3-7710-4ae8-b21d-a54043886883",
                "node_type": "1",
                "metadata": {
                    "page_label": "6",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "390d87a2fcad76ec1b85ebe9aa14d0f805bce21455f6f3e71e2997c250f888cf",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "2024 questionbased retrieval using atomic units for enterprise rag arxiv240512363 cscl 49 stephen e robertson and hugo zaragoza 2009 the probabilistic relevance framework bm25 and beyond found trends inf retr 3 4 2009 333389 httpsdoiorg1015611500000019 50 sara rosenthal avirup sil radu florian and salim roukos 2024 clapnq cohesive longform answers from passages in natural questions for rag systems corr abs240402103 2024 httpsdoiorg1048550arxiv240402103 arxiv24040210351 corby rosset holam chung guanghui qin ethan c chau zhuo feng ahmed awadallah jennifer neville and nikhil rao 2024 researchy ques tions a dataset of multiperspective decompositional questions for llm web agents corr abs240217896 2024 httpsdoiorg1048550arxiv240217896 arxiv240217896 52 ivan stelmakh yi luan bhuwan dhingra and mingwei chang 2022 asqa factoid questions meet longform answers in proceedings of the 2022 conference on empirical methods in natural language processing emnlp 2022 abu dhabi united arab emirates december 711 2022  yoav goldberg zornitsa kozareva and yue zhang eds association for computational linguistics 82738288 httpsdoiorg1018653v12022emnlpmain566 53 weiwei sun lingyong yan xinyu ma pengjie ren dawei yin and zhaochun ren 2023 is chatgpt good at search investigating large language models as reranking agent arxiv230409542 2023 54 nandan thakur nils reimers andreas r\u00fcckl\u00e9 abhishek srivastava and iryna gurevych 2021 beir a heterogeneous benchmark for zeroshot evalua tion of information retrieval models in proceedings of the neural information processing systems track on datasets and benchmarks 1 neurips datasets and benchmarks 2021 december 2021 virtual  joaquin vanschoren and saikit yeung eds httpsdatasetsbenchmarksproceedingsneuripsccpaper2021hash 65b9eea6e1cc6bb9f0cd2a47751a186fabstractround2html 55 guangzhi xiong qiao jin zhiyong lu and aidong zhang 2024 benchmarking retrievalaugmented generation for medicine corr abs240213178 2024 httpsdoiorg1048550arxiv240213178 arxiv240213178 56 shiqi yan jiachen gu yun zhu and zhenhua ling 2024 corrective retrieval augmented generation corr abs240115884 2024 httpsdoiorg1048550 arxiv240115884 arxiv240115884 57 peilin yang hui fang and jimmy lin 2017 anserini enabling the use of lucene for information retrieval research in international conference on research and development in information retrieval sigir  httpsdoiorg1011453077136 3080721 58 lianmin zheng weilin chiang ying sheng siyuan zhuang zhanghao wu yonghao zhuang zi lin zhuohan li dacheng li eric p xing hao zhang joseph e gonzalez and ion stoica 2023 judging llmasajudge with mtbench and chatbot arena in advances in neural information pro cessing systems 36 annual conference on neural information processing sys tems 2023 neurips 2023 new orleans la usa december 10  16 2023  alice oh tristan naumann amir globerson kate saenko moritz hardt and sergey levine eds httppapersnipsccpaper_filespaper2023hash 91f18a1287b398d378ef22505bf41832abstractdatasets_and_benchmarkshtml",
        "mimetype": "text/plain",
        "start_char_idx": 4254,
        "end_char_idx": 7251,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "a97641d8-88fc-4bed-8f96-1bab56e1fd87",
        "embedding": null,
        "metadata": {
            "page_label": "7",
            "file_name": "Ragnar\u00f6k.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
            "file_type": "application/pdf",
            "file_size": 1681358,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "1b5ba17e-225e-411d-830b-37cf24c599d2",
                "node_type": "4",
                "metadata": {
                    "page_label": "7",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "757358bed74c9d1ad7f554e160503e4add32971fad2151b688dcf5d86df5da87",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "ragnar\u00f6k a reusable rag framework and baselines for trec 2024 retrievalaugmented generation track conference17 july 2017 washington dc usa a trecraggy 2024 additional details we manually classify each available topic in trec deep learn ing tracks 20212023  131516 into one of the seven different topic categories we manually labeled each topic following the guidelines7mentioned below simple  topic asking for information about a simple fact eg how to emulsion a house simple with condition  topic asking for information about a topic with an imposed condition eg how to cook thinly sliced home fries set a topic containing multiple short entities in the answer eg what themes are in action movies aggregation  a topic that requires aggregation of multiple retrieved segments eg how to put together a scuba regu lator comparison  a topic that requires comparison of the retrieved segments eg does light intensity or concentration of car bon dioxide have a higher rate of photosynthesis multihop  a topic that requires to chain multiple information from different retrieved segments eg the population of kings grant fayetteville prior to liberty hills false premise  a topic that has a false preposition or assump tion eg do larger lobsters become tougher when cooked b trecresearchy 2024 additional details note that for researchy questions  51 the following eight intrinsic attributes were measured by gpt4 on a scale of 010 ambiguity checks if the questions intent is moderately ambiguous suggesting multiple interpretations incompleteness checks if the question is difficult to answer due to missing crucial context or details assumptive checks if the question has some builtin assump tions that may influence the answer multifaceted checks if the question requires considering multiple perspectives to provide a comprehensive answer knowledgeintensive checks if the question demands spe cialized knowledge and extensive research to answer thor oughly subjective measures if the question contains some level of subjectivity with potential for varying opinions reasoningintensive checks if the question requires signifi cant reasoning and synthesis of information to answer harmful checks to what extent the question is harmful or inappropriate it is worth noting that all the questions provided scored 0 in harm fulness and a tiny fraction scored highly on ambiguity we used a score of 5as the threshold to label the query for that intrinsic attribute c ragnar\u00f6k system arena figure 4 showcases the ragnar\u00f6k webui dark mode and the user query why have used car prices increased from trec2024 re searchy issued to two blinded systems this blind setup enables 7guidelines have been inspired from the 2024 meta comprehensive rag benchmarksystem this is a chat between a user and an artificial intelligence assistant the assistant gives helpful detailed and polite answers to the users questions based on the context the assistant should also indicate when the answer cannot be found in the context instruction please give a complete answer to the question cite each context document that supports your answer within brackets  using the ieee format question query contexts 1 passage title passage text 2 passage title passage text  20 passage title passage text instruction please give a complete answer to the question cite each context document that supports your answer within brackets  using the ieee format figure 2 chatqa prompt template  35 used for rag gen eration with intext citations with gpt4o in our ragnar\u00f6k framework fair leaderboards especially when incentives to game leaderboards are huge in this competitive proprietary llm space the output displays the answers in humanreadable form allowing users to assess the quality of responses without bias figure 5 demonstrates the responses tab for the example in fig ure 4 the responses tab reformats the final answers into the json output expected by the io definitions of the trec 2024 rag track this feature is particularly useful for developers and researchers who need to ensure that their systems outputs conform to specific standards and formats required by evaluation frameworks by incorporating both humanreadable and jsonformatted out puts ragnar\u00f6k provides a comprehensive evaluation platform that caters to a wide range of needs in the research and development community the ability to toggle between different views and for mats ensures that users can efficiently analyze and interpret the effectiveness of various rag systems",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4500,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "ff519056-c615-4152-9b62-09e186cd5e42",
        "embedding": null,
        "metadata": {
            "page_label": "8",
            "file_name": "Ragnar\u00f6k.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
            "file_type": "application/pdf",
            "file_size": 1681358,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "8e1ddd01-713d-42a7-8724-8e4cd24e6825",
                "node_type": "4",
                "metadata": {
                    "page_label": "8",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "5869c88775e4406658f635edd8b9febc90810bc1cc2cbc132a482102f951b997",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "conference17 july 2017 washington dc usa pradeep and thakur et al figure 3 webui showcasing the ragnar\u00f6k system arena and the user query what inspired pink floyds the wall with answers from two pipelines sidebyside comparing gpt4o answer left and command r answer right table 3 an example of the first segment of two nearduplicate documents present in the ms marco v2 segment collection during the deduplication procedure the segments of one of the documents is kept in the ms marco v21 segment collection msmarco_doc_00_9951701740 whereas the other segment is discarded as a duplicate msmarco_doc_00_9951711910 segment id url title segment ms marco v21 segment discarded msmarco_doc_00 _9951711910httpcenterserveorgtt fp_tipshtmlserve center re sourcesserve center get to know us about us we believe that educational improvement requires a partnership with our clients and we focus on doing work that is important and directly relevant to both policymakers and practitioners services program evaluation capacity building technical assistance strategic planning and much more customized services designed to meet the specific needs of our partnering organizations projects during its history serve has been awarded over 200 million in contracts and grants and has successfully managed 14 major awards including multiple contracts with the us department of education resources forging connections with a variety of individuals and organizations the serve center offers an array of resources developed to inform and aide both the policymaker and the practioner ms marco v21 segment kept msmarco_doc_00 _9951701740httpcenterserveorgtt serve center re sourcesserve center get to know us about us we believe that educational improvement requires a partnership with our clients and we focus on doing work that is important and directly relevant to both policymakers and practitioners services program evaluation capacity building technical assistance strategic planning and much more customized services designed to meet the specific needs of our partnering organizations projects during its history serve has been awarded over 200 million in contracts and grants and has successfully managed 14 major awards including multiple contracts with the us department of education resources forging connections with a variety of individuals and organizations the serve center offers an array of resources developed to inform and aide both the policymaker and the practioner",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2460,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "f4015618-be38-4d29-b6ef-a989f23df8ea",
        "embedding": null,
        "metadata": {
            "page_label": "9",
            "file_name": "Ragnar\u00f6k.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
            "file_type": "application/pdf",
            "file_size": 1681358,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "a73a03b7-b02b-42ea-abf4-95481249f530",
                "node_type": "4",
                "metadata": {
                    "page_label": "9",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "5df715f3c5267d6afd43c29fca38a84a7bfd2ad3befebb2c201074a7036d16c1",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "ragnar\u00f6k a reusable rag framework and baselines for trec 2024 retrievalaugmented generation track conference17 july 2017 washington dc usa figure 4 webui dark mode showcasing the ragnar\u00f6k system arena for the user query on why have used car prices increased from trec2024 researchy with two different blinded pipelines the output tab displays the answers in humanreadable form figure 5 the responses tab for the example in figure 4 note that the responses tab reformats the final answers into the json format expected by the io definitions of the trec 2024 rag track",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 566,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "4a14e369-5474-4e69-9f12-d81edc744fd5",
        "embedding": null,
        "metadata": {
            "page_label": "10",
            "file_name": "Ragnar\u00f6k.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
            "file_type": "application/pdf",
            "file_size": 1681358,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "5be0aef2-af80-4843-9cc6-3af4842e0dd8",
                "node_type": "4",
                "metadata": {
                    "page_label": "10",
                    "file_name": "Ragnar\u00f6k.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\Ragnar\u00f6k.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1681358,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "3603b5078f9e3a5d875eaed89bc128c82220936970b4297302c97d7b917eae4d",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "conference17 july 2017 washington dc usa pradeep and thakur et al table 4 an endtoend rag example for a randomly sampled topic topic id 2027497 in the trecraggy 2024 collection how often should you take your toddler to the potty when potty training 1 top10 retrieved segments from the r retrieval stage ordered from top to bottom in terms of relevancy segment id title segment 1 msmarco_v21_doc_51_766815931 2_1606878413how often should i take my tod dler to the potty there are some guidelines to followselfesteem can be fragile at this time so its important to toilet train gently letting your child lead the way additionally its unwise to begin potty training unless your child is truly ready take this potty training readiness quiz featured in parents before you begin elizabeth pantley    and you still must be on the lookout for signs your toddler has to go ahead of schedule 2 msmarco_v21_doc_51_766815931 6_1606884302how often should i take my tod dler to the potty there are some guidelines to followdavis and keyser stressed the importance of keeping your own emotions in check throughout the toilet training process the bottom line when it comes to potty time theres no magic number    youll just have that much longer to plan a good one 3 msmarco_v21_doc_08_935420812 8_1683502976qa know your childs level of readiness for potty training  par entsstart by sitting on the toilet the first thing in the morning after taking off the overnight diaper make it funsing songs read a book drink some juice and see what happens     dr carrie m brown my 3yearold is adamant about not using the potty how can i motivate her 4 msmarco_v21_doc_08_935420812 0_1683481876qa know your childs level of readiness for potty training  par entsqa know your childs level of readiness for potty training  parents home toddlers  preschoolers potty training potty training tips qa    have your child sit on the potty soon after shes finished breakfast and again after dinner 5 msmarco_v21_doc_28_472446307 23_1012991039potty trained toddler having acci dents on purposebut dont feel disappointed if most of these tries end up with no pee in the pottyyou wouldnt be able to pee    get more tips on how to ease your childs potty training poop anxiety 6 msmarco_v21_doc_28_472446307 22_1012988885potty trained toddler having acci dents on purposeget tips on what to do when your 4 year old wont poop on potty    routines give him the predictability he needs so he knows exactly what to do and when you might use the potty after waking up before leaving the house or after eating meals 7 msmarco_v21_doc_57_1222573163 16_2241797192tips on starting potty training how to potty train your toddlerteach him to check for dryness this offers him an added sense of control if hes dry give him a pat on the back or a big hug but dont criticize him if hes wet    if you overreact you might discourage your toddlers future attempts 8 msmarco_v21_doc_57_1222573163 15_2241795642tips on starting potty training how to potty train your toddlereven if youre too late and hes already done the deed have him sit on the potty anyway to reinforce the connection keep her motivated    even the most enthu siastic toddler can take several weeks to master potty training proficiency  often with as many steps backward as forward 9 msmarco_v21_doc_51_766815931 5_1606882767how often should i take my tod dler to the potty there are some guidelines to followaccording to becoming the parent you want to be by laura davis and janis keyser its common for children to go back and forth  somedays preferring diapers and other days selecting to use the toilet    you can only take your child to the bathroom too much if it begins to feel stressful or punitive 10 msmarco_v21_doc_51_766815931 0_1606874600how often should i take my tod dler to the potty there are some guidelines to followhow often should i take my toddler to the potty there are some guidelines to follow life ashley batzromper how often should i take my toddler to the potty there are some guidelines to follow by kelly mullenmcwilliams july 22 2017 i dont know about you but im personally super pumped for that perfect day    he article noted that toilet training is also one of the most difficult milestones for children and their parents and that it can become highly emotional 2 results from the ag augmented generation stage using top10 retrieved segments model rag answer command r text there is no magic number for how often you should take your toddler to the potty when potty training citations 1 8 text however it is recommended to take them frequently especially after waking up before leaving the house after eating meals or before bath time citations 0 1 2 3 4 5 8 text you can also use a timer and set it to every 30 minutes to an hour for a potty break citations 5 text it is important to be patient and gentle throughout the process as it can be emotionally challenging for both the child and the parent citations 0 1 3 6 7 8 9 gpt4o text when potty training your toddler it is recommended to take them to the potty frequently to increase the chances of success  citations  text according to elizabeth pantley author of the nocry potty training solution most toddlers pee four to eight times per day and have one or two bowel movements daily citations  text she suggests setting up a potty routine such as taking your toddler to the potty first thing in the morning after eating and before activities like riding in the car or going to sleep citations 0 text additionally davis and keyser emphasize the importance of following a schedule such as morning pee and before nap time while also being attentive to signs that your toddler needs to go citations  text they caution against making the process stressful or punitive as negative emotions can hinder progress citations 1 text dr carrie m brown recommends planning bathroom trips every 90120 minutes during the day and remaining calm and consistent even if accidents occur citations  text this approach helps toddlers learn the routine and increases the likelihood of successful potty use citations 2",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 6093,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "dc17cb34-840e-4054-9bdc-c3c1e51b0425",
        "embedding": null,
        "metadata": {
            "page_label": "1",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "5a82ef8b-7f94-440b-96fe-dab27d0b5298",
                "node_type": "4",
                "metadata": {
                    "page_label": "1",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "40238871d55ab3720f3c402bef4e3ae03ab74d4203b0bf0aa0cbce42fa55b1a3",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint selfrag learning to retrieve  generate and critique through selfreflection akari asai zeqiu wu yizhong wang avirup sil hannaneh hajishirzi university of washingtonallen institute for aiibm research ai akarizeqiuwuyizhongwhannaneh cswashingtonedu aviusibmcom abstract despite their remarkable capabilities large language models llms often produce responses containing factual inaccuracies due to their sole reliance on the paramet ric knowledge they encapsulate retrievalaugmented generation rag an ad hoc approach that augments lms with retrieval of relevant knowledge decreases such issues however indiscriminately retrieving and incorporating a fixed number of retrieved passages regardless of whether retrieval is necessary or passages are relevant diminishes lm versatility or can lead to unhelpful response generation we introduce a new framework called selfreflective retrievalaugmented gen eration  selfragthat enhances an lms quality and factuality through retrieval and selfreflection our framework trains a single arbitrary lm that adaptively retrieves passages ondemand and generates and reflects on retrieved passages and its own generations using special tokens called reflection tokens generating reflection tokens makes the lm controllable during the inference phase enabling it to tailor its behavior to diverse task requirements experiments show that self rag7b and 13b parameters significantly outperforms stateoftheart llms and retrievalaugmented models on a diverse set of tasks specifically selfrag outperforms chatgpt and retrievalaugmented llama2chat on opendomain qa reasoning and fact verification tasks and it shows significant gains in improving factuality and citation accuracy for longform generations relative to these models1 1 i ntroduction stateoftheart llms continue to struggle with factual errors mallen et al 2023 min et al 2023 despite their increased model and data scale ouyang et al 2022 retrievalaugmented generation rag methods figure 1 left lewis et al 2020 guu et al 2020 augment the input of llms with relevant retrieved passages reducing factual errors in knowledgeintensive tasks ram et al 2023 asai et al 2023a however these methods may hinder the versatility of llms or introduce unnecessary or offtopic passages that lead to lowquality generations shi et al 2023 since they retrieve passages indiscriminately regardless of whether the factual grounding is helpful moreover the output is not guaranteed to be consistent with retrieved relevant passages gao et al 2023 since the models are not explicitly trained to leverage and follow facts from provided passages this work introduces selfreflective retrievalaugmented generation  selfragto improve an llms generation quality including its factual accuracy without hurting its versatility via ondemand retrieval and selfreflection we train an arbitrary lm in an endtoend manner to learn to reflect on its own generation process given a task input by generating both task output and intermittent special tokens ie reflection tokens  reflection tokens are categorized into retrieval andcritique tokens to indicate the need for retrieval and its generation quality respectively figure 1 right in particular given an input prompt and preceding generations selfragfirst determines if augmenting the continued generation with retrieved passages would be helpful if so it outputs a retrieval token that calls a retriever model on demand step 1 subsequently selfragconcurrently processes multiple retrieved passages evaluating their relevance and then generating corresponding task outputs step 2 it then generates critique tokens to criticize its own output and choose best one step 3 in terms of factuality and overall quality this process differs from conventional rag figure 1 left which 1our code and trained models are available at httpsselfraggithubio  1arxiv231011511v1 cscl 17 oct 2023",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3895,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "573d51eb-e5fc-4461-b230-d39bcc976cf5",
        "embedding": null,
        "metadata": {
            "page_label": "2",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "bcef763a-b467-4b5b-900c-35c381862db1",
                "node_type": "4",
                "metadata": {
                    "page_label": "2",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "c356f489759a9a0e7d940d141d5150e1e8b7b9164f4c6c986baf401d0fb012ed",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint step 1 retrieve k documentscalifornia was named after a \ufb01ctional island in a spanish book prompt how did us states get their names us states got their names from a variety of sources eleven states are named after an individual person eg california was named after christopher columbus some states including texas and utah are named after native american tribe retrievalaugmented generation ragours selfre\ufb02ective retrievalaugmented generation selfrag popular names by states in texas emma is a popular baby name of the \ufb01fty states eleven are named after an individual person prompt how did us states get their names  step 2 prompt lm with k docs and generateretriever lm prompt how did us states get their names us states got their names from a variety of sources retrievestep 1 retrieve on demand prompt  11 of 50 state namesrelevant step 2 generate segment in parallel come from personssupportedirrelevanttexas is namedafter a native american tribe step 3 critique outputs and select best segmentorigins in a 16thcentury novel las sergas de esplandi\u00e1n californias name has itsrelevantpartially us states got their names from a variety of sources 11 of 50 states names are come from persons 26 states are named after native americans including utah prompt write an essay of your best summer vacation prompt write an essay of your best summer vacation no retrievalmy best summer vacation is when my family and i embarked on a road trip along my best repeat no information in passagescontradictoryprompt  prompt  retrieve figure 1 overview of selfragselfraglearns to retrieve critique and generate text passages to enhance overall generation quality factuality and verifiability consistently retrieves a fixed number of documents for generation regardless of the retrieval necessity eg the bottom figure example does not require factual knowledge and never second visits the generation quality moreover selfragprovides citations for each segment with its selfassessment of whether the output is supported by the passage leading to easier fact verification selfragtrains an arbitrary lm to generate text with reflection tokens by unifying them as the next token prediction from the expanded model vocabulary we train our generator lm on a diverse collection of text interleaved with reflection tokens and retrieved passages reflection tokens inspired by reward models used in reinforcement learning ziegler et al 2019 ouyang et al 2022 are inserted offline into the original corpus by a trained critic model this eliminates the need to host a critic model during training reducing overhead the critic model in part is supervised on a dataset of input output and corresponding reflection tokens collected by prompting a propriety lm ie gpt4 openai 2023 while we draw inspiration from studies that use control tokens to start and guide text generation lu et al 2022 keskar et al 2019 our trained lm uses critique tokens to assess its own predictions after each generated segment as an integral part of the generation output selfragfurther enables a customizable decoding algorithm to satisfy hard or soft constraints which are defined by reflection token predictions in particular our inferencetime algorithm enables us to 1 flexibly adjust retrieval frequency for different downstream applications and 2 customize models behaviors to user preferences by leveraging reflection tokens through segmentlevel beam search using the weighted linear sum of the reflection token probabilities as segment score empirical results on six tasks including reasoning and longform generation demonstrate that self ragsignificantly outperforms pretrained and instructiontuned llms that have more parameters and widely adopted rag approaches with higher citation accuracy in particular selfragoutperforms retrievalaugmented chatgpt on four tasks llama2chat touvron et al 2023 and alpaca dubois et al 2023 on all tasks our analysis demonstrates the effectiveness of training and inference with reflection tokens for overall performance improvements as well as testtime model customizations eg balancing the tradeoff between citation previsions and completeness 2 r elated work retrievalaugmented generation retrievalaugmented generation rag augments the input space of lms with retrieved text passages guu et al 2020 lewis et al 2020 leading to large improvements in knowledgeintensive tasks after finetuning or used with offtheshelf lms ram et al 2023 a more recent work luo et al 2023 instructiontunes an lm with a fixed number 2",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4517,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "3e86004c-95b3-4216-9f86-731267255da2",
        "embedding": null,
        "metadata": {
            "page_label": "3",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "38b4cc95-2238-498f-b019-b0e5e803c480",
                "node_type": "4",
                "metadata": {
                    "page_label": "3",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "0e2149715bb6209fdb01b90cfa5a1d7ea244f70c247bf8a32dd0c91b7d255b28",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint of retrieved passages prepended to input or pretrain a retriever and lm jointly followed by few shot finetuning on task datasets izacard et al 2022b while prior work often retrieves only once at the beginning jiang et al 2023 propose to adaptively retrieve passages for generation on top of a proprietary llm or schick et al 2023 train an lm to generate api calls for named entities yet the improved task performance of such approaches often comes at the expense of runtime efficiency mallen et al 2023 robustness to irrelevant context shi et al 2023 and lack of attributions liu et al 2023a gao et al 2023 we introduce a method to train an arbitrary lm to learn to use retrieval ondemand for diverse instructionfollowing queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions concurrent rag work a few concurrent works2on rag propose new training or prompting strategies to improve widelyadopted rag approaches lin et al 2023 finetune both the retriever and lm on instructiontuning datasets in two steps while we also train our model on diverse instructionfollowing datasets selfragenables retrieval on demand and selection of the best possible model output via finegrained selfreflection making it widely applicable and more robust and controllable yoran et al 2023 use a natural language inference model and xu et al 2023 use a summarization model to filter out or compress retrieved passages before using them to prompt the lm to generate the output selfragprocesses passages in parallel and filters out irrelevant ones through selfreflection without relying on external models at inference moreover our selfreflection mechanism also evaluates other aspects of the model output quality including factuality lats zhou et al 2023 prompt offtheshelf lms to search for relevant information for question answering tasks and to generate with tree search guided by lmgenerated value scores while their value function simply indicates an overall score of each generation selfragtrains to an arbitrary lm to learn to generate finegrained selfreflection and customizable inference training and generating with critics training llms with reinforcement learning eg proximal policy optimization or ppo schulman et al 2017 from human feedback rlhf has proven effective in aligning llms with human preferences ouyang et al 2022 wu et al 2023 introduce finegrained rlhf with multiple reward models though our work also studies finegrained critique on retrieval and generation we train our target lm on task examples augmented with reflection tokens from a critic model offline with a far lower training cost compared to rlhf in addition reflection tokens in s elfragenable controllable generation at inference while rlhf focuses on human preference alignment during training other works use general control tokens to guide lm generation lu et al 2022 korbak et al 2023 while selfraguses reflection tokens to decide the need for retrieval and to selfevaluate generation quality xie et al 2023 propose a selfevaluation guided decoding framework but they focus only on reasoning tasks with one evaluation dimension reasoning path consistency and without retrieval recent work on llm refinement dhuliawala et al 2023 madaan et al 2023 paul et al 2023 prompts a model to generate task output natural language feedback and refined task output iteratively but at the cost of inference efficiency 3 s elfrag learning to retrieve  generate and critique we introduce selfreflective retrievalaugmented generation  selfrag shown in figure 1 selfragis a framework that enhances the quality and factuality of an llm through retrieval and selfreflection without sacrificing llms original creativity and versatility our endtoend training lets an lm mgenerate text informed by retrieved passages if needed and criticize the output by learning to generate special tokens these reflection tokens table 1 signal the need for retrieval or confirm the outputs relevance support or completeness in contrast common rag approaches retrieve passages indiscriminately without ensuring complete support from cited sources 31 p roblem formalization and overview formally given input x we train mto sequentially generate textual outputs yconsisting of multiple segments y y1     y t where ytindicates a sequence of tokens for the tth segment3generated tokens in ytinclude text from the original vocabulary as well as the reflection tokens table 1 2all work is arxived within a week of this preprint 3in this paper we treat one sentence as a segment in our experiments but our framework is applicable to any segment unit ie subsentence 3",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4676,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "98178894-32bb-4402-85fb-492204503e64",
        "embedding": null,
        "metadata": {
            "page_label": "4",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "a59e3605-1f94-4076-8e00-0561a282003d",
                "node_type": "4",
                "metadata": {
                    "page_label": "4",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "c239fa35f1a6abe4bc3d090021844352d517013e3cc6449650f47853d9bab9e0",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint type input output definitions retrieve xx y yes no continue  decides when to retrieve with r isrel x d relevant  irrelevant  dprovides useful information to solve x issup x d y fully supported  partially supported no support all of the verificationworthy statement in y is supported by d isuse x y 5 4 3 2 1  yis a useful response to x table 1 four types of reflection tokens used in selfrag each type uses several tokens to represent its output values the bottom three rows are three types of critique tokens and the bold text indicates the most desirable critique tokens x y d indicate input output and a relevant passage respectively algorithm 1 selfraginference require generator lm m retriever r largescale passage collections d1     d n 1input input prompt xand preceding generation ytoutput next output segment yt 2mpredicts retrieve given x yt 3ifretrieve yes then 4 retrieve relevant text passages dusingrgiven x yt1 retrieve 5 mpredicts isrelgiven x dandytgiven x d y tfor each dd generate 6 mpredicts issupand isusegiven x yt dfor each dd critique 7 rank ytbased on isrelissupisuse detailed in section 33 8else if retrieve nothen 9 mgenpredicts ytgiven x  generate 10 mgenpredicts isusegiven x yt critique inference overview figure 1 and algorithm 1 present an overview of s elfragat inference for every xand preceding generation yt the model decodes a retrieval token to evaluate the utility of retrieval if retrieval is not required the model predicts the next output segment as it does in a standard lm if retrieval is needed the model generates a critique token to evaluate the retrieved passages relevance the next response segment and a critique token to evaluate if the information in the response segment is supported by the passage finally a new critique token evaluates the overall utility of the response4to generate each segment selfragprocesses multiple passages in parallel and uses its own generated reflection tokens to enforce soft constraints section 33 or hard control algorithm 1 over the generated task output for instance in figure 1 right the retrieved passages d1is selected at the first time step since d2does not provide direct evidence  isrelis irrelevant andd3output is only partially supported while d1are fully supported training overview selfragenables an arbitrary lm to generate text with reflection tokens by unifying them as next token predictions from the expanded model vocabulary ie the original vocabulary plus reflection tokens specifically we train the generator model mon a curated corpus with interleaving passages retrieved by a retriever rand reflection tokens predicted by a critic model csummarized in appendix algorithm 2 we train cto generate reflection tokens for evaluating retrieved passages and the quality of a given task output section 321 using the critic model we update the training corpus by inserting reflection tokens into task outputs offline subsequently we train the final generator model  m using the conventional lm objective section 322 to enable mto generate reflection tokens by itself without relying on the critic at inference time 32 s elfragtraining here we describe the supervised data collection and training of two models the critic csection 321 and the generator msection 322 321 t raining the critic model data collection for critic model manual annotation of reflection tokens for each segment is expensive wu et al 2023 a stateoftheart llm like gpt4 openai 2023 can be effectively 4we follow liu et al 2023a in using a perceived utility value that is independent of retrieved passages 4",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3586,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "8eae47ed-1e86-4295-80b4-0fb3bb2d9e13",
        "embedding": null,
        "metadata": {
            "page_label": "5",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "68d57731-8a83-4f25-9883-34753e274bc9",
                "node_type": "4",
                "metadata": {
                    "page_label": "5",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "5fc7b6550c5cfe105658016bafbb981460ff8c3cbc11ce2400a5ca07e858529e",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint input how did us states get their names input write an essay of your best summer vacationoutput my best summer vacation was a magical escape to the coastal town of santorini the azure waters charming whitewashed building are unforgettable critic lmoutput 1 of 50 states names come from persons for instance louisiana was named in honor of king louis xiv of france and georgia was named after king george ii retrieve partially augmented output retrieveplouisiana named inpof the \ufb01fty states eleven are named after an individual personp 11 of 50 states names come from person relevantsupportedhonor of louis xiv of francep relevantfor instance louisiana was named after king louis xiv andutil 5georgia was named after king george ii util 5augmented output my best summer vacation was a magical escape to the coastal town of santorini the azure waters charming whitewashed building are unforgettable experienceno retrievalno retrieval retriever figure 2 selfragtraining examples the left example does not require retrieval while the right one requires retrieval thus passages are inserted more examples are in appendix table 4 used to generate such feedback liu et al 2023b however depending on such proprietary lms can raise api costs and diminish reproducibility chen et al 2023 we create supervised data by prompting gpt4 to generate reflection tokens and then distill their knowledge into an inhouse c for each group of reflection tokens we randomly sample instances from the original training data xsample ysample   x y as different reflection token groups have their own definitions and input as shown in table 1 we use different instruction prompts for them here we use retrieve as an example we prompt gpt4 with a typespecific instruction given an instruction make a judgment on whether finding some external documents from the web helps to generate a better response followed by fewshot demonstrations ithe original task input xand output yto predict an appropriate reflection token as text pri x y  manual assessment reveals that gpt4 reflection token predictions show high agreement with human evaluations we collect 4k20k supervised training data for each type and combine them to form training data for c appendix section d shows the full list of instructions and a1 contains more details and our analysis critic learning after we collect training data dcritic  we initialize cwith a pretrained lm and train it on dcritic using a standard conditional language modeling objective maximizing likelihood max cexyrdcritic logpcrx y rfor reflection tokens 1 though the initial model can be any pretrained lm we use the same one as the generator lm ie llama 27b touvron et al 2023 for cinitialization the critic achieves a higher than 90 agreement with gpt4based predictions on most reflection token categories appendix table 5 322 t raining the generator model data collection for generator given an inputoutput pair x y we augment the original output yusing the retrieval and critic models to create supervised data that precisely mimics the self raginferencetime process section 31 for each segment yty we run cto assess whether additional passages could help to enhance generation if retrieval is required the retrieval special token retrieve yes is added and rretrieves the top kpassages d for each passage cfurther evaluates whether the passage is relevant and predicts isrel if a passage is relevant cfurther evaluates whether the passage supports the model generation and predicts issup critique tokens isreland issupare appended after the retrieved passage or generations at the end of the output y orytcpredicts the overall utility token isuse and an augmented output with reflection tokens and the original input pair is added to dgen see the example training data in figure 2 generator learning we train the generator model mby training on the curated corpus augmented with reflection tokens dgenusing the standard next token objective max mexyr dgenlogpmy rx 2 unlike ctraining eq 1 mlearns to predict the target output as well as the reflection tokens during training we mask out the retrieved text chunks surrounded by p andp in figure 2 for loss calculation and expand the original vocabulary vwith a set of reflection tokens critique retrieve connections to prior work on learning with critique recent work incorporates additional critique feedback during training eg rlhf ouyang et al 2022 via ppo while ppo relies on 5",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4450,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "b2a721fe-9c04-4c93-892e-b97785692992",
        "embedding": null,
        "metadata": {
            "page_label": "6",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "6cf4b63b-44a4-44e0-b5a3-126732eed1e9",
                "node_type": "4",
                "metadata": {
                    "page_label": "6",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "73b8c4b594056b66f8fb6a22bfa89c6484db1828076704f801de783f61c2539b",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint separate reward models during training we compute critique offline and directly insert them into the training corpus where the generator lm is trained with a standard lm objective this significantly reduces training costs compared to ppo our work also relates to prior work that incorporates special tokens to control generation keskar et al 2019 lu et al 2022 korbak et al 2023 our selfrag learns to generate special tokens to evaluate its own prediction after each generated segment enabling the use of a soft reranking mechanism or hard constraints at inference discussed next 33 s elfraginference generating reflection tokens to selfevaluate its own output makes selfragcontrollable during the inference phase enabling it to tailor its behavior to diverse task requirements for tasks demanding factual accuracy min et al 2023 we aim for the model to retrieve passages more frequently to ensure that the output aligns closely with the available evidence conversely in more openended tasks like composing a personal experience essay the emphasis shifts towards retrieving less and prioritizing the overall creativity or utility score in this section we describe approaches to enforce control to meet these distinct objectives during the inference process adaptive retrieval with threshold selfragdynamically decides when to retrieve text passages by predicting retrieve  alternatively our framework allows a threshold to be set specifically if the prob ability of generating the retrieve yes token normalized over all output tokens in retrieve surpasses a designated threshold we trigger retrieval details in appendix section a3 treedecoding with critique tokens at each segment step t when retrieval is required based either on hard or soft conditions rretrieves kpassages and the generator mprocesses each passage in parallel and outputs kdifferent continuation candidates we conduct a segmentlevel beam search with the beam size b to obtain the top bsegment continuations at each timestamp t and return the best sequence at the end of generation the score of each segment ytwith respect to passage dis updated with a critic score sthat is the linear weighted sum of the normalized probability of each critique token type for each critique token group geg isrel we denote its score at timestamp tassg t and we compute a segment score as follows fyt d critique  pytx d y t scritique where 3 scritique  x ggwgsg tforgisrelissupisuse 4 where sg tpt\u02c6rpng i1ptristands for the generation probability of the most desirable reflection token \u02c6reg isrelrelevant  for the critique token type gwithngdistinct tokens that represent different possible values for g the weights wgin eq 4 are hyperparameters that can be adjusted at inference time to enable customized behaviors at test time for instance to ensure that result yis mostly supported by evidence we can set a weight term for the issupscore higher while relatively lowering weights for other aspects alternatively we could further enforce hard constraints during decoding using critique  instead of using a soft reward function in eq 4 we could explicitly filter out a segment continuation when the model generates an undesirable critique token eg issupno support   balancing the tradeoff between multiple preferences has been studied in rlhf touvron et al 2023 wu et al 2023 which often requires training to change models behaviors s elfragtailors an lm with no additional training 4 e xperiments 41 t asks and datasets we conduct evaluations of our selfragand diverse baselines on a range of downstream tasks holistically evaluating outputs with metrics designed to assess overall correctness factuality and fluency throughout these experiments we conduct zeroshot evaluations where we provide instruc tions describing tasks without fewshot demonstrations wei et al 2022 sanh et al 2022 details of our experiments settings including testtime instructions are available in the appendix section b1 closedset tasks include two datasets ie a fact verification dataset about public health  pubhealth  zhang et al 2023 and a multiplechoice reasoning dataset created from scientific exams  arc 6",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4151,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "fc50363f-4528-483a-a3ac-2dcfebf0db5e",
        "embedding": null,
        "metadata": {
            "page_label": "7",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "9474acd0-2017-4b44-9182-442b258db2af",
                "node_type": "4",
                "metadata": {
                    "page_label": "7",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "7ad91c6059889e7b9758652403b7c97bdb10c109bfa8b50c735fdb0f176e4034",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint challenge  clark et al 2018 we use accuracy as an evaluation metric and report on the test set we aggregate the answer probabilities of target classes for both of these datasets appendix section b2 shortform generations tasks include two opendomain question answering qa datasets popqa mallen et al 2023 and triviaqaunfiltered joshi et al 2017 where systems need to answer arbitrary questions about factual knowledge for popqa we use the longtail subset consisting of 1399 rare entity queries whose monthly wikipedia page views are less than 100 as the triviaqaunfiltered open test set is not publicly available we follow prior works validation and test split min et al 2019 guu et al 2020 using 11313 test queries for evaluation we evaluate performance based on whether gold answers are included in the model generations instead of strictly requiring exact matching following mallen et al 2023 schick et al 2023 longform generation tasks include a biography generation task min et al 2023 and a longform qa task alceasqa gao et al 2023 stelmakh et al 2022 we use factscore min et al 2023 to evaluate biographies and we use official metrics of correctness strem fluency based on mauve pillutla et al 2021 and citation precision and recall gao et al 2023 for asqa5 42 b aselines baselines without retrievals we evaluate strong publicly available pretrained llms llama2 7b13btouvron et al 2023 instructiontuned models alpaca 7b13bdubois et al 2023 our replication based on llama2 and models trained and reinforced using private data chat gpt ouyang et al 2022 and llama2chat 13b for instructiontuned lms we use the official system prompt or instruction format used during training if publicly available we also compare our method to concurrent work cove 65bdhuliawala et al 2023 which introduces iterative prompt engineering to improve the factuality of llm generations baselines with retrievals we evaluate models augmented with retrieval at test time or during training the first category includes standard rag baselines where an lm llama2 alpaca generates output given the query prepended with the top retrieved documents using the same retriever as in our system it also includes llama2ft where llama2 is finetuned on all training data we use without the reflection tokens or retrieved passages we also report the result of retrievalaugmented baselines with lms trained with private data retchatgpt and retllama2chat which deploy the same augmentation technique above as well as perplexityai an instructgptbased production search system the second category includes concurrent methods that are trained with retrieved text passages ie sail luo et al 2023 to instructiontune an lm on the alpaca instructiontuning data with top retrieved documents inserted before instructions and toolformer schick et al 2023 to pretrain an lm with api calls eg wikipedia apis6 43 e xperimental settings training data and settings our training data consists of diverse instructionfollowing inputoutput pairs in particular we sample instances from openinstruct processed data wang et al 2023 and knowledgeintensive datasets petroni et al 2021 stelmakh et al 2022 mihaylov et al 2018 in total we use 150k instructionoutput pairs we use llama2 7b and 13b touvron et al 2023 as our generator base lm and we use llama2 7b as our base critic lm for the retriever model r we use offtheshelf contrieverms marco izacard et al 2022a by default and retrieve up to ten documents for each input more training details are in the appendix section b1 inference settings as a default configuration we assign the weight terms isrelissupisuse values of 10 10 and 05 respectively to encourage frequent retrieval we set the retrieval threshold to 02 for most tasks and to 0 for alce gao et al 2023 due to citation requirements we speed up inference using vllm kwon et al 2023 at each segment level we adopt a beam width of 2 for a tokenlevel generation we use greedy decoding by default we use the top five documents from contrieverms marco izacard et al 2022a for biographies and opendomain qa we use additional top five documents retrieved by a web search engine following luo et al 2023 for asqa we use the authorprovided top 5 documents by gtrxxl ni et al 2022 across all baselines for a fair comparison 5httpsgithubcomprincetonnlpalce 6we report numbers using the results reported in the paper as the implementations are not available 7",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4412,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "f859b592-537a-4e4d-b1c9-146c8aa7d154",
        "embedding": null,
        "metadata": {
            "page_label": "8",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "13a9f908-cfa1-42f5-9986-12640c88e293",
                "node_type": "4",
                "metadata": {
                    "page_label": "8",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "afcd9cc0867c3174da3137e27a336b0e7da5233e2cb2993eb2dc5957673ce251",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint table 2 overall experiment results on six tasks bold numbers indicate the best performance among nonproprietary models and graycolored bold text indicates the best proprietary model when they outperforms all nonproprietary modelsindicates concurrent or recent results reported by concurrent work  indicates numbers that are not reported by the original papers or are not applicable models are sorted based on scale fs em rg mau prec rec denote factscore factuality strem rouge correctness mauve fluency citation precision and recall respectively shortform closedset longform generations with citations popqa tqa pub arc bio asqa lm acc acc acc acc fs em rg mau pre rec lms with proprietary data llama2c 13b 200 593 494 384 559 224 296 286   retllama2c 13b 518 598 521 379 799 328 348 438 198 361 chatgpt 293 743 701 753 718 353 362 688   retchatgpt 508 657 547 753  407 399 797 651 766 perplexityai     712      baselines without retrieval llama2 7b 147 305 342 218 445 79 153 190   alpaca 7b 236 545 498 450 458 188 294 617   llama2 13b 147 385 294 294 534 72 124 160   alpaca 13b 244 613 555 549 502 229 320 706   cove 65b     712      baselines with retrieval toolformer 6b  488         llama2 7b 382 425 300 480 780 152 221 320 29 40 alpaca 7b 467 641 402 480 766 309 333 579 55 72 llama2ft 7b 487 573 643 658 782 310 358 512 50 75 sail 7b   692 484       llama2 13b 457 470 302 260 775 163 205 247 23 36 alpaca 13b 461 669 511 576 777 348 367 566 20 38 our selfrag 7b 549 664 724 673 812 300 357 743 669 678 our selfrag 13b 558 693 745 731 802 317 370 716 703 713 5 r esults and analysis 51 m ainresults comparison against baselines without retrieval table 2 top presents the baselines without retrieval our selfragbottom two rows demonstrates a substantial performance advantage over supervised finetuned llms in all tasks and even outperforms chatgpt in pubhealth popqa biography generations and asqa rouge and mauve our approach also significantly outperforms a concurrent method that employs sophisticated prompt engineering specifically on the bio generation task our 7b and 13b models outperform the concurrent cove dhuliawala et al 2023 which iteratively prompts llama2 65bto refine output comparison against baselines with retrieval as shown in tables 2 bottom our selfragalso outperforms existing rag in many tasks obtaining the best performance among nonproprietary lmbased models on all tasks while our method outperforms other baselines on popqa or bio powerful instructiontuned lms with retrieval eg llama2chat alpaca show large gains from their nonretrieval baselines however we found that these baselines provide limited solutions for tasks where we cannot simply copy or extract substrings of retrieved passages on pubhealth and arcchallenge baselines with retrieval do not improve performance notably from their no retrieval counterparts we also observe that most baselines with retrieval struggle to improve citation accuracy on asqa our model shows significantly higher citation precision and recall than all models except chatgpt gao et al 2023 found that chatgpt consistently exhibits superior efficacy in this particular task surpassing smaller lms our selfragbridges this performance gap even outperforming chatgpt in citation precision which measures whether the modelgenerated claim is fully supported by cited evidence we also found that on the metrics for factual precision selfrag 7b occasionally outperforms our 13b due to the tendency of smaller selfragto often generate 8",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3516,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "05cb5f7d-eebc-41e3-8b11-21ca32fb6c3f",
        "embedding": null,
        "metadata": {
            "page_label": "9",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "2d5f0b9d-a173-479e-b168-7fc6c893dd3c",
                "node_type": "4",
                "metadata": {
                    "page_label": "9",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "a6cdd06bcab1e1c0079fd1322b38a6cd330a5306e7c4bb9b371c860d0e24d897",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint pqa med as acc acc em selfrag50k 455 735 321 training no retriever r 436 678 310 no critic c 426 720 181 test no retrieval 247 730  hard constraints 283 726  retrieve top1 418 731 286 remove issup 441 732 306 a ablation 1 2700705precision 1 2 weight for issupport9095mauve b customization 00 02 04 06098099099100accuracy pubhealth 00 02 04 06 retrieval threshold060810accuracypopqa000510 frequency 025050075100 frequency c retrieval figure 3 analysis on selfragaablation studies for key components of selfragtraining and inference based on our 7b model b effects of soft weights on asqa citation precision and mauve fluency c retrieval frequency andnormalized accuracy on pubhealth and popqa precisely grounded yet shorter outputs llama2ft 7b which is the baseline lm trained on the same instructionoutput pairs as selfragwithout retrieval or selfreflection and is retrievalaugmented at test time only lags behind s elfrag this result indicates s elfraggains are not solely from training data and demonstrate the effectiveness of s elfragframework 52 a nalysis ablation studies we conduct a set of ablations of our framework to identify which factors play key roles we evaluate two model variants trained differently than our model no retriever trains an lm using the standard instructionfollowing method given instructionoutput pairs without retrieved passages no critic trains an lm trained with inputoutput pairs that are always augmented with the top one retrieved document without reflection tokens this is similar to sail luo et al 2023 and we use our instructionoutput data instead of using the alpaca dataset dubois et al 2023 as in sail we also conduct ablation on our inferencetime algorithm including no retrieval disables retrieval during inference hard constraints indicates the model performance that retrieves when retrieve yes instead of using the adaptive threshold retrieve top 1 always retrieves and uses the top one document only similar to standard rag approaches remove issupindicates the model performance that removes issupscore only during critiqueguided beam search in eq 4 in this ablation experiment we use a training instance size of 50k for a more efficient exploration of training variations later in this section we conduct an analysis of the effect of training data size we conduct the ablation studies on three datasets popqa pubhealth and asqa on asqa we evaluate models on sampled 150 instances and exclude ablations involving adaptive or no retrieval processes we show in table 3a the ablation results the top part of the table shows results for training ablations and the bottom part is for inference ablations we see that all components play important roles we also observe a large performance gap between selfragand no retriever or critic baselines across tasks indicating that training an lm with those models largely contributes to the performance gain of selfrag using the top passages regardless of their relevance retrieve top 1 as in conventional rag approaches causes a large drop in popqa and asqa and removing issupduring the beam search results hurts performance on asqa this demonstrates the effectiveness of selfrags capabilities of carefully selecting generations based finegrained multiple criterion instead of naively using all of the top passages from the retrieval model or solely depending on relevance scores effects of inferencetime customization one key benefit of our proposed framework is that it enables us to control how much each critique type affects the final generation sampling we analyze the effects of different parameter weights on the top of our 7b model during inference time on asqa where multiple evaluation aspects are considered figure 3b shows the effects of changing the weighting term for issup which criticizes how supported the output is by the text passage as the figure shows increasing the weight leads to positive effects on the models citation precision since this puts more emphasis on whether model generation is supported by the evidence on the 9",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4047,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "c311ac25-db8a-4ea4-9f17-ca469f1b09c3",
        "embedding": null,
        "metadata": {
            "page_label": "10",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "a271b7b6-c91f-4597-b1da-c9b4a30aef72",
                "node_type": "4",
                "metadata": {
                    "page_label": "10",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "f71c73bf107ef1644d2430e01b04258428df37b9f4520c31dba751c58ab48c2f",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint 0 50 100 150 num of training k3540455055perfomance a popqa 0 100 num of training k717273 b pubhealth 0 100 num of training k4060 c asqa precpop bio s  p 925 700 isrel 950 900 issup 900 850 d human evaluation on popqa and bio generation figure 4 training scale and human analysis a b c training scale analysis shows the effect of the training data scale on popqa pubhealth and asqa citation precision respectively d human analysis on s elfragoutputs as well as reflection tokens contrary a larger weight results in lower mauve scores when generation gets longer and more fluent there are often more claims that are not fully supported by citations consistent with findings by liu et al 2023a our framework lets practitioners choose and customize models behaviors at test time by adjusting such parameters without requiring additional training efficiency and accuracy tradeoff using our framework practitioners can adjust how often retrieval occurs using the token probability of reward tokens we evaluate how this adaptive threshold affects overall accuracy and frequency of retrieval and we evaluate the performance with varying numbers of threshold \u03b4larger \u03b4results in less retrieval on pubhealth and popqa figure 3c shows that the models retrieval frequencies dramatically change on both datasets as \u03b4varies on one hand performance deterioration by retrieving less is smaller on pubhealth but larger in popqa effects of training data size we conduct an analysis of how the data scale affects the models performance in particular we randomly sample 5k 10k 20k and 50k instances from our original 150k training instances and finetune four selfrag 7bvariants on those subsets then we compare the model performance on popqa pubhealth and asqa citation precision with our final self ragtrained on the full 150k instances we also evaluate figures 4a 4b and 4c shows the models performance trained on different amount of data across all datasets increasing data size often shows upward trajectories and the improvements are significantly larger in popqa and asqa while we do not observed such significant improvements on llama2ft 7bwhen increasing the training data from 50k to 150k these results also indicate that further expanding the training data of selfragmay lead to further improvements although in this work we limit our training data size to 150k human evaluations we conduct small human evaluations on selfragoutputs as well as the reliability of predicted reflection tokens in particular we sampled 50 samples from popqa and bio results following menick et al 2022 human annotators evaluate sp  which indicates whether the model output is plausible ie the output is a reasonable and ontopic response to the question as if it were occurring in a conversation and supported ie the provided evidence is sufficient to verify the validity of the answer for sp we do not consider the instances where selfrag predicts irrelevant orno support  we then ask our annotators whether the modelpredicted reflection tokens about isreland issupmatch their inspections eg whether the fully supported output is supported by the cited evidence human annotators find selfraganswers are often plausible and supported by relevant passages with higher sp scores on shortform popqa which is consistent with menick et al 2022 human annotators also find isreland issupreflection token predictions are mostly aligned with their assessments appendix table 6 shows several annotated examples and explanations on assessments 6 c onclusion this work introduces selfrag a new framework to enhance the quality and factuality of llms through retrieval on demand and selfreflection selfragtrains an lm to learn to retrieve generate and critique text passages and its own generation by predicting the next tokens from its original vocabulary as well as newly added special tokens called reflection tokens selfragfurther enables the tailoring of lm behaviors at test time by leveraging reflection tokens our holistic evaluations on six tasks using multiple metrics demonstrate that selfragsignificantly outperforms llms with more parameters or with conventional retrievalaugmented generation approaches 10",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4184,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "360bf171-001e-46ed-ad97-338494d79697",
        "embedding": null,
        "metadata": {
            "page_label": "11",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "f826a274-94f3-4b8c-83d4-5637e59fb271",
                "node_type": "4",
                "metadata": {
                    "page_label": "11",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "049abd1e229ff17cff9928b5ca59518dbb75fa466b753b1f30aa57e67446aa3a",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint ethical concerns this work aims to improve the factuality of llm outputs the lack of which continues to cause nu merous realworld problems eg spread of misinformation and provision of incorrect and dangerous advice while our method shows significant improvements in terms of performance factuality and citation accuracy it can still generate outputs that are not fully supported by the citations we hope that explicit selfreflection and finegrained attribution may help users verify factual errors in the model outputs acknowledgments we thank sewon min scott wentau yih sean welleck and kawin ethayarajh for fruitful discussions in the early stages of this work we thank sewon min joongwon daniel kim and sandy kaplan for valuable feedback on the paper and tianyu gao and weijia shi for their help on evaluations akari asai is supported by the ibm fellowship we thank stability ai for providing computing to train and evaluate the lms in this work and microsoft accelerate foundation models research program for the access to openai apis this work was funded in part by the darpa mcs program through niwc pacific n660011924031 nsf iis2044660 and gifts from ai2 references akari asai kazuma hashimoto hannaneh hajishirzi richard socher and caiming xiong learn ing to retrieve reasoning paths over wikipedia graph for question answering in international conference on learning representations  2020 url httpsopenreviewnetforum idsjgvhkrydh  akari asai sewon min zexuan zhong and danqi chen retrievalbased language models and appli cations in proceedings of the 61st annual meeting of the association for computational linguistics tutorial  2023a url httpsaclanthologyorg2023acltutorials6  akari asai timo schick patrick lewis xilun chen gautier izacard sebastian riedel hannaneh hajishirzi and wentau yih taskaware retrieval with instructions in findings of the associ ation for computational linguistics  2023b url httpsaclanthologyorg2023 findingsacl225  bernd bohnet vinh q tran pat verga roee aharoni daniel andor livio baldini soares jacob eisenstein kuzman ganchev jonathan herzig kai hui et al attributed question answering evaluation and modeling for attributed large language models arxiv preprint arxiv221208037  2022 url httpsarxivorgabs221208037  lingjiao chen matei zaharia and james zou how is chatgpts behavior changing over time arxiv preprint arxiv230709009  2023 url httpsarxivorgabs230709009  peter clark isaac cowhey oren etzioni tushar khot ashish sabharwal carissa schoenick and oyvind tafjord think you have solved question answering try arc the ai2 reasoning challenge arxiv preprint arxiv180305457  2018 url httpsarxivorgabs180305457  tri dao dan fu stefano ermon atri rudra and christopher r e flashattention fast and memory efficient exact attention with ioawareness in advances in neural information processing systems  2022 url httpsopenreviewnetforumidh4dqfpsibmx  shehzaad dhuliawala mojtaba komeili jing xu roberta raileanu xian li asli celikyilmaz and jason weston chainofverification reduces hallucination in large language models arxiv preprint arxiv230911495  2023 url httpsarxivorgabs230911495  emily dinan stephen roller kurt shuster angela fan michael auli and jason weston wizard of wikipedia knowledgepowered conversational agents in international conference on learning representations  2019 url httpsopenreviewnetforumidr1l73irqkm  yann dubois xuechen li rohan taori tianyi zhang ishaan gulrajani jimmy ba carlos guestrin percy liang and tatsunori b hashimoto alpacafarm a simulation framework for methods that 11",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3564,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "82f7f22f-d0df-4a13-aa5b-907c857beff1",
        "embedding": null,
        "metadata": {
            "page_label": "12",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "1f1200a1-c316-4629-8654-a7106c15bd68",
                "node_type": "4",
                "metadata": {
                    "page_label": "12",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "fb66da81b073b727df39956584bf6b66e1ae9906fdced25161e5511cf158ceba",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint learn from human feedback arxiv preprint arxiv230514387  2023 url httpsarxiv orgabs230514387  tianyu gao howard yen jiatong yu and danqi chen enabling large language models to generate text with citations arxiv preprint arxiv230514627  2023 url httpsarxivorgabs 230514627  kelvin guu kenton lee zora tung panupong pasupat and mingwei chang retrieval augmented language model pretraining in international conference on machine learning  2020 url httpsdlacmorgdoipdf10555535249383525306  gautier izacard mathilde caron lucas hosseini sebastian riedel piotr bojanowski armand joulin and edouard grave unsupervised dense information retrieval with contrastive learning transactions on machine learning research  2022a url httpsopenreviewnet forumidjkn1pxi7b0  gautier izacard patrick lewis maria lomeli lucas hosseini fabio petroni timo schick jane dwivediyu armand joulin sebastian riedel and edouard grave fewshot learning with retrieval augmented language models arxiv preprint arxiv220803299  2022b url https arxivorgabs220803299  zhengbao jiang frank f xu luyu gao zhiqing sun qian liu jane dwivediyu yiming yang jamie callan and graham neubig active retrieval augmented generation arxiv preprint arxiv230506983  2023 url httpsarxivorgabs230506983  mandar joshi eunsol choi daniel weld and luke zettlemoyer triviaqa a large scale distantly supervised challenge dataset for reading comprehension in proceedings of the 55th annual meeting of the association for computational linguistics volume 1 long papers  2017 url httpsaclanthologyorgp171147  nitish shirish keskar bryan mccann lav r varshney caiming xiong and richard socher ctrl a conditional transformer language model for controllable generation arxiv preprint arxiv190905858  2019 url httpsarxivorgabs190905858  tomasz korbak kejian shi angelica chen rasika vinayak bhalerao christopher buckley jason phang samuel r bowman and ethan perez pretraining language models with human preferences ininternational conference on machine learning  2023 url httpsopenreviewnet forumidat8iw8koec  tom kwiatkowski jennimaria palomaki olivia redfield michael collins ankur parikh chris alberti danielle epstein illia polosukhin jacob devlin kenton lee kristina toutanova llion jones matthew kelcey mingwei chang andrew m dai jakob uszkoreit quoc le and slav petrov natural questions a benchmark for question answering research transactions of the association for computational linguistics  2019 url httpsaclanthologyorg q191026  woosuk kwon zhuohan li siyuan zhuang ying sheng lianmin zheng cody hao yu joseph e gonzalez hao zhang and ion stoica efficient memory management for large language model serving with pagedattention in proceedings of the acm sigops 29th symposium on operating systems principles  2023 url httpsarxivorgabs230906180  patrick lewis ethan perez aleksandra piktus fabio petroni vladimir karpukhin naman goyal heinrich k uttler mike lewis wentau yih tim rockt aschel sebastian riedel and douwe kiela retrievalaugmented generation for knowledgeintensive nlp tasks in advances in neural infor mation processing systems  2020 url httpsproceedingsneuripsccpaper 2020file6b493230205f780e1bc26945df7481e5paperpdf  xi victoria lin xilun chen mingda chen weijia shi maria lomeli rich james pedro rodriguez jacob kahn gergely szilvasy mike lewis luke zettlemoyer and scott yih radit retrieval augmented dual instruction tuning 2023 url httpsarxivorgabs231001352  nelson f liu tianyi zhang and percy liang evaluating verifiability in generative search engines arxiv preprint arxiv230409848  2023a url httpsarxivorgabs230409848  12",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3597,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "e187ed7b-35b9-4621-8827-f8d435913c50",
        "embedding": null,
        "metadata": {
            "page_label": "13",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "93ccbfca-c8f7-4879-9371-54ca057769ac",
                "node_type": "4",
                "metadata": {
                    "page_label": "13",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "311092987f2f30440937af700360a987204b4deb0b91768c4fecc491248e7f90",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint yang liu dan iter yichong xu shuohang wang ruochen xu and chenguang zhu gpteval nlg evaluation using gpt4 with better human alignment arxiv preprint arxiv230316634  2023b urlhttpsarxivorgabs230316634  ximing lu sean welleck jack hessel liwei jiang lianhui qin peter west prithviraj am manabrolu and yejin choi quark controllable text generation with reinforced unlearning inadvances in neural information processing systems  2022 url httpsopenreview netforumid5haids3ux5o  hongyin luo yungsung chuang yuan gong tianhua zhang yoon kim xixin wu danny fox helen meng and james glass sail searchaugmented instruction learning arxiv preprint arxiv230515225  2023 url httpsarxivorgabs230515225  aman madaan niket tandon prakhar gupta skyler hallinan luyu gao sarah wiegreffe uri alon nouha dziri shrimai prabhumoye yiming yang shashank gupta bodhisattwa prasad majumder katherine hermann sean welleck amir yazdanbakhsh and peter clark self refine iterative refinement with selffeedback arxiv preprint arxiv230317651  2023 url httpsarxivorgabs230317651  alex mallen akari asai victor zhong rajarshi das daniel khashabi and hannaneh hajishirzi when not to trust language models investigating effectiveness of parametric and nonparametric memories in proceedings of the 61st annual meeting of the association for computational linguistics volume 1 long papers  2023 url httpsaclanthologyorg2023 acllong546  jacob menick maja trebacz vladimir mikulik john aslanides francis song martin chadwick mia glaese susannah young lucy campbellgillingham geoffrey irving et al teaching language models to support answers with verified quotes arxiv preprint arxiv220311147  2022 urlhttpsarxivorgabs220311147  todor mihaylov peter clark tushar khot and ashish sabharwal can a suit of armor conduct electricity a new dataset for open book question answering in proceedings of the 2018 conference on empirical methods in natural language processing  2018 url httpsaclanthology orgd181260  sewon min danqi chen hannaneh hajishirzi and luke zettlemoyer a discrete hard em approach for weakly supervised question answering in proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natu ral language processing emnlpijcnlp  2019 url httpsaclanthologyorg d191284  sewon min kalpesh krishna xinxi lyu mike lewis wentau yih pang wei koh mohit iyyer luke zettlemoyer and hannaneh hajishirzi factscore finegrained atomic evaluation of factual precision in long form text generation arxiv preprint arxiv230514251  2023 url https arxivorgabs230514251  reiichiro nakano jacob hilton suchir balaji jeff wu long ouyang christina kim christopher hesse shantanu jain vineet kosaraju william saunders et al webgpt browserassisted questionanswering with human feedback arxiv preprint arxiv211209332  2021 url https arxivorgabs211209332  jianmo ni chen qu jing lu zhuyun dai gustavo hernandez abrego ji ma vincent zhao yi luan keith hall mingwei chang and yinfei yang large dual encoders are generalizable retrievers in proceedings of the 2022 conference on empirical methods in natural language processing  2022 url httpsaclanthologyorg2022emnlpmain669  openai gpt4 technical report arxiv preprint arxiv230308774  2023 url httpsarxiv orgabs230308774  long ouyang jeffrey wu xu jiang diogo almeida carroll wainwright pamela mishkin chong zhang sandhini agarwal katarina slama alex gray john schulman jacob hilton fraser kelton luke miller maddie simens amanda askell peter welinder paul christiano jan leike and 13",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3558,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "e291e21d-5e5a-49a1-b8e3-217f65ca2b8d",
        "embedding": null,
        "metadata": {
            "page_label": "14",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "a8352954-1f7b-4a4e-817b-337ccef5c975",
                "node_type": "4",
                "metadata": {
                    "page_label": "14",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "23a47504199c57a2d19ea90702a148d9671b1cc7bd4e3839cf3c4025ca360e97",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint ryan lowe training language models to follow instructions with human feedback in advances in neural information processing systems  2022 url httpsopenreviewnetforum idtg8kacxeon  debjit paul mete ismayilzada maxime peyrard beatriz borges antoine bosselut robert west and boi faltings refiner reasoning feedback on intermediate representations arxiv preprint arxiv230401904  2023 url httpsarxivorgabs230401904  fabio petroni aleksandra piktus angela fan patrick lewis majid yazdani nicola de cao james thorne yacine jernite vladimir karpukhin jean maillard vassilis plachouras tim rockt aschel and sebastian riedel kilt a benchmark for knowledge intensive language tasks in proceedings of the 2021 conference of the north american chapter of the association for computational linguistics human language technologies  2021 url httpsaclanthologyorg 2021naaclmain200  krishna pillutla swabha swayamdipta rowan zellers john thickstun sean welleck yejin choi and zaid harchaoui mauve measuring the gap between neural text and human text using divergence frontiers in advances in neural information processing systems  2021 url https openreviewnetforumidtqx7njp7pr  samyam rajbhandari jeff rasley olatunji ruwase and yuxiong he zero memory optimizations toward training trillion parameter models in proceedings of the international conference for high performance computing networking storage and analysis  2020 url httpsdlacm orgdoi10555534337013433727  ori ram yoav levine itay dalmedigos dor muhlgay amnon shashua kevin leytonbrown and yoav shoham incontext retrievalaugmented language models transactions of the association for computational linguistics  2023 url httpsarxivorgabs230200083  victor sanh albert webson colin raffel stephen bach lintang sutawika zaid alyafeai antoine chaffin arnaud stiegler arun raja manan dey m saiful bari canwen xu urmish thakker shanya sharma sharma eliza szczechla taewoon kim gunjan chhablani nihal nayak de bajyoti datta jonathan chang mike tianjian jiang han wang matteo manica sheng shen zheng xin yong harshit pandey rachel bawden thomas wang trishala neeraj jos rozen abheesht sharma andrea santilli thibault fevry jason alan fries ryan teehan teven le scao stella biderman leo gao thomas wolf and alexander m rush multitask prompted training enables zeroshot task generalization in international conference on learning representations  2022 url httpsopenreviewnetforumid9vrb9d0wi4  timo schick jane dwivediyu roberto dess \u0131 roberta raileanu maria lomeli luke zettlemoyer nicola cancedda and thomas scialom toolformer language models can teach themselves to use tools arxiv preprint arxiv230204761  2023 url httpsarxivorgabs2302 04761  john schulman filip wolski prafulla dhariwal alec radford and oleg klimov proximal policy optimization algorithms arxiv preprint arxiv170706347  2017 url httpsarxivorg abs170706347  freda shi xinyun chen kanishka misra nathan scales david dohan ed h chi nathanael scharli and denny zhou large language models can be easily distracted by irrelevant context inproceedings of the 40th international conference on machine learning  2023 url https proceedingsmlrpressv202shi23ahtml  ivan stelmakh yi luan bhuwan dhingra and mingwei chang asqa factoid questions meet long form answers in proceedings of the 2022 conference on empirical methods in natural language processing  2022 url httpsaclanthologyorg2022emnlpmain566  james thorne andreas vlachos christos christodoulopoulos and arpit mittal fever a large scale dataset for fact extraction and verification in proceedings of the 2018 conference of the north american chapter of the association for computational linguistics human language tech nologies volume 1 long papers  2018 url httpsaclanthologyorgn181074  14",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3749,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "eaa2b393-c3cb-4c1f-be87-3d5b0b680e7d",
        "embedding": null,
        "metadata": {
            "page_label": "15",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "12d81db2-303e-4cf3-8660-424da0bbd9ab",
                "node_type": "4",
                "metadata": {
                    "page_label": "15",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "5e4e83eb88583f4730f89bdfae29908d66076958441f57ecfec04574234b479b",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint hugo touvron louis martin kevin stone peter albert amjad almahairi yasmine babaei nikolay bashlykov soumya batra prajjwal bhargava shruti bhosale et al llama 2 open foundation and finetuned chat models arxiv preprint arxiv230709288  2023 url httpsarxiv orgabs230709288  yizhong wang hamish ivison pradeep dasigi jack hessel tushar khot khyathi raghavi chandu david wadden kelsey macmillan noah a smith iz beltagy et al how far can camels go exploring the state of instruction tuning on open resources arxiv preprint arxiv230604751  2023 urlhttpsarxivorgabs230604751  jason wei maarten bosma vincent zhao kelvin guu adams wei yu brian lester nan du andrew m dai and quoc v le finetuned language models are zeroshot learners in international conference on learning representations  2022 url httpsopenreviewnetforum idgezrgcozdqr  zeqiu wu yushi hu weijia shi nouha dziri alane suhr prithviraj ammanabrolu noah a smith mari ostendorf and hannaneh hajishirzi finegrained human feedback gives better rewards for language model training arxiv preprint arxiv230601693  2023 url https arxivorgabs230601693  yuxi xie kenji kawaguchi yiran zhao xu zhao minyen kan junxian he and qizhe xie decom position enhances reasoning via selfevaluation guided decoding arxiv preprint arxiv230500633  2023 url httpsarxivorgabs230500633  fangyuan xu weijia shi and eunsol choi recomp improving retrievalaugmented lms with compression and selective augmentation 2023 url httpsarxivorgabs2310 04408  ori yoran tomer wolfson ori ram and jonathan berant making retrievalaugmented language models robust to irrelevant context 2023 url httpsarxivorgabs231001558  xiang yue boshi wang kai zhang ziru chen yu su and huan sun automatic evaluation of attribution by large language models arxiv preprint arxiv230506311  2023 url https arxivorgabs230506311  tianhua zhang hongyin luo yungsung chuang wei fang luc gaitskell thomas hartvigsen xixin wu danny fox helen meng and james glass interpretable unified language checking arxiv preprint arxiv230403728  2023 url httpsarxivorgabs230403728  andy zhou kai yan michal shlapentokhrothman haohan wang and yuxiong wang language agent tree search unifies reasoning acting and planning in language models 2023 url https arxivorgabs231004406  daniel m ziegler nisan stiennon jeffrey wu tom b brown alec radford dario amodei paul christiano and geoffrey irving finetuning language models from human preferences arxiv preprint arxiv190908593  2019 url httpsarxivorgabs190908593  15",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2498,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "8652acb9-47ff-462a-bf51-00f9c8c00eef",
        "embedding": null,
        "metadata": {
            "page_label": "16",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "2ba0cd0c-43e8-47eb-ab55-38547d82e197",
                "node_type": "4",
                "metadata": {
                    "page_label": "16",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "a828c196d6fc469a893ca87d517bb2f849a94c382a6007398984ba6750a9bdcf",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint appendix a s elfragdetails 17 a1 reflection tokens                                   17 a2 s elfragtraining                                   17 a3 s elfraginference                                  19 b experimental details 19 b1 more details of training                                19 b2 more details of evaluations                              20 c results 20 c1 analysis                                         20 c2 human evaluation examples                              21 c3 qualitative examples                                  21 d full list of instructions and demonstrations for gpt4 21 16",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 612,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "c1642367-8b88-4789-861f-5aa2c89d6d83",
        "embedding": null,
        "metadata": {
            "page_label": "17",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "f1763b26-50bf-4574-844c-e5042527be0e",
                "node_type": "4",
                "metadata": {
                    "page_label": "17",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "21167849e3c86b69062f3a86086ec7b8662b07da2e386fdb81c3c102bf064f8e",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint a s elfragdetails a1 r eflection tokens  definitions of reflection tokens below we provide a detailed definition of reflection type and output tokens the first three aspects will be provided at each segment level while the final aspect is only given at each output level retrievalondemand retrieve  given an input and previousstep generation if applicable an lm determines whether the continuation requires factual grounding noindicates retrieval is unnecessary as the sequence does not require factual grounding or may not be enhanced by knowledge retrieval yes indicates retrieval is necessary we additionally have continue to use evidence  which indicates that a model can continue to use the evidence retrieved previously for instance a passage may contain rich factual information and thus selfrag generates multiple segments based on the passage relevant isrel retrieved knowledge may not be always relevant to the input this aspect indicates whether the evidence provides useful information  relevant  or not  irrelevant  supported issup attribution is the concept of whether the output is fully supported by certain evidence menick et al 2022 bohnet et al 2022 this aspect judges how much infor mation in the output is entailed by the evidence we evaluate attributions in three scale fully supported partially supported  and no support  contradictory  follow ing yue et al 2023 nakano et al 2021 useful isuse following the definitions from liu et al 2023a we define the perceived utility as whether the response is a helpful and informative answer to the query independently from whether it is in fact factual or not this can be also viewed as plausibility in menick et al 2022 for usefulness we use a fivescale evaluation 1 is the lowest and 5 is the highest details of gpt4based data collections we use the instruction and demonstration pairs to prompt gpt4 listed in section d following an official recommendation we separate instructions and outputs with  we use the temperature 1 and set the maximum output token counts to be 200 we discard instances where gpt4 does not follow the designated output formats or output sequences that do not match our expected category names as a result we collected 12594 for retrieve  11181 for issup 19317 for relevance 3831 for utility manual analysis of the gpt4 predictions the authors of this paper manually assess randomly sampled 20 instances for each aspect and check if gpt4 predictions match their assessments given the same instruction demonstrations and test instances we found our assessments show high agreement with gpt4 predictions especially for relevance 95 retrieval necessity 95 and the degree of support 90 agreement was slightly lower in usefulness 80 mostly due to the disagreement between 1 and 2 or 4 and 5 a2 s elfragtraining overview of training algorithm 2 provides a highlevel overview of our training full list of seed datasets to sample diverse inputoutput pairs we sample instances of the open instruct wang et al 2023 dataset in particular we use their sharegpt gpt4 alpaca alpaca openassistant and flan subsets subsets we also sample instances from a couple of knowledge intensive datasets natural questions kwiatkowski et al 2019 wizard of wikipedia dinan et al 2019 and fever thorne et al 2018 from the kilt benchmark petroni et al 2021 asqa stel makh et al 2022 and multiple qa datasets including arceasy and openbookqa mihaylov et al 2018 table 3 shows the full list of training instances and in total we use 145619 instances performance of the critic cwe evaluate the accuracy of reward predictions by splitting gpt4 generated feedback into training development and test sets the accuracy of the reward model is as follows table 5 shows the model performance of predicting gpt4 judgments as you can see overall our finetuned reward model shows high prediction matching with gpt4 predicted feedback 17",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3895,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "2d8e0570-7a24-45c6-aab9-c11c3c2aa91c",
        "embedding": null,
        "metadata": {
            "page_label": "18",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e1329edc-7abc-4c56-a84f-16d875230d02",
                "node_type": "4",
                "metadata": {
                    "page_label": "18",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "ff4278401918a1257e1263e5bb06b1ac8b3ba4fe7d76a193aae3f98d837ff471",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint algorithm 2 selfragtraining 1input inputoutput data dx y generator mc\u03b8 2initialize cwith a pretrained lm 3sample data xsample ysample   x y training critic lm section 321 4forx yxsample ysampledo data collections for c 5 prompt gpt4 to collect a reflection token rforx y 6 addx y r todcritic 7update cwith next token prediction loss critic learning eq 1 8initialize mwith a pretrained lm training generator lm section 322 9forx yx y do data collection for mwithdcritic 10 runcto predict rgiven x y 11 addx y r todgen 12update mondgenwith next token prediction loss generator lm learning eq 2 dataset name category data source the number of instances gpt4 alpaca instructionfollowing openinstruct 26168 stanford alpaca instructionfollowing openinstruct 25153 flanv2 instructionfollowing openinstruct 17817 sharegpt instructionfollowing openinstruct 13406 open assistant 1 instructionfollowing openinstruct 9464 wizard of wikipedia knowledgeintensive kilt 17367 natural questions knowledgeintensive kilt 15535 fever knowledgeintensive kilt 9966 openboookqa knowledgeintensive hf dataset 4699 arceasy knowledgeintensive hf dataset 2147 asqa knowledgeintensive asqa 3897 table 3 the generator lm mtraining data statistics base lm retrieve issup isrel isuse llama27b 938 935 802 735 flan3b 856 731 820 721 figure 5 reward prediction accuracy using gpt4 predictions as groundtruth predictions while our final model uses llama27b as a base lm we also train and compare flan3b wei et al 2022 model on the same data to investigate the effectiveness of different data sizes affect final reward predictions in most aspects our reward model shows higher than 80 accuracy indicating the powerful ability of finetuned specialized lms to evaluate text while both models show relatively lower performance on isuse this is because both models often confuse between the two highest cases 5 and 4 where human annotators can also disagree details of mdata creation here we provide detailed data creation procedures algorithm 3 summarizes the process here we set yttoyfor simplification once we train the critic model we first run it on input data from the aforementioned datasets to predict whether retrieval is needed or not for the instances where the critic predicts retrieve no we only predict the isusegiven input and output for the instances where the critic predicts retrieve yes we first retrieve passages using the input and the entire output as queries to find passages that are relevant to the entire output we then split output sentences using spacy7for each sentence we run cto predict whether the retrieval is necessary or not given the input preceding segments and the initial retrieved passage if cpredicts retrieve no then do not insert any paragraph at the tth segment if cpredicts retrieve yes then we use the original input and the tth segment as a retrieval query to find relevant passages for the tth segment for each retrieved passage we predict isreland issup if there is any passage and continuation with isrelrelevant and issupfully supported issuppartially 7httpsspacyio 18",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3089,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "517a6099-f179-4ef7-9430-4b5c1a28af08",
        "embedding": null,
        "metadata": {
            "page_label": "19",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "9ce2a96b-9e7c-4706-9d0a-6da93c2e0fe3",
                "node_type": "4",
                "metadata": {
                    "page_label": "19",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "173df57f1e500c0893a299a312d65705ad7344dbf6bae5da91c9d28a79d3c6a0",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint supported  then we sample it as the continuation if there is more than one passage satisfying this criterion we use the one with the highest retrieval score if there are only isrelirrelevant or issupno support passages we randomly sample one passage algorithm 3 mgendata creation 1input inputoutput data dx y 2forx y x ydo 3 given x ycpredicts retrieve 4 ifretrieve is predicted then 5 retrieve relevant passages dusingrgiven x y retrieve passages 6 fordddo 7 cpredicts isrelfor each d  predict relevance of passages 8 cpredicts issupfor each y d predict supports of outputs 9 cpredicts isusefor each d  predict overall utility  ttonly 10 sample d 11 else if retrieve is not predicted then 12 cpredicts isusegiven x y add augmented x y d r todgen training examples table 4 show several training examples used for mtraining a3 s elfraginference details of beamsearch score calculations we first compute scores for each critique type by taking the normalized probabilities of desirable tokens for isrel we compute the score as follows sisrel pisrelrelevant  pisrelrelevant  pisrelirrelevant  for issup we compute the score as follows sisrel pissupfully  s 05pissuppartially  s where sp tfullypartially nopissupt for isusewhere we have a fivescale score we compute the weighted sum of the scores we assigns weighted scores of w1050051 to the tokens isuse12345 and compute the final scores as follows sisuse 5x iwipisusei s where sp t12345pisuset details of adaptive retrieval for retrieval based on soft constraints we trigger retrieval if the following condition is satisfied pretrieve yes pretrieve yes ppretrieve no \u03b4 b e xperimental details b1 m ore details of training more details of training and computations we use 4 nvidia a100 with 80gb memory to train our models all models are trained for 3 epochs with a batch size of 128 a peak learning rate of 2e5 with 3 warmup steps and linear decay afterward we set the maximum token length to be 2048 for the 7b model and 1524 for the 13b model due to the memory constraint we use deepspeed stage 3 rajbhandari et al 2020 to conduct multigpu distributed training with training precision bfloat16 enabled flashattention dao et al 2022 is used to make the longcontext training more efficient we run inference of our trained models using 12 quadro rtx 6000 gpus with 24gb memory 19",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2336,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "49757bdc-718b-461a-9ab7-3f92b5a29b5e",
        "embedding": null,
        "metadata": {
            "page_label": "20",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "c6080f15-9a11-4dfa-972b-6af3cbdf8dfd",
                "node_type": "4",
                "metadata": {
                    "page_label": "20",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "915adab371fbda850c08cd37d19a411d60e41cf4c23c6e690e5d8a8d2a278dc3",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint b2 m ore details of evaluations retrieval setup details by default we use contrieverms marco to retrieve the top five documents from wikipedia and use official wikipedia embeddings based on 2018 english wikipedia on popqa where question and answer pairs are created based on wikidata in 2022 we found that the 2018 wikipedia sometimes lacks articles about some entities that have been more recently added to wikipedia therefore for popqa we used the december 2020 preprocessed wikipedia corpus provided by izacard et al 2022b and generated document embeddings8the issues of performance variance from different wikipedia dumps have been reported by prior work asai et al 2020 izacard et al 2022b yet we observe limited effectiveness of such offtheshelf retrieval models trained primarily on knowledgeintensive tasks for openended generation eg instruction following recent or concurrent work studies instructiontuning of retrieval systems asai et al 2023b or joint training of retrieval and lm components lin et al 2023 while we leave exploring the effectivess of such appraoches for future work for bio generation and opendomain qa tasks we additionally retrieve five documents using google programmable search9and search documents from english wikipedia as this api only provides snippets we retrieve wikipedia introductory paragraphs for the corresponding entities detailed experimental settings for individual datasets for openqa datasets we set the max imum new token number to 100 tokens for closedset tasks pubhealth and arcc we set the maximum new token length to 50 for all baselines for selfraginference on pubhealth and arcc instead of determining the output with the highest score 4 as in other tasks we aggregate the scores for each option and select the answer option with the highest score we found in zeroshot settings of fact checking some llms can generate capitalized class labels eg true while our gold labels are lowercased therefore across different lms for fact checking we lowercase the predictions in multiple choice tasks we found some models generate answers in slightly different ways eg a instead of a we slightly modify instructions for each llm to avoid such format violations and further conduct string matching between each candidate and model predictions if format violations still remain after that processing in closed set tasks model predictions match one of the gold classes in almost all cases for alce we found that llama2chat tend to generate significantly lower outputs than other models eg on average their output is nearly 100 token while chatgpt generates 40 tokens on average resulting in inflated strem scores we limit the maximum generation length to 100 tokens for all baselines to avoid this issue rather than the original 300 tokens in the alce paper consequently all of the baseline output length is within 3060 tokens for factscore we set the maximum new token length to 500 for baselines and 200 for selfragat each segment level taskspecific instructions table 5 shows the list of the instructions used during evaluations for opendomain qa we do not provide explicit instructions c r esults c1 a nalysis reliance on parametric and nonparametric memories we conduct analysis on how frequently model answers come from retrieved passages nonparametric memories or their own parametric memories on two opendomain qa datasets triviaqa and popqa we conduct the following analysis 1 sample query models successfully answer correctly 2 for each query in this group check whether the matched groundtruth answer is a substring of the retrieved passage or not we evaluate selfrag7b alpaca 7b alpaca 13b and llama2chat13b we found that selfrag significantly less frequently generates answers that are not included in the provided evidence in particular in alpaca 30b 20 of the correct predictions are not included in the provided passages followed by llama2chat 13b 18 and alpaca 15 while it is only 2 in selfrag when retrieved passages are not relevant selfraggenerates isrelirrelevant  indicating that the following answers may not be factually grounded while those instructiontuned models continue to generate plausible answers 8httpsgithubcomfacebookresearchatlas 9httpsprogrammablesearchenginegooglecomabout 20",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4265,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "d98e2bb2-e683-44c1-a375-1805a08e1178",
        "embedding": null,
        "metadata": {
            "page_label": "21",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e4a851b0-7f5e-449a-8072-db60c0f2f4a5",
                "node_type": "4",
                "metadata": {
                    "page_label": "21",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "d18abec9988f49698455fadf6522f5f32c41a47ce861f094dc7bc3c340e3b9cc",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint c2 h uman evaluation examples table 6 shows examples with human evaluations on sp and correctness of isreland issup reflection tokens c3 q ualitative examples table 7 shows several examples predicted by our selfrag13b the first example is the model output to an asqa question the first reference states that emperor constantine made sunday a day of rest from labor and further the second citation supports the fact that the official adoption of sunday as a day of rest by constantine in ad 321 in the second example the model predicts contradictory to the first output as the output says the person has served as the ceo since 2010 while the passage says he stepped down as ceo in 2015 indicating those factual contradictions as reflection tokens enables to enforcement of hard control and also verification of model outputs easily in the third example while the generation is mostly correct selfragpredicts partially support to the statement listing the name of the songs as they were not explicitly mentioned d f ull list of instructions and demonstrations for gpt4 here we show the instructions and demonstrations used to prompt gpt4 to collect reflection tokens table 8 shows the instructions and demonstrations for the initial retrieval token table 9 shows the instruction and demonstrations used to collect the threeway output tokens for retrieve given instruction preceding sentences and previously retrieved passages due to the longer demonstration and test input we only use a single demonstration table 10 shows an instruction and demonstrations used to collect the threeway output tokens for isrel table 11 shows an instruction and demon strations used to collect the threeway output tokens for isrel table 12 shows an instruction and demonstrations used to collect the fiveway output tokens for isuse 21",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 1824,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "fe273532-6089-44fb-9723-152f0520b63b",
        "embedding": null,
        "metadata": {
            "page_label": "22",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "ba623fc8-2039-4918-90e3-94f84402eb5b",
                "node_type": "4",
                "metadata": {
                    "page_label": "22",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "eb4977e1a9f699e104d0c2daa67ad2a7a60c3c6699c3cb0acc2136d8a688816c",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint input identify the two types of computer memory from gpt4 alpaca  output retrieve yespcomputer memory computer memory  in computing memory refers to the computer hardware integrated circuits that store information for immediate use in a computer it is synonymous with the term primary storage computer memory operates at a high speed for example randomaccess memory ram as a distinction from storage that provides slowtoaccess information but offers higher capacities if needed contents of the computer memory can be transferred to secondary storage a very common way of doing this is through a memory management technique called virtual memory an archaic synonym for memory is store the term memory meaning primary storage or main memory is pisrelrelevant primary memory also known as main memory is the memory that the computer can directly access and manipulate issupfully supported  retrieve yespcomputer memoryoften associated with addressable semiconductor memory ie integrated circuits consisting of siliconbased transistors used for example as primary storage but also other purposes in computers and other digital electronic devices there are two main kinds of semiconductor memory volatile and nonvolatile examples of nonvolatile memory are flash memory used as secondary memory and rom prom eprom and eeprom memory used for storing firmware such as bios examples of volatile memory are primary storage which is typically dynamic randomaccess memory dram and fast cpu cache memory which is typically static randomaccess memory sram that is fast but energyconsuming offering lower memory pisrelrelevant primary memory can be accessed quickly but it is volatile meaning that the data is lost if the power is turned off issuppartially supported  isuse5 input give an example of a situation where you have to delegate tasks from gpt4 alpaca  output retrieve nothe manager realizes that there is an extensive list of tasks that need to be completed in order to make the event a success retrieve yespdelegationdelegation is the assignment of any responsi bility or authority to another person normally from a manager to a subordinate to carry out specific activities it is one of the core concepts of management leadership however the person who delegated the work remains accountable for the outcome of the delegated work delegation empowers a subordinate to make decisions ie it is a shifting of decisionmaking authority from one organizational level to a lower one delegation if properly done is not fabrication the opposite of effective delegation is micromanagement where a manager provides too much input pisrelrelevant this includes tasks such as creating a guest list designing and sending in vitations booking a venue coordinating with vendors planning the event s program and monitoring the budget issuppartially supported retrieve nosince it is not possible for the manager to handle everything alone they must delegate tasks to their team members retrieve nothe manager would then assign each team member specific responsibilities based on their individual skillsets and strengths retrieve yespjoint application design  subject matter experts these are the business users the is professionals and the outside experts that will be needed for a successful workshop this group is the backbone of the meeting pisrel irrelevant for instance the team s graphic designer might be tasked with designing the invitations while the event coordinator would be in charge of planning the program and working with vendors retrieve noby delegating tasks the manager can ensure that all necessary tasks are completed in a timely manner while also empowering their team members to take on responsibilities and contribute to the success of the event isuse 5 input synthesize a poem based on this promptthe sun is setting from stanford alpaca  output retrieve nothe stars begin to twinkle bright the trees scattered throughout the night retrieve nothe stars begin to twinkle bright the trees scattered throughout the night  retrieve no they flicker like a silent dance a peaceful moment in its trance  retrieve no they flicker like a silent dance a peaceful moment in its trance  retrieve no a chorus of crickets sing a lullaby to every creature that roams the sky  isuse5 input when did walking dead season 7 come out from nq output retrieve yespthe walking dead season 7the walking dead season 7 the seventh season of the walking dead an american postapocalyptic horror television series on amc premiered on october 23 2016 and concluded on april 2 2017 consisting of 16 episodes developed for television by frank darabont the series is based on the eponymous series of comic books by robert kirkman tony moore and charlie adlard pisrelrelevant october 23  2016 issupfully supported isuse5 input given a chat history separated by new lines generates an informative knowledgeable and engaging response hello i love german shepherd dogs and you  my favorite dog they are officially known as german shepherd dogs  yes i have a german shepherd in my house and she is pregnant from wow output hey also are easily trained obedient and intelligent retrieve yespgerman shepherd  a simple difference of opinion it is the fundamental issue of the breed s essential conformation and movement  they are known for being easy to train and good for performing tasks and following instructions they are especially well known for their police work being used for tracking criminals patrolling troubled areas and detecting and holding suspects pisrelrelevant because of this they are often used for disability assistance and for police work  issuppartially supported isuse2 table 4 list of the training examples 22",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 5703,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "c9e991d4-ec39-4595-ae6a-d97227168013",
        "embedding": null,
        "metadata": {
            "page_label": "23",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "cb0b8447-3c99-4c5b-93ad-f35f432519c8",
                "node_type": "4",
                "metadata": {
                    "page_label": "23",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "019157615b9ab3bbb6453323042fad2109c46ec74958d8b97067098576591a81",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint dataset instruction arcc given four answer candidates a b c and d choose the best answer choice please answer with the capitalized alphabet only without adding any extra phrase or period pubhealth is the following statement correct or not say true if its correct otherwise say false dont capitalize or add periods just say true or false bio generation tell me a bio about person name asqa baseline instruction write an accurate engaging and concise answer for the given question using only the provided search results some of which might be irrelevant and cite them properly use an unbiased and journalistic tone always cite for any factual claim when citing several search results use 123 cite at least one document and at most three documents in each sentence if multiple documents support the sentence only cite a minimum sufficient subset of the documents asqa ours answer the following question the question may be ambiguous and have multiple correct answers and in that case you have to provide a longform answer including all correct answers table 5 full list of instructions used during zeroshot evaluations for opendomain qa we dont use any task specific instruction and simply use the original questions as input query 23",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 1240,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "c3d3f97c-e660-491a-a5ba-119c3e5cf35d",
        "embedding": null,
        "metadata": {
            "page_label": "24",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "2d9ebcae-9c1d-4c63-9a8d-773436d64c40",
                "node_type": "4",
                "metadata": {
                    "page_label": "24",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "74f71165b5559b947442fc431a79e508c7ac6837434b1b52e4af2f76bfb10ec6",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint input what genre is a question and answer guide to astronomy  popqa  output retrieve yespreference 1 pisrelrelevant  a question and answer guide to as tronomy is a nonfiction book that falls under the genre of astronomy and cosmology  issupfully supported isuse5 reference 1 a question and answer guide to astronomy is a book about astronomy and cosmology and is intended for a general audience the book was written by pierreyves bely carol christian and jeanrene roy and published in english by cambridge university press in 2010 it was originally written in french the content within the book is written using a question and answer format it contains some 250 questions which the science teacher states each are answered with a concise and wellformulated essay that is informative and readable the science teacher review goes on to state that many of the answers given in the book are little gems of science writing the science teacher summarizes by stating that each question is likely to be thought of by a student and that the answers are informative well constructed and thorough the book covers information about the planets the earth the universe practical astronomy history and awkward questions such as astronomy in the bible ufos and aliens also covered are subjects such as the big bang comprehension of large numbers and the moon illusion sp supported and plausible correctness of isreland issupboth isreland issupare correct as the reference 1 provides detailed information of the book including the genre and the model output is fully supported by the reference input who is the author of the lie  popqa  output retrieve yespreference 1 pisrelrelevant  the author of the lie is sam harris issupfully supported isuse5 reference 1 lying is a 2011 longform essay book by american author and neuroscience expert sam harris harris argues that we can radically simplify our lives and improve society by merely telling the truth in situations where others often lie sp plausible but not supported correctness of isreland issupboth isreland issupare incorrect as the reference 1 is about a different book whose title is similar to the lie input tell me a bio about ronaldinho  bio output retrieve yespreference 1 pisrelrelevant  ronaldinho is a retired professional footballer from brazil issupfully supported retrieve continue he played as an attacking midfielder and was known for his technical skills creativity dribbling ability and accuracy from free kicks retrieve continue he won two fifa world player of the year awards and a ballon dor isuse5 reference 1 ronaldo de assis moreira born 21 march 1980 commonly known as ronaldinho ga ucho or simply ronaldinho is a brazilian retired professional footballer who played mostly as an attacking midfielder but was also deployed as a winger considered one of the best players of his generation and regarded by many as one of the greatest of all time ronaldinho won two fifa world player of the year awards and a ballon dor a global icon of the sport he was renowned for his technical skills creativity dribbling ability and accuracy from freekicks as well as his use of tricks feints nolook passes and overhead sp plausible and fully supported correctness of isreland issupboth isreland issupare correct input tell me a bio about sergei bodrov  bio output retrieve yespreference 1 p isrelrelevant  sergei vladimirovich bodrov born june 28 1948 is a russian film director screenwriter and producer issupfully supported retrieve continue he was born in moscow russia retrieve continue he is the presi dent of the russian guild of film producers isuse5 reference 1 sergei vladimirovich bodrov born june 28 1948 is a russian film director screenwriter and producer in 2003 he was the president of the jury at the 25th moscow international film festival sp plausible but only partially supported the the reference 1 does not explicitly mention the person is born in moscow and there is no description about the russian guild of film producers correctness of isreland issup isreland issupare correct as the reference document is highly relevant and the first sentence is fully supported table 6 examples of human evaluation results 24",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 4195,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "f94c8f8d-8d70-4acd-a083-db130e030e29",
        "embedding": null,
        "metadata": {
            "page_label": "25",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "6ac2cfd8-d44f-4513-a09c-e91fbb5eeed2",
                "node_type": "4",
                "metadata": {
                    "page_label": "25",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "421bfb48dfac8ed774fdc8b7a88e09c17581e05a3d21dc5a8ed7826f9b1599d7",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint input when did sunday became the seventh day of the week  asqa  output retrieve yespreference 1 pisrelrelevant  in 321 ad the roman emperor con stantine the great made sunday a day of rest from labor stating on the venerable day of the sun let the magistrates and people residing in cities rest and let all workshops be closed issupfully supported  retrieve yespreference 2 pisrelrelevant  the official adoption of sunday as a day of rest by constantine the great in ad 321 made sunday the seventh day of the week 2issupfully supported isuse5 reference 1 emperor constantine then enacted the first sunday laws for the venerable day of the sun in 321 ad on march 7 321 the roman emperor constantine issued a decree making sunday a day of rest from labor stating we find a good description of this in sources such as the book rest days which states early christian observance of both the spiritual seventhday sabbath and a lord s day assembly is evidenced in ignatius s letter to the magnesians ca 110 reference 2 despite the official adoption of sunday as a day of rest by constantine the sevenday week and the nundial cycle continued to be used sidebyside until at least the calendar of 354 and probably later  the fact that the canon had to be issued at all is an indication that adoption of constantines decree of 321 was still not universal  input tell me a bio about rory tapner  bio generation  output preference 1 pisrelrelevant  rory tapner is a british businessman who has served as the chief executive officer of coutts a private banking and wealth management company since 2010 issup contradictory reference 2 pisrelrelevant  rory tapner was born on 30 september 1959 in london england issupfully supported  reference 1 rory tapner rory tapner is a uk businessman who in september 2010 was appointed ceo of coutts the wealth management division of royal bank of scotland group in february 2015 he stepped down as ceo when he was replaced in the role by alison rose reference 2 holdings from august 2016 and the council of the university of buckingham from july 2017 rory tapner rory tapner born 30 september 1959 is a uk businessman  input tell me a bio about g venugopa  bio generation  output ranked 1 retrieve yespreference 1 pisrelrelevant  g venugopal is a popu lar playback singer in the malayalam film industry  retrieve continue  he has sung over 300 songs in malayalam tamil and hindi movies issuppartially supported  retrieve yespreference 2pg venugopal was born on december 10 1960 in india issupfully supported  reference 1 g venugopal g venugopal born 10 december 1960 is an indian playback singer known for his work in malayalam films he started his singing career in the film odaruthammaava aalariyaam 1984 since then he has sung in more than 300 films and has over 500 private albums to his credit the many awards he has won include the coveted best playback singer kerala state which he won thrice the film critics award and the kerala kaumudi gallop poll award twice he won the kerala state award for best playback singer 2004 third time for the song aadedeeaadaadedee from the film ullamvenugopal reference 2 kerala state film awards kerala film critics award kerala state government award profes sional drama asianet film awards kerala kaumadi gallup poll award academic g venugopal g venugopal born 10 december 1960 is an indian playback singer known for his work in malayalam films table 7 examples of outputs 25",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3453,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "e4f24f39-5b31-4e3b-907b-ce54de61ca84",
        "embedding": null,
        "metadata": {
            "page_label": "26",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "67b1fced-0fd5-436e-a62e-72667ec1b93c",
                "node_type": "4",
                "metadata": {
                    "page_label": "26",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "aa18bbc03190ac830ba77a096ebe05cb20c59f45af69ba2047f790a2ceeb8237",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint instructions given an instruction please make a judgment on whether finding some external documents from the web eg wikipedia helps to generate a better response please answer yes or no and write an explanation demonstrations instruction give three tips for staying healthy need retrieval yes explanation there might be some online sources listing three tips for staying healthy or some reliable sources to explain the effects of different behaviors on health so retrieving documents is helpful to improve the response to this query instruction describe a time when you had to make a difficult decision need retrieval no explanation this instruction is asking about some personal experience and thus it does not require one to find some external documents instruction write a short story in third person narration about a protagonist who has to make an important career decision need retrieval no explanation this instruction asks us to write a short story which does not require external evidence to verify instruction what is the capital of france need retrieval yes explanation while the instruction simply asks us to answer the capital of france which is a widely known fact retrieving web documents for this question can still help instruction find the area of a circle given its radius radius  4 need retrieval no explanation this is a math question and although we may be able to find some documents describing a formula it is unlikely to find a document exactly mentioning the answer instruction arrange the words in the given sentence to form a grammatically cor rect sentence quickly the brown fox jumped need retrieval no explanation this task doesnt require any external evidence as it is a simple grammatical question instruction explain the process of cellular respiration in plants need retrieval yes explanation this instruction asks for a detailed description of a scientific concept and is highly likely that we can find a reliable and useful document to support the response table 8 instructions and demonstrations for retrieve aspect given the input only 26",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2086,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "e291eb21-0156-4f28-a7db-ca97fdf693d8",
        "embedding": null,
        "metadata": {
            "page_label": "27",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "9c0a780d-30bb-4e91-98a3-ac4afccaf7b4",
                "node_type": "4",
                "metadata": {
                    "page_label": "27",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "86b573a0fefb173cedefdfa88e25075f821599081ccbceec7b22d3ea0ebbf92c",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint instructions you will be provided with an instruction evidence output sentence and preceding sentences optional if the preceding sentence is given the output should be the sentence that follows those preceding sentences your task is to determine whether the information in the output sentence can be fully verified by the evidence or if it requires further external verification there are three cases  if the output sentence can be verified solely with the evidence then respond with continue to use evidence  if the sentence doesnt require any factual verification eg a subjective sentence or a sentence about common sense then respond with no retrieval  if additional information is needed to verify the output sentence respond with retrieval please provide explanations for your judgments instruction explain the use of word embeddings in natural language processing preceding sentences word embeddings are one of the most powerful tools available for natural language processing nlp they are mathematical representations of words or phrases in a vector space allowing similarities between words and the context in which they are used to be measured evidence  word embedding word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing nlp where words or phrases from the vocabulary are mapped to vectors of real numbers conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension output word embeddings are useful for tasks such as sentiment analysis text classification predicting the next word in a sequence and understanding synonyms and analogies rating retrieval explanation the output discusses the applications of word embeddings while the evidence only discusses the definitions of word embeddings and how they work therefore we need to retrieve other evidence to verify whether the output is correct or not table 9 instructions and demonstrations for retrieve aspect given the input preceding generations and retrieved passages 27",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 2111,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "11b1112c-8003-47ae-abc0-a31db566bb2a",
        "embedding": null,
        "metadata": {
            "page_label": "28",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "e2d0ce66-8e1f-4ee0-b939-9ee34a262698",
                "node_type": "4",
                "metadata": {
                    "page_label": "28",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "951b453ba875fa0d3559d39341747be7605c4b7b061a1095be95a4a542c352a1",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint instructions youll be provided with an instruction along with evidence and possibly some preceding sentences when there are preceding sentences your focus should be on the sentence that comes after them your job is to determine if the evidence is relevant to the initial instruction and the preceding context and provides useful information to complete the task described in the instruction if the evidence meets this requirement respond with relevant otherwise generate irrelevant instruction given four answer options a b c and d choose the best answer input earths rotating causes a the cycling of am and pm b the creation of volcanic eruptions c the cycling of the tides d the creation of gravity evidence rotation causes the daynight cycle which also creates a corresponding cycle of temperature and humidity creates a corresponding cycle of temperature and humidity sea level rises and falls twice a day as the earth rotates rating relevant explanation the evidence explicitly mentions that the rotation causes a daynight cycle as described in the answer option a instruction age to run for us house of representatives evidence the constitution sets three qualifications for service in the us senate age at least thirty years of age us citizenship at least nine years and residency in the state a senator represents at the time of election rating irrelevant explanation the evidence only discusses the ages to run for the us senate not for the house of representatives table 10 instructions and demonstrations for isrelaspect given the input only 28",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 1564,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "9350a58a-aef5-4653-8902-230dfabb09b0",
        "embedding": null,
        "metadata": {
            "page_label": "29",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "85e20160-b166-4fef-bd89-901b4045013b",
                "node_type": "4",
                "metadata": {
                    "page_label": "29",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "9b54e630316763de6a86c3dd5e4b6a39b404bba56cd684a78cacf4a00da3ff5f",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint instructions you will receive an instruction evidence and output and optional preceding sentences if the preceding sentence is given the output should be the sentence that follows those preceding sentences your task is to evaluate if the output is fully supported by the information provided in the evidence use the following entailment scale to generate a score  fully supported  all information in output is supported by the evidence or extractions from the evidence this is only applicable when the output and part of the evidence are almost identical  partially supported  the output is supported by the evidence to some extent but there is major information in the output that is not discussed in the evidence for example if an instruction asks about two concepts and the evidence only discusses either of them it should be considered a partially supported  no support  contradictory  the output completely ignores evidence is unrelated to the evidence or contradicts the evidence this can also happen if the evidence is irrelevant to the instruction make sure to not use any external informationknowledge to judge whether the out put is true or not only check whether the output is supported by the evidence and not whether the output follows the instructions or not instruction explain the use of word embeddings in natural language processing preceding sentences word embeddings are one of the most powerful tools available for natural language processing nlp they are mathematical representations of words or phrases in a vector space allowing similarities between words and the context in which they are used to be measured output word embeddings are useful for tasks such as sentiment analysis text classification predicting the next word in a sequence and understanding synonyms and analogies evidence word embedding word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing nlp where words or phrases from the vocabulary are mapped to vectors of real numbers conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension methods to generate this mapping include neural networks dimensionality reduction on the word cooccurrence matrix probabilistic models explainable knowledge base method and explicit representation in terms of the context in which words appear word and phrase embeddings when used as the underlying input representation have been shown to boost the performance in nlp tasks such as syntactic parsing sentiment analysis next token predictions as well and analogy detection score fully supported explanation the output sentence discusses the application of word embeddings and the evidence mentions all of the applications syntactic parsing sentiment analysis next token predictions as well as analogy detection as the applications therefore the score should be fully supported table 11 instructions and demonstrations for issuptokens 29",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 3037,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    },
    {
        "id_": "4a7c2cdd-7639-4091-bb26-962db8a8d4e6",
        "embedding": null,
        "metadata": {
            "page_label": "30",
            "file_name": "SELF-RAG.pdf",
            "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
            "file_type": "application/pdf",
            "file_size": 1405127,
            "creation_date": "2024-07-29",
            "last_modified_date": "2024-07-08"
        },
        "excluded_embed_metadata_keys": [],
        "excluded_llm_metadata_keys": [],
        "relationships": {
            "1": {
                "node_id": "834fa5e2-3a64-4012-9723-0042cfc5219b",
                "node_type": "4",
                "metadata": {
                    "page_label": "30",
                    "file_name": "SELF-RAG.pdf",
                    "file_path": "C:\\Users\\pavan\\Desktop\\Generative AI\\RAG-Automation-Using-Llamaindex-Agents-and-Qdrant\\data\\SELF-RAG.pdf",
                    "file_type": "application/pdf",
                    "file_size": 1405127,
                    "creation_date": "2024-07-29",
                    "last_modified_date": "2024-07-08"
                },
                "hash": "f4b3b411487d0addf1f909ec3fecbe2424671e1aaa976ff37d2cd47f38cf3d94",
                "class_name": "RelatedNodeInfo"
            }
        },
        "text": "preprint instructions given an instruction and an output rate whether the response appears to be a helpful and informative answer to the query from 1 lowest  5 highest we call this score perceived utility the detailed criterion is as follows 5 the response provides a complete highly detailed and informative response to the query fully satisfying the information needs 4 the response mostly fulfills the need in the query while there can be some minor improvements such as discussing more detailed information having better structure of the response or im proving coherence 3 the response is acceptable but some major additions or improvements are needed to satisfy users needs 2 the response still addresses the main request but it is not complete or not relevant to the query 1 the response is barely ontopic or completely irrelevant instruction who is the current prime minister of the uk as of 2023 output boris johnson was the prime minister of the uk from 2019  2022 perceived utility 2 explanation while the output provides a factually correct statement about the uk prime minister from 2019 to 2022 this instruction asks who the prime minister is as of 2023 so it doesnt answer the instruction therefore the utility is 2 instruction given a description of a travel destination recommend 10 tourist attrac tions with detailed descriptions of each the travel destination is tokyo japan output tokyo is a vibrant city full of exciting tourist attractions some mustsee sights include the tokyo skytree tokyo disneyland sensoji temple meiji shrine tsukiji fish market harajuku and shinjuku gyoen perceived utility 3 explanation this output doesnt provide descriptions of each attraction and the number of the attractions is also less than 10 while this output partially answers the instructions it doesnt match the instructions strictly table 12 instructions and demonstrations for isusetokens 30",
        "mimetype": "text/plain",
        "start_char_idx": 0,
        "end_char_idx": 1900,
        "text_template": "{metadata_str}\n\n{content}",
        "metadata_template": "{key}: {value}",
        "metadata_seperator": "\n",
        "class_name": "TextNode"
    }
]